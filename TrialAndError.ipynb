{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Grounds - Fit Loop\n",
    "Creating a library that helps with the pytorch looping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1063,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T02:51:18.935380Z",
     "start_time": "2020-05-08T02:51:18.916444Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "from uuid import uuid4\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from tqdm.autonotebook import tqdm\n",
    "from typing import Union, List, Callable, Optional, Any, Dict, Tuple\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import _LRScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoopState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1347,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T16:15:06.168116Z",
     "start_time": "2020-05-08T16:15:06.138783Z"
    }
   },
   "outputs": [],
   "source": [
    "class LoopState:\n",
    "    \"\"\"\n",
    "    Maintains train/valid/test loop state for a single run of \n",
    "    a certain number of epochs, does not used to preserve state \n",
    "    between runs.\n",
    "    \"\"\"\n",
    "    _stages = ['batch','epoch_start','epoch_end']\n",
    "    _batch_step, _epoch_start, _epoch_end = _stages\n",
    "    def __init__(self, phase:str, floop:FitLoop, no_cast:bool, \n",
    "                 no_float:bool, is_train:bool, is_test:bool,\n",
    "                 dl:DataLoader\n",
    "                ):\n",
    "        \"\"\"\n",
    "        phase : phase name 'train', 'valid' or 'test'\n",
    "        floop : the calling FitLoop object\n",
    "        \"\"\"\n",
    "        self.__batch = ()\n",
    "        self.__floop = floop\n",
    "        self._no_cast = no_cast\n",
    "        self._no_float = no_float\n",
    "        self.phase = phase\n",
    "        self.batch_num = 0\n",
    "        self.epoch_num = 0\n",
    "        self.metrics = {s:{} for s in self._stages}\n",
    "        self.is_train = is_train\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        # For easy access\n",
    "        bs = dl.batch_size\n",
    "        dr = dl.drop_last\n",
    "        sz = len(dl.dataset)\n",
    "        bt = sz / bs\n",
    "        \n",
    "        # Gives dataset size and batch count\n",
    "        self.size = sz\n",
    "        self.batches = math.floor(bt) if dr else math.ceil(bt)\n",
    "        self.batch_size = 0\n",
    "    \n",
    "    def __getattr__(self, name:str) -> Any:\n",
    "        # To get attributes from the FitLoop object \n",
    "        # for use in the stage functions.\n",
    "        return getattr(self.__floop, name)\n",
    "    \n",
    "    def __getitem__(self, metric_name:str):\n",
    "        # To get the metrics stored in the batch step stage\n",
    "        metric_value = self.metrics[self._batch_step][metric_name]\n",
    "        try:\n",
    "            return torch.tensor(metric_value).float()\n",
    "        except:\n",
    "            return metric_value\n",
    "    \n",
    "    \"\"\"\n",
    "    Getter and setter for the current batch\n",
    "    \"\"\"\n",
    "    @property\n",
    "    def batch(self) -> Tuple[Tensor,...]:\n",
    "        if self._no_cast:\n",
    "            return self.__batch\n",
    "        \n",
    "        return (\n",
    "            d.to(device=self.device,dtype=self.dtype) \n",
    "            if d.is_floating_point() \n",
    "            else d.to(device=self.device,dtype=torch.long) \n",
    "            for d in self.__batch\n",
    "        )\n",
    "    \n",
    "    @batch.setter\n",
    "    def batch(self, current_batch:Tuple[Tensor,...]) -> None:\n",
    "        self.__batch = current_batch\n",
    "        \n",
    "    \"\"\"\n",
    "    Functions to append rdict values to self.metrics\n",
    "    \"\"\"\n",
    "    def _append(self, rdict:Dict[str, float], stage:str) -> None:\n",
    "        #  Append metrics to the specific stage.\n",
    "        if rdict is None:\n",
    "            if stage == self._epoch_end:\n",
    "                print(f\"no rdict returned from: f{self.phase}_{stage}\")\n",
    "            \"\"\"\n",
    "            TODO: Add warning if rdict of stage is None\n",
    "            \"\"\"\n",
    "            return\n",
    "        \n",
    "        for key in rdict:\n",
    "            if key not in self.metrics[stage]:\n",
    "                self.metrics[stage][key] = []\n",
    "            self.metrics[stage][key].append(rdict[key])\n",
    "            \n",
    "    def _append_batch_step(self, rdict:Dict[str, float]) -> None:\n",
    "        # Called after batch step rdict is returned\n",
    "        self._append(rdict, self._batch_step)\n",
    "        \n",
    "    def _append_epoch_start(self, rdict:Dict[str, float]) -> None:\n",
    "        # Called before epoch start\n",
    "        self._append(rdict, self._epoch_start)\n",
    "        \n",
    "    def _append_epoch_end(self, rdict:Dict[str, float]) -> None:\n",
    "        # Called after epoch end step rdict is returned\n",
    "        self._append(rdict, self._epoch_end)\n",
    "    \n",
    "        \n",
    "    \"\"\"\n",
    "    Functions to clear rdict values from self.metrics\n",
    "    \"\"\"\n",
    "    def _clear(self, stage:str) -> None:\n",
    "        # Clear the batch metrics at the end of the batch.\n",
    "        for mlist in self.metrics[stage]:\n",
    "            self.metrics[stage][mlist].clear()\n",
    "            \n",
    "    def _clear_batch_step(self) -> None:\n",
    "        # Called before epoch start\n",
    "        self._clear(self._batch_step)\n",
    "        \n",
    "    def _clear_epoch_start(self) -> None:\n",
    "        # Called ??\n",
    "        self._clear(self._epoch_start)\n",
    "        \n",
    "    def _clear_epoch_end(self) -> None:\n",
    "        # Called after loop end\n",
    "        self._clear(self._epoch_end)\n",
    "    \n",
    "    \"\"\"\n",
    "    State updates before epoch start and batch step stages\n",
    "    \"\"\"\n",
    "    def _pre_epoch_start_update(self, epoch_num:int) -> None:\n",
    "        self._clear_batch_step()\n",
    "        self.batch_num = 0\n",
    "        self.epoch_num = epoch_num\n",
    "    \n",
    "    def _pre_batch_step_update(self, current_batch):\n",
    "        self.batch_size = current_batch[0].size(0)\n",
    "        self.batch_num += 1\n",
    "        self.batch = current_batch\n",
    "    \n",
    "    \"\"\"\n",
    "    Functions to get various metrics at different stages \n",
    "    \"\"\"\n",
    "    def _get_epoch_metric(self, criteria:str) -> float:\n",
    "        # Last added metric that is to be used as a model \n",
    "        # selection criteria\n",
    "        metric = self.metrics[self._epoch_end][criteria][-1]\n",
    "        if self._no_float:\n",
    "            return metric\n",
    "        else:\n",
    "            return float(metric)\n",
    "    \n",
    "    def _get_epoch_metrics(self, \n",
    "                display_metrics:Optional[Union[str,List[str]]]=None\n",
    "                ) -> Dict[str,float]:\n",
    "        # Return the last saved epoch metrics\n",
    "        if isinstance(display_metrics, str):\n",
    "            return {display_metricss:self._get_epoch_metric(display_metrics)}\n",
    "        elif isinstance(display_metrics, list):\n",
    "            return {\n",
    "                metric:self._get_epoch_metric(metric)\n",
    "                for metric in display_metrics\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                metric: self._get_epoch_metric(metric)\n",
    "                for metric in self.metrics[self._epoch_end]\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## FitLoopDefaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T17:48:33.456334Z",
     "start_time": "2020-05-06T17:48:33.451212Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class FitLoopDefault:\n",
    "    def train_step(state):\n",
    "        print(\"default train_step\")\n",
    "        return {}\n",
    "\n",
    "    def valid_step(state):\n",
    "        \n",
    "        print(\"default valid_step\")\n",
    "        return {}\n",
    "\n",
    "    def test_step(state):\n",
    "        print(\"default test_step\")\n",
    "        return {}\n",
    "        \n",
    "    def train_epoch_end(state):\n",
    "        print(\"default train_epoch_end\")\n",
    "        return {}\n",
    "\n",
    "    def valid_epoch_end(state):\n",
    "        print(\"default valid_epoch_end\")\n",
    "        return {}\n",
    "\n",
    "    def test_epoch_end(state):\n",
    "        print(\"default test_epoch_end\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Metric calls\n",
    "\n",
    "```python\n",
    "# Value access\n",
    "Fitloop.metrics.train['loss']          # returns all losses from epoch end\n",
    "Fitloop.metrics.train.end['loss']      # returns all losses from epoch end\n",
    "Fitloop.metrics.train.start['loss']    # returns all losses from epoch start\n",
    "Fitloop.metrics.train.batch['loss']    # returns all losses from batch step\n",
    " \n",
    "Fitloop.metrics.train['loss'][0]       # returns losses for run 0 from epoch end\n",
    "Fitloop.metrics.valid.batch['accu'][3] # returns all validation accuracies for batch step from run 3\n",
    " \n",
    "# Value visualization \n",
    "Fitloop.metrics.plot()                 # plots validation criteria against training criteria (eg accuracy)\n",
    "                                       # if criteria not available, then first key from rdict.\n",
    "Fitloop.metrics.train.plot()           # if loss then loss else, plots first value from rdict\n",
    "Fitloop.metrics.train.plot('loss')     # plots loss \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics/MetricsAggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1420,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T16:39:20.421385Z",
     "start_time": "2020-05-08T16:39:20.394041Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "run = 0\n",
    "metric = 'loss'\n",
    "\n",
    "FitLoop.metrics.valid.start[metric:dict][run:array] -> float\n",
    "----\n",
    "FitLoop.metrics.valid.start[metric][run]\n",
    "MetricsAggregator.valid.start[metric][run]\n",
    "Metrics.start[metric][run]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class Metrics:\n",
    "    \"\"\"\n",
    "    Class to keep track of metrics for a single phase.\n",
    "    \"\"\"\n",
    "    def __init__(self, name, is_train, no_float):\n",
    "        self.stages = []\n",
    "        self._name = name\n",
    "        self._runs = 0\n",
    "        self._in_run = False\n",
    "        self._is_train = is_train\n",
    "        self._no_float = no_float\n",
    "        \n",
    "    def __repr__(self):\n",
    "        stages = ', '.join(self.stages)\n",
    "        return f\"<Metrics({self._name}) :: stages:[{stages}] at {hex(id(self))}>\"\n",
    "    \n",
    "    def _complete_run(self, is_train):\n",
    "        \"\"\"\n",
    "        Set appropriate flags.\n",
    "        Convert values for the run to numpy arrays.\n",
    "        \"\"\"\n",
    "        if not is_train == self._is_train:\n",
    "            return\n",
    "        \n",
    "        self._in_run = False\n",
    "        for stage in self.stages:\n",
    "            self_stage = getattr(self, stage)\n",
    "            for metric in self_stage:\n",
    "                m_run = self_stage[metric][self._runs - 1]\n",
    "                try:\n",
    "                    self_stage[metric][self._runs - 1] = np.array(m_run)\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "    def _append(self, stage, rdict):\n",
    "        \"\"\"\n",
    "        Add values in rdict to this object.\n",
    "        Metrics.stage[name:str][run:int] -> value:float\n",
    "        \"\"\"\n",
    "        if not self._in_run:\n",
    "            # Set run flag and value\n",
    "            self._in_run = True\n",
    "            self._runs += 1\n",
    "            \n",
    "        if not hasattr(self, stage):\n",
    "            self.stages.append(stage)\n",
    "            setattr(self, stage, {})\n",
    "            \n",
    "        self_stage = getattr(self, stage)\n",
    "        \n",
    "        for key in rdict:\n",
    "            val = rdict[key]\n",
    "            if key not in self_stage:\n",
    "                self_stage[key] = []\n",
    "                \n",
    "            if len(self_stage[key]) < self._runs:\n",
    "                self_stage[key].append([])\n",
    "                \n",
    "            if not self._no_float:\n",
    "                try:\n",
    "                    val = float(val)\n",
    "                except:\n",
    "                    pass\n",
    "            self_stage[key][self._runs - 1].append(val)\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"\n",
    "        Clears all recorded metrics for this phase.\n",
    "        \"\"\"\n",
    "        for stage in self.stages:\n",
    "            getattr(self, stage).clear()\n",
    "        self._runs = 0\n",
    "        \n",
    "        \n",
    "    \n",
    "class MetricsAggregator:\n",
    "    \"\"\"\n",
    "    Class to keep track of metrics for all phases.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.train_runs = 0\n",
    "        self.test_runs = 0\n",
    "        self._in_test = False\n",
    "        self._in_train = False\n",
    "        self._is_checkup = False\n",
    "        self._no_float = None\n",
    "        self.phases = []\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"<MetricsAggregator :: train_runs:{self.train_runs} test_runs:{self.test_runs} at {hex(id(self))}>\"\n",
    "    \n",
    "    def _start_run(self, is_checkup:bool, is_test:bool, no_float:bool) -> None:\n",
    "        \"\"\"\n",
    "        Set flags and run counters.\n",
    "        \"\"\"\n",
    "        if is_checkup:\n",
    "            self._is_checkup = True\n",
    "            return\n",
    "        \n",
    "        self._no_float = no_float\n",
    "        if is_test:\n",
    "            self._in_test = True\n",
    "            self.test_runs += 1\n",
    "        else:\n",
    "            self._in_train = True\n",
    "            self.train_runs += 1\n",
    "    \n",
    "    def _complete_run(self, is_checkup:bool, is_test:bool) -> None:\n",
    "        \"\"\"\n",
    "        Set flags and run counters for self and\n",
    "        attached Metrics objects\n",
    "        \"\"\"\n",
    "        if is_checkup:\n",
    "            self._is_checkup = False\n",
    "            return\n",
    "        for phase in self.phases:\n",
    "            getattr(self, phase)._complete_run(self._in_train)\n",
    "            \n",
    "        if is_test:\n",
    "            self._in_test = False\n",
    "        else:\n",
    "            self._in_train = False\n",
    "        \n",
    "    def _append(self, phase:str, stage:str, rdict:Dict[str,Union[Tensor, float, Any]]) -> None:\n",
    "        \"\"\"\n",
    "        Create a Metrics object for each phase and attach\n",
    "        it to self if not present.\n",
    "        \n",
    "        ._append rdict values to a stage in each of the \n",
    "        metrics object.\n",
    "        \"\"\"\n",
    "        if self._is_checkup:\n",
    "            return\n",
    "        \n",
    "        if not hasattr(self, phase):\n",
    "            self.phases.append(phase)\n",
    "            setattr(self, phase, Metrics(phase, self._in_train,self._no_float))\n",
    "        if self._in_train:\n",
    "            getattr(self, phase)._append(stage, rdict)\n",
    "        elif self._in_test:\n",
    "            getattr(self, phase)._append(stage, rdict)\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"\n",
    "        Clears recorded metrics for all phases.\n",
    "        \"\"\"\n",
    "        for phase in self.phases:\n",
    "            getattr(self, phase).clear()\n",
    "        self.test_runs = 0\n",
    "        self.train_runs = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FitLoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T05:41:41.920640Z",
     "start_time": "2020-05-07T05:41:41.913668Z"
    }
   },
   "outputs": [],
   "source": [
    "def ftime(t1:float,t2:float)->str:\n",
    "    t = t2-t1\n",
    "    gm = time.gmtime(t)\n",
    "    try:\n",
    "        ms = str(t).split('.')[1][:3]\n",
    "    except:\n",
    "        ms = \"\"\n",
    "    if gm.tm_hour > 0:\n",
    "        fstr = \"%H h %M m %S s\"\n",
    "    elif gm.tm_min > 0:\n",
    "        fstr = \"%M m %S s\"\n",
    "    else:\n",
    "        fstr = \"%S s\"\n",
    "    return time.strftime(fstr, gm) + f\" {ms} ms\"\n",
    "\n",
    "def ptime(t:float)->str:\n",
    "    gm = time.gmtime(t)\n",
    "    try:\n",
    "        fs = str(t).split('.')[1]\n",
    "        ms = fs[:3]\n",
    "        us = fs[3:6]\n",
    "    except:\n",
    "        ms = \"\"\n",
    "    if gm.tm_hour > 0:\n",
    "        fstr = \"%H h %M m %S s\"\n",
    "    elif gm.tm_min > 0:\n",
    "        fstr = \"%M m %S s\"\n",
    "    else:\n",
    "        fstr = \"%S s\"\n",
    "    return time.strftime(fstr, gm) + f\" {ms} ms {us} us\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1460,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T19:26:04.199453Z",
     "start_time": "2020-05-08T19:26:04.057494Z"
    }
   },
   "outputs": [],
   "source": [
    "class FitLoop:\n",
    "    \"\"\"\n",
    "    FitLoop trains Pytorch models.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    SECTION: 0 \n",
    "    \n",
    "    Initialization\n",
    "    \"\"\"\n",
    "    _sets = ['train','valid','test'] # DONT CHANGE THESE, A LOT MAY BREAK\n",
    "    _TR, _VA, _TE = _sets\n",
    "    \n",
    "    _model_type = ['pretrained','best']\n",
    "    _PR, _BS = _model_type\n",
    "    def __init__(self, \n",
    "                 # Basic Blocks\n",
    "                 model: Module, \n",
    "                 optimizer: Union[Optimizer,List[Optimizer]], \n",
    "                 loss_function: Callable[[Tensor,Tensor],Tensor], \n",
    "                 \n",
    "                 # DataLoader\n",
    "                 train_dl: Optional[DataLoader]=None, \n",
    "                 valid_dl: Optional[DataLoader]=None, \n",
    "                 test_dl: Optional[DataLoader]=None, \n",
    "                 \n",
    "                 # Batch Step\n",
    "                 train_step: Callable[[LoopState],Dict[str, Any]]=FitLoopDefault.train_step,\n",
    "                 valid_step: Optional[Callable[[LoopState],Dict[str, Any]]]=FitLoopDefault.valid_step,\n",
    "                 test_step: Optional[Callable[[LoopState],Dict[str, Any]]]=FitLoopDefault.test_step,\n",
    "                 \n",
    "                 # Epoch Start Step\n",
    "                 train_epoch_start: Optional[Callable[[LoopState],Dict[str, Any]]]=None,\n",
    "                 valid_epoch_start: Optional[Callable[[LoopState],Dict[str, Any]]]=None,\n",
    "                 test_epoch_start: Optional[Callable[[LoopState],Dict[str, Any]]]=None,\n",
    "                 \n",
    "                 # Epoch End Step\n",
    "                 train_epoch_end: Callable[[LoopState],Dict[str, Any]]=FitLoopDefault.train_epoch_end,\n",
    "                 valid_epoch_end: Optional[Callable[[LoopState],Dict[str, Any]]]=FitLoopDefault.valid_epoch_end,\n",
    "                 test_epoch_end: Optional[Callable[[LoopState],Dict[str, Any]]]=FitLoopDefault.test_epoch_end,\n",
    "                 \n",
    "                 # Other Stage Functions\n",
    "                 preloop: Optional[Callable[[dict],None]]=None,\n",
    "                 postloop: Optional[Callable[[dict],None]]=None,\n",
    "                 \n",
    "                 # Other Args\n",
    "                 lr_scheduler: Optional[Union[_LRScheduler, Any, List[Union[_LRScheduler,Any]]]]=None,\n",
    "                 device: torch.device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'), \n",
    "                 configure_optimizer:Callable[[FitLoop],None]=None,\n",
    "                 dtype: torch.dtype=torch.float32,\n",
    "                 \n",
    "                 # Model Evaluation\n",
    "                 criteria: Optional[str]=None,\n",
    "                 criteria_direction: int=1,\n",
    "                 \n",
    "                 # Preserving Model State\n",
    "                 save_to_disk: bool=False,\n",
    "                 save_path: str=\"models\",\n",
    "                 pretrained_model_name: Optional[str]=None,\n",
    "                 best_model_name: Optional[str]=None,\n",
    "                ) -> None:\n",
    "        \"\"\"\n",
    "        FitLoop constructor\n",
    "        ----\n",
    "        Parameters:\n",
    "        # Basic Blocks\n",
    "            The bare minimum required along with train_dl.\n",
    "            - model : nn.Module model that has to be trained\n",
    "            - optimizer : an optimizer from torch.optim\n",
    "            - loss_function : function to compute loss\n",
    "         \n",
    "        # DataLoader\n",
    "            - train_dl : training DataLoader\n",
    "            - valid_dl : validation DataLoader, if None validation will be ignored\n",
    "            - test_dl : testing DataLoader, if None `.test()` will not run\n",
    "         \n",
    "        # Batch Step\n",
    "            Functions that take in a LoopState object to perform \n",
    "            required calculations, functions should return a dict with values\n",
    "            to be used in the epoch end step.\n",
    "            - train_step : portion of the loop where forward and backward \n",
    "                passes take place.\n",
    "            - valid_step : validation portion of the loop.\n",
    "            - test_step : called when `FitLoop.test()` is called.\n",
    "        \n",
    "        # Epoch Start Step\n",
    "            - train_epoch_start : Train phase stage function in the epoch loop at the start.\n",
    "            - valid_epoch_start : Valid phase stage function in the epoch loop at the start.\n",
    "            - test_epoch_start : Test phase stage function in the epoch loop at the start.\n",
    "        \n",
    "        # Epoch End Step\n",
    "            Functions that take in a LoopState object to perform \n",
    "            required calculations, functions should return a dict with values\n",
    "            that are to be returned when the loop is over.\n",
    "            - train_epoch_end : after training epoch has ended.\n",
    "            - valid_epoch_end : after validation epoch has ended.\n",
    "            - test_epoch_end : called when the test loop is done, one iteration\n",
    "                over all batches in the test dataloader.\n",
    "                \n",
    "        # Other Stage Functions\n",
    "            - preloop : function that is called before the epoch loop runs, it is passed\n",
    "                all the loop variables (local()) in a dict.\n",
    "            - postloop : function that is called after the epoch loop runs, it is passed\n",
    "                all the loop variables (local()) in a dict.\n",
    "        \n",
    "        # Other Args\n",
    "            - lr_scheduler : scheduler from torch.optim.lr_scheduler\n",
    "            - device : torch.device model will be cast to device this prior to the loop\n",
    "            - configure_optimizer : function that configures the optimizer, will be called\n",
    "                whenever the model weights have to be restored.\n",
    "            - dtype : floating point dtype to cast model and data to\n",
    "            \n",
    "        # Model Evaluation\n",
    "            - criteria : model evaluation metric that is returned in the dict of the\n",
    "                `valid_epoch_end` stage function if None (default) best model and \n",
    "                best score are not tracked.\n",
    "            - criteria_direction : whether more is better (1) or less is better (-1) \n",
    "                for model score criteria.\n",
    "        \n",
    "        # Preserving Model State\n",
    "            - save_to_disk : True then save pretrained and best_model to the disk, else it is \n",
    "                stored as an attribute.\n",
    "            - save_path : location where the initial and pretrained models are to be saved\n",
    "            - pretrained_model_name : Name to save the pretrained model by\n",
    "            - best_model_name : Name to save the best model by\n",
    "        \"\"\"\n",
    "        # Basic Blocks\n",
    "        self.model = model.to(device=device, dtype=dtype)\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_function = loss_function\n",
    "        \n",
    "        # DataLoaders\n",
    "        self.train_dl = train_dl\n",
    "        self.valid_dl = valid_dl\n",
    "        self.test_dl = test_dl\n",
    "        \n",
    "        # Batch Step\n",
    "        self.train_step = train_step\n",
    "        self.valid_step = valid_step\n",
    "        self.test_step = test_step\n",
    "        \n",
    "        # Epoch Start Step\n",
    "        self.train_epoch_start = train_epoch_start\n",
    "        self.valid_epoch_start = valid_epoch_start\n",
    "        self.test_epoch_start = test_epoch_start\n",
    "        \n",
    "        # Epoch End Step\n",
    "        self.train_epoch_end = train_epoch_end\n",
    "        self.valid_epoch_end = valid_epoch_end\n",
    "        self.test_epoch_end = test_epoch_end\n",
    "        \n",
    "        # Other Stage Functions\n",
    "        self.preloop = preloop\n",
    "        self.postloop = postloop\n",
    "        \n",
    "        # Other Args\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.device = device\n",
    "        self.configure_optimizer = configure_optimizer\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        # Model Evaluation\n",
    "        self.criteria = criteria\n",
    "        self.criteria_direction = criteria_direction\n",
    "        \n",
    "        # Preserving Model State\n",
    "        if pretrained_model_name is None:\n",
    "            u = str(uuid4()).split('-')[1]\n",
    "            pretrained_model_name = f\"pretrained_{u}.pt\"\n",
    "        if best_model_name is None:\n",
    "            u = str(uuid4()).split('-')[1]\n",
    "            best_model_name = f\"best_{u}.pt\"\n",
    "        self.pretrained_model_name = pretrained_model_name\n",
    "        self.best_model_name = best_model_name\n",
    "        self.save_to_disk = save_to_disk\n",
    "        self.save_path = Path(save_path)\n",
    "        \n",
    "        # INITIALIZE NON ARGS\n",
    "        self.best_model_state_dict = None\n",
    "        self.pretrained_model_state_dict = None\n",
    "        self.epoch_num = 0\n",
    "        self.best_score = self.criteria_direction * float('-inf')\n",
    "        self.time_profile = {}\n",
    "        self.metrics = MetricsAggregator()\n",
    "        self.__save_model(self._PR)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        if self.criteria is not None:\n",
    "            cri = f\"{self.criteria}:{self.best_score:0.4f}\"\n",
    "        else:\n",
    "            cri = f\"best_score:{self.best_score}\"\n",
    "        return f\"<FitLoop :: epoch_num:{self.epoch_num} {cri} at {hex(id(self))}>\"\n",
    "        \n",
    "            \n",
    "            \n",
    "    # ---------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    SECTION: 1\n",
    "    \n",
    "    Helper functions used in `__loop`\n",
    "    \"\"\"\n",
    "    \n",
    "    def __call_batch_step(self, state:LoopState, track_batch_metrics:bool) -> None:\n",
    "        phase = state.phase\n",
    "        step_funcs = [self.train_step, self.valid_step, self.test_step]\n",
    "        step_funcs = {s:f for s,f in zip(self._sets, step_funcs)}\n",
    "        step_func = step_funcs[phase]\n",
    "        \n",
    "        if step_func is None:\n",
    "            raise AttributeError(f\"{phase}_step not assigned\")\n",
    "        rdict = step_func(state)\n",
    "        if isinstance(rdict,dict):\n",
    "            state._append_batch_step(rdict)\n",
    "            if track_batch_metrics:\n",
    "                self.metrics._append(phase,'batch_step',rdict)\n",
    "        \n",
    "    def __call_epoch_start_step(self, state:LoopState) -> None:\n",
    "        phase = state.phase\n",
    "        step_funcs = [self.train_epoch_start,self.valid_epoch_start,self.test_epoch_start]\n",
    "        step_funcs = {s:f for s,f in zip(self._sets, step_funcs)}\n",
    "        step_func = step_funcs[phase]\n",
    "        \n",
    "        if step_func is None:\n",
    "            return None\n",
    "        rdict = step_func(state)\n",
    "        if isinstance(rdict,dict):\n",
    "            state._append_epoch_start(rdict)\n",
    "            self.metrics._append(phase,'epoch_start',rdict)\n",
    "        \n",
    "    def __call_epoch_end_step(self, state:LoopState) -> None:\n",
    "        phase = state.phase\n",
    "        step_funcs = [self.train_epoch_end,self.valid_epoch_end,self.test_epoch_end]\n",
    "        step_funcs = {s:f for s,f in zip(self._sets, step_funcs)}\n",
    "        step_func = step_funcs[state.phase]\n",
    "        \n",
    "        if step_func is None:\n",
    "            raise AttributeError(f\"{phase}_end_step not assigned\")\n",
    "        rdict = step_func(state)\n",
    "        if isinstance(rdict,dict):\n",
    "            state._append_epoch_end(rdict)\n",
    "            self.metrics._append(phase,'epoch_end',rdict)\n",
    "        \n",
    "    def __get_dl(self, is_test:bool, use_test_dl:Optional[bool]=None,\n",
    "                train_dl:Optional[DataLoader]=None,\n",
    "                valid_dl:Optional[DataLoader]=None,\n",
    "                test_dl:Optional[DataLoader]=None\n",
    "                )-> Dict[str,DataLoader]:\n",
    "        if is_test:\n",
    "            if test_dl is not None:\n",
    "                return {self._TE:test_dl}\n",
    "            \n",
    "            if use_test_dl is not None and not use_test_dl:\n",
    "                te_dl = valid_dl if valid_dl is not None else self.valid_dl\n",
    "                if te_dl is None:\n",
    "                    raise AttributeError(\"valid_dl not assigned\")\n",
    "                return {self._TE:te_dl}\n",
    "            \n",
    "            elif self.test_dl is None:\n",
    "                raise AttributeError(\"test_dl not assigned\")\n",
    "            return {self._TE:self.test_dl}\n",
    "        \n",
    "        va_dl = valid_dl if valid_dl is not None else self.valid_dl\n",
    "        tr_dl = train_dl if train_dl is not None else self.train_dl\n",
    "        \n",
    "        if tr_dl is None:\n",
    "            raise AttributeError(\"train_dl not assigned, please use the train_dl kwarg\")\n",
    "        if  va_dl is not None:\n",
    "            return {self._TR:tr_dl, self._VA:va_dl}\n",
    "        else:\n",
    "            return {self._TR:tr_dl}\n",
    "    \n",
    "    def __profile_time(self, t1, t2, name, is_test):\n",
    "        if t1 is None or t2 is None:\n",
    "            return\n",
    "        else:\n",
    "            t = t2 - t1\n",
    "            a,*b =  name.split('_')\n",
    "            if len(b) == 0:\n",
    "                a += \"_t\" if is_test else \"\"\n",
    "                if a not in self.time_profile:\n",
    "                    self.time_profile[a] = []\n",
    "                self.time_profile[a].append(t)\n",
    "            else:\n",
    "                b = '_'.join(b)\n",
    "                b += \"_t\" if is_test else \"\"\n",
    "                if a not in self.time_profile:\n",
    "                    self.time_profile[a] = {}\n",
    "                if b not in self.time_profile[a]:\n",
    "                    self.time_profile[a][b] = []\n",
    "                self.time_profile[a][b].append(t)\n",
    "                \n",
    "    def print_time_profile(self):\n",
    "        if len(self.time_profile) == 0:\n",
    "            print(\"please run FitLoop.run_profiler(print_outcome=False) first\")\n",
    "        else:\n",
    "            print(\"AVERAGE TIMES\")\n",
    "            for i,m in enumerate(self.time_profile):\n",
    "                if isinstance(self.time_profile[m], list):\n",
    "                    temp = torch.tensor(self.time_profile[m]).mean().item()\n",
    "                    prf = f\"{i+1}. {m}:\".ljust(20)\n",
    "                    print(f\"{prf} {ptime(temp)}\")\n",
    "                else:\n",
    "                    print(f\"{i+1}. {m}\")\n",
    "                    \n",
    "                    for j,n in enumerate(self.time_profile[m]):\n",
    "                        temp = torch.tensor(self.time_profile[m][n]).mean().item()\n",
    "                        prf = f\"{j+1}. {n}:\".ljust(18)\n",
    "                        print(f\"  {prf} {ptime(temp)}\")\n",
    "            \n",
    "    \n",
    "    def __profile_other(self,val,name):\n",
    "        # TODO: Profiler for other metrics: CPU, GPU, RAM usages.\n",
    "        print(\"NOT IMPLEMENTED YET\")\n",
    "        pass\n",
    "        \n",
    "        \n",
    "    # ---------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    SECTION: 2\n",
    "    \n",
    "    The main loop function __loop \n",
    "    \"\"\"\n",
    "    \n",
    "    def __loop(self, \n",
    "            epochs:int=1,  print_every:int=1, \n",
    "            steps: Optional[int]=None, load_best:bool=False, \n",
    "            profiler:bool=False, is_test:bool=False,\n",
    "            track_batch_metrics:bool=True, define_all:bool=False,\n",
    "            continue_loop:int=0, no_print:bool=False, no_cast:bool=False,\n",
    "            display_metrics:Optional[Union[str,List[str]]]=None, no_float:bool=False,\n",
    "            no_progress:bool=False,is_sanity_check:bool=False, use_test_dl:Optional[bool]=None,\n",
    "            train_dl:Optional[DataLoader]=None,\n",
    "            valid_dl:Optional[DataLoader]=None,\n",
    "            test_dl:Optional[DataLoader]=None\n",
    "           ) -> None:\n",
    "        \"\"\"\n",
    "        Runs the training loop for `epochs`\n",
    "        ----\n",
    "        PARAMETERS\n",
    "         - epochs : should be a non negative integer\n",
    "         - print_every : if 0 will not print, else will print at given epoch\n",
    "         - steps : number of batches to run in each phase [train,valid] \n",
    "             for check if everything is working.\n",
    "         - load_best : whether to load the best model after training, works only if validation\n",
    "             parameters are defined `valid_dl`, `valid_step`, `valid_epoch_end`\n",
    "         - profiler : whether to keep track of time taken by various sections\n",
    "         - is_test : whether it is a model testing loop or training/validation loop\n",
    "         - track_batch_metrics : whether to store the values returned in the batch steps\n",
    "         - define_all : If True then `torch.set_grad_enabled`, `optimizer.zero_grad` and model mode \n",
    "             ie [train,eval] have to be called where required (usually in the `train_step` function).\n",
    "         - continue_loop : Will ask whether to continue training after `continue_loop` epochs, should\n",
    "             be a positive integer.\n",
    "         - no_print : If True will suppress all print statements, can be used when custom logging is\n",
    "             used in the stage functions.\n",
    "         - no_cast : True, if data casting has to be manually set in the stage functions\n",
    "         - display_metrics : List of metrics returned in the epoch_end stage rdict that has to be \n",
    "             displayed, if None (default) all the returned metrics are displayed.\n",
    "         - no_float : True don't apply float conversion to returned metrics.\n",
    "         - no_progress : False don't show the progress bars.\n",
    "         - is_sanity_check : For sanity check mode.\n",
    "         - use_test_dl : For use with sanity check, to use valid dl or test dl.\n",
    "         - train_dl : Will use this instead of DatLoader passed in the constructor call.\n",
    "         - valid_dl : Will use this instead of DatLoader passed in the constructor call.\n",
    "         - test_dl : Will use this instead of DatLoader passed in the constructor call.\n",
    "        \n",
    "        \"\"\"\n",
    "        time_ = lambda p : time.perf_counter() if p else None\n",
    "        tpe = lambda : time_(print_every != 0) # Returns the time \n",
    "        tpr = lambda : time_(profiler) # Times keeping used by profiler\n",
    "        \n",
    "        prof_total = tpr() # ⏳\n",
    "        total_time_start = tpe()\n",
    "        \n",
    "        # INITILIZING VARIABLES -----\n",
    "        is_train = not(is_test or is_sanity_check or profiler)\n",
    "        pre = self.preloop is not None\n",
    "        post = self.postloop is not None\n",
    "        self.metrics._start_run((is_sanity_check or profiler),is_test, no_float)\n",
    "        \n",
    "        # Storage\n",
    "        prof_time = {}\n",
    "        dl = self.__get_dl(is_test, use_test_dl, train_dl, valid_dl, test_dl)\n",
    "        sz = { k : len(dl[k].dataset) for k in dl }\n",
    "        phases = [ph for ph in dl]\n",
    "        state = {ph: LoopState(ph,self,no_cast,no_float,is_train, is_test, dl[ph]) for ph in phases}\n",
    "\n",
    "        # Markers\n",
    "        self.__save_model(self._BS)\n",
    "        \n",
    "        # TQDM Progressbar\n",
    "        tot_size = torch.tensor([len(dl[d]) for d in dl]).sum().long().item()\n",
    "        if steps is not None:\n",
    "            tot_size = torch.tensor([len(dl[d]) if len(dl[d]) < steps else steps for d in dl])\\\n",
    "                .sum().long().item()\n",
    "                                 \n",
    "        l_bar='{desc}: {percentage:3.0f}%|' \n",
    "        r_bar='| [ {n_fmt}/{total_fmt} ] :: [ {elapsed} < {remaining} ] :: [ {rate_fmt} ] ' \n",
    "        bar_format = f'{l_bar}'+'{bar}'+f'{r_bar}'\n",
    "        etqdm = lambda e: tqdm(range(e),desc=\"EPOCH :\", disable=no_progress or is_test, \\\n",
    "                               bar_format=bar_format, unit=\"epoch\",dynamic_ncols=True)\n",
    "        btqdm = lambda : tqdm(range(tot_size),leave=False or is_test,disable=no_progress,\\\n",
    "                               bar_format=bar_format,unit=\"batch\",dynamic_ncols=True)\n",
    "         \n",
    "        # PROFILER STATEMENT ---------\n",
    "        if profiler:\n",
    "            print(f\"RUNNING PROFILER: {'TEST' if is_test else 'TRAIN'} LOOP\" , (\"\" if is_test else f\"{epochs} EPOCH(s)\"))\n",
    "            for dlo in dl: \n",
    "                dlo_b = len(dl[dlo])\n",
    "                dlo_s = len(dl[dlo].dataset)\n",
    "                bs = dl[dlo].batch_size\n",
    "                if steps is not None and dlo_b > steps:\n",
    "                    dlo_b = steps\n",
    "                lb = dlo_s % bs\n",
    "                lb = lb if lb > 0 else bs\n",
    "                print(f\"  {dlo.ljust(5)} dl :: batches: {dlo_b:4} batch_size: {dl[dlo].batch_size:4} last_batch: {lb:4} dataset_size: {dlo_s:6}\")\n",
    "            print()\n",
    "        \n",
    "        \n",
    "        # CONVENIENCE FUNCTIONS ------\n",
    "        \n",
    "        # Function to get formatted epochs (from 1 not 0)\n",
    "        r_just_val = len(str(epochs))*2 + 3\n",
    "        estr = lambda e: f\"[{e + 1}/{epochs}]\".rjust(r_just_val)\n",
    "\n",
    "        # Function to print every `print_every` epochs.\n",
    "        def eprint(e,st):\n",
    "            if not no_print:\n",
    "                if (e == 0) and (print_every != 0):\n",
    "                    print(st,end=\"\")\n",
    "                elif (e + 1) % print_every == 0:\n",
    "                    print(st,end=\"\")\n",
    "\n",
    "        # Function for phase strings.\n",
    "        def statstr(phase, epoch_metrics, rjust=True):\n",
    "            mt = ' | '.join([f\"{m}: {epoch_metrics[m]:0.4f}\"for m in epoch_metrics])\n",
    "            st =  f\"{phase} :: {mt} \\n\"\n",
    "            if rjust:\n",
    "                return st.rjust(r_just_val + len(st) + 3)\n",
    "            else:\n",
    "                return st\n",
    "            \n",
    "        # To set is_test\n",
    "        def _profile_time(t1,t2,name):\n",
    "            self.__profile_time(t1,t2,name,is_test=is_test)\n",
    "            \n",
    "        prof_preloop = tpr()\n",
    "        pre and self.preloop(locals())\n",
    "        pre and profiler and _profile_time(prof_total_start, tpr(), 'preloop') # ⏳\n",
    "            \n",
    "        profiler and _profile_time(prof_total, tpr(), 'initialize') # ⏳\n",
    "        \n",
    "        # EPOCH LOOP - START -----------\n",
    "        prof_epoch_loop = tpr()\n",
    "        for e in etqdm(epochs):\n",
    "            prof_epoch_inner = tpr() # ⏳\n",
    "            epoch_time_start = tpe()\n",
    "            \n",
    "            # UPDATE: epoch_num\n",
    "            if not is_sanity_check and not profiler and not is_test:\n",
    "                self.epoch_num += 1\n",
    "            \n",
    "            # PHASE LOOP [TRAIN|VALID,TEST] - START\n",
    "            prof_phase_loop = tpr() # ⏳\n",
    "            prog_bar_phase = btqdm()\n",
    "            for phase in phases:\n",
    "                prof_phase_inner = tpr() # ⏳\n",
    "                prog_bar_phase.desc = phase.upper().ljust(5)+\" :\"\n",
    "                \n",
    "                # EPOCH START STEP - START \n",
    "                prof_epoch_start = tpr() # ⏳\n",
    "                self.__call_epoch_start_step(state[phase])\n",
    "                profiler and _profile_time(prof_epoch_start,tpr(),f'{phase}_epoch_start') # ⏳\n",
    "                # EPOCH START STEP - END \n",
    "                \n",
    "                is_tr = phase == self._TR\n",
    "                if is_tr:\n",
    "                    eprint(e,estr(e)+f\" - \")\n",
    "                \n",
    "                # UPDATE: batch_num, metrics['batch'], epoch_num\n",
    "                state[phase]._pre_epoch_start_update(e)\n",
    "                \n",
    "                \n",
    "                if not define_all:\n",
    "                    if is_tr:\n",
    "                          model.train()\n",
    "                    else:\n",
    "                          model.eval()\n",
    "                            \n",
    "                # BATCH LOOP - START \n",
    "                prof_batch_loop = tpr() # ⏳\n",
    "                for step, batch in enumerate(dl[phase]):\n",
    "                    prof_batch_inner = tpr() # ⏳\n",
    "                    \n",
    "                    if steps is not None and step == steps: break\n",
    "                    \n",
    "                    # Update LoopState: batch_num, batch and batch_size\n",
    "                    state[phase]._pre_batch_step_update(batch)\n",
    "                    \n",
    "                    # BATCH STEP - START \n",
    "                    prof_batch_step = tpr() # ⏳\n",
    "                    if define_all:\n",
    "                        self.__call_batch_step(state[phase], track_batch_metrics)\n",
    "                    else:\n",
    "                        if isinstance(self.optimizer,list):\n",
    "                            for opt in self.optimizer:opt.zero_grad()\n",
    "                        else:\n",
    "                            self.optimizer.zero_grad()\n",
    "                        with torch.set_grad_enabled(is_tr):\n",
    "                            self.__call_batch_step(state[phase], track_batch_metrics)\n",
    "                    profiler and _profile_time(prof_batch_step,tpr(),f'{phase}_step') # ⏳\n",
    "                    # BATCH STEP - END \n",
    "                    prog_bar_phase.update(1)\n",
    "                    \n",
    "                    profiler and _profile_time(prof_batch_inner,tpr(),f'{phase}_batch_inner') # ⏳\n",
    "                    \n",
    "                profiler and _profile_time(prof_batch_loop,tpr(),f'{phase}_batch_loop') # ⏳\n",
    "                # BATCH LOOP - END \n",
    "                \n",
    "                # EPOCH END STEP - START \n",
    "                prof_epoch_end = tpr()\n",
    "                self.__call_epoch_end_step(state[phase])\n",
    "                profiler and _profile_time(prof_epoch_end,tpr(),f'{phase}_epoch_end') # ⏳\n",
    "                # EPOCH END STEP - END \n",
    "                \n",
    "                # UPDATE MARKERS\n",
    "                if not (is_tr or is_test or profiler or is_sanity_check) and self.criteria is not None:\n",
    "                    score = state[phase]._get_epoch_metric(self.criteria)\n",
    "                    direc = self.criteria_direction > 0\n",
    "                    is_better = (score > self.best_score) if direc else (score < self.best_score)\n",
    "                    if is_better:\n",
    "                        self.best_score = score\n",
    "                        self.__save_model(self._BS)\n",
    "\n",
    "                # PRINT EPOCH[PHASE] METRICS\n",
    "                epoch_metrics = state[phase]._get_epoch_metrics(display_metrics)\n",
    "                if is_tr or is_test:\n",
    "                    eprint(e,statstr(phase, epoch_metrics,False))\n",
    "                else:\n",
    "                    eprint(e,statstr(phase, epoch_metrics))\n",
    "                    \n",
    "                profiler and _profile_time(prof_phase_inner,tpr(),f'{phase}_phase_inner') # ⏳\n",
    "                \n",
    "            profiler and _profile_time(prof_phase_loop,tpr(),f'phase loop') # ⏳\n",
    "            # PHASE LOOP [TRAIN|VALID,TEST] - END\n",
    "            \n",
    "            # PRINT EPOCH TIMES\n",
    "            epoch_time_end = tpe()\n",
    "            epoch_time = ftime(epoch_time_start, epoch_time_end)\n",
    "            epoch_time = f\"epoch time: {epoch_time}\" + (\"\\n\" if no_progress else \"\")\n",
    "            not is_test and eprint(e,epoch_time.rjust(len(epoch_time) + r_just_val + 3)+\"\\n\")\n",
    "            \n",
    "            prog_bar_phase.close()\n",
    "            # CONTINUE LOOP ?\n",
    "            if continue_loop > 0 and \\\n",
    "                not (is_sanity_check or is_test or profiler) and \\\n",
    "                ((e + 1) % continue_loop == 0) and (e + 1 != epochs):\n",
    "                cont = input(\"continue loop ([y]/n): \")\n",
    "                not no_print and print()\n",
    "                if cont == 'n':\n",
    "                    break\n",
    "            \n",
    "            profiler and _profile_time(prof_epoch_inner,tpr(),f'epoch_inner') # ⏳\n",
    "\n",
    "        profiler and _profile_time(prof_epoch_loop,tpr(),f'epoch_loop') # ⏳\n",
    "        # EPOCH LOOP - END \n",
    "        \n",
    "        prof_postloop = tpr()\n",
    "        post and self.postloop(locals())\n",
    "        post and profiler and _profile_time(prof_postloop, tpr(), 'postloop') # ⏳\n",
    "        \n",
    "        # PRINT FINAL METRICS\n",
    "        eprint(0,\"-\"*r_just_val)\n",
    "        total_time_end = tpe()\n",
    "        total_time = ftime(total_time_start,total_time_end)\n",
    "        eprint(0, f\"\\ntotal time: {total_time}\\n\")\n",
    "        if self.criteria is not None and not is_test and self._VA in dl:\n",
    "            eprint(0, f\"best score: {self.best_score:0.4f}\\n\")\n",
    "\n",
    "        # RESTORE BEST MODEL\n",
    "        prof_restore_model = tpr() # ⏳\n",
    "        if load_best or profiler or is_sanity_check:\n",
    "            self.__load_model(self._BS)\n",
    "        profiler and _profile_time(prof_restore_model,tpr(),f'restore model') # ⏳\n",
    "\n",
    "        self.metrics._complete_run((is_sanity_check or profiler),is_test)\n",
    "        profiler and _profile_time(prof_total,tpr(),f'total',) # ⏳\n",
    "        # __loop - END \n",
    "\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    SECTION: 3 - A\n",
    "    \n",
    "    Loop methods for training and testing of the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def fit(self, \n",
    "            epochs:int=1, print_every:int=1,\n",
    "            display_metrics:Optional[Union[str,List[str]]]=None,\n",
    "            train_dl:Optional[DataLoader]=None,\n",
    "            valid_dl:Optional[DataLoader]=None,\n",
    "            track_batch_metrics:bool=True, load_best:bool=True,\n",
    "            continue_loop:int=0, define_all:bool=False,  \n",
    "            no_print:bool=False, no_cast:bool=False,\n",
    "            no_float:bool=False, no_progress:bool=False,\n",
    "           ) -> None:\n",
    "        \"\"\"\n",
    "        Runs the training loop for `epochs`\n",
    "        ----\n",
    "        PARAMETERS\n",
    "         - epochs : should be a non negative integer\n",
    "         - print_every : if 0 will not print, else will print at given epoch\n",
    "         - display_metrics : List of metrics returned in the epoch_end stage rdict that has to be \n",
    "             displayed, if None (default) all the returned metrics are displayed.\n",
    "         - train_dl : Will use this instead of DatLoader passed in the constructor call.\n",
    "         - valid_dl : Will use this instead of DatLoader passed in the constructor call.\n",
    "         - track_batch_metrics : whether to store the values returned in the batch steps\n",
    "         - load_best : whether to load the best model after training, works only if validation\n",
    "             parameters are defined `valid_dl`, `valid_step`, `valid_epoch_end`\n",
    "         -  continue_loop : Will ask whether to continue training after `continue` epochs; should\n",
    "             be a positive integer.\n",
    "         - define_all : If True then `torch.set_grad_enabled`, `optimizer.zero_grad` and model mode \n",
    "             ie [train,eval] have to be called where required (usually in the `train_step` function).\n",
    "         - no_print : If True will suppress all print statements, can be used when custom logging is\n",
    "             used in the stage functions.\n",
    "         - no_cast : True, if data casting has to be manually set in the stage functions\n",
    "         - no_float : True, don't apply float conversion to returned metrics.\n",
    "         - no_progress : True, don't show the progress bar.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.__loop(epochs=epochs, print_every=print_every,\n",
    "                    display_metrics=display_metrics, track_batch_metrics=track_batch_metrics,\n",
    "                    load_best=load_best, continue_loop=continue_loop, define_all=define_all,\n",
    "                    no_print=no_print, no_cast=no_cast, no_float=no_float, no_progress=no_progress,\n",
    "                    train_dl=train_dl, valid_dl=valid_dl\n",
    "                   )\n",
    "    \n",
    "    def train(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Alias for FitLoop.fit\n",
    "        \"\"\"\n",
    "        self.fit(*args, **kwargs)\n",
    "    \n",
    "    def test(self, test_dl:Optional[DataLoader]=None,\n",
    "            no_print:bool=False, no_cast:bool=False, no_float:bool=False, \n",
    "            ) -> None:\n",
    "        \"\"\"\n",
    "        For model testing. Runs loop for one epoch using test DataLoader and test stage functions.\n",
    "        ----\n",
    "        PARAMETERS\n",
    "         - test_dl : Will use this instead of DatLoader passed in the constructor call.\n",
    "         - no_print : If True will suppress all print statements, can be used when custom logging is\n",
    "             used in the stage functions.\n",
    "         - no_cast : True, if model and data casting has to be manually set in the stage functions\n",
    "         - no_float : True don't apply float conversion to returned metrics.\n",
    "         - no_progress : True, don't show the progress bar.\n",
    "        \"\"\"\n",
    "        self.__loop(is_test=True, no_print=no_print, no_cast=no_cast, no_float=no_float, test_dl=test_dl)\n",
    "        \n",
    "    \"\"\"\n",
    "    SECTION: 3 - B\n",
    "    \n",
    "    Loop methods for (sort of) unit testing and timing of components.\n",
    "    \"\"\"\n",
    "    def run_profiler(self,\n",
    "            epochs:Optional[int]=1, steps: Optional[int]=None, define_all:bool=False,\n",
    "            no_cast:bool=False, no_float:bool=False, no_progress:bool=False, print_outcome:bool=True,\n",
    "            train_dl:Optional[DataLoader]=None,\n",
    "            valid_dl:Optional[DataLoader]=None,\n",
    "            test_dl:Optional[DataLoader]=None\n",
    "            ) -> Dict[str,Union[Dict[str,List[float]],List[float]]]:\n",
    "        \"\"\"\n",
    "        Runs the loop in profiler mode, ie run all three (train, valid, test) phases \n",
    "        (if set) for given number of epochs and steps and print the average time taken \n",
    "        at different stages, loop output is not printed.\n",
    "        \n",
    "        Returns the time_profile dict.\n",
    "        \n",
    "        Criteria based checkpointing is not run, ie best_model and best_score are not saved.\n",
    "        Model state is not altered (it's reloaded) if the profiler is not interrupted.\n",
    "        ----\n",
    "        PARAMETERS\n",
    "         - epochs : should be a non negative integer\n",
    "         - steps : number of batches to iterate over in each phase [train,valid,test] \n",
    "             to check if everything is working as expected, if None then all batches are\n",
    "             iterated over.\n",
    "         - define_all : If True then `torch.set_grad_enabled`, `optimizer.zero_grad` and model mode \n",
    "             ie [train,eval] have to be called where required (usually in the `train_step` function).\n",
    "         - no_cast : True, if data casting has to be manually set in the stage functions\n",
    "         - no_float : True don't apply float conversion to returned metrics.\n",
    "         - no_progress : True, don't show the progress bar.\n",
    "         - print_outcome : If False won't print profiler outcome, can be accesed from FitLoop.time_profile\n",
    "         - train_dl : Will use this instead of DatLoader passed in the constructor call.\n",
    "         - valid_dl : Will use this instead of DatLoader passed in the constructor call.\n",
    "         - test_dl : Will use this instead of DatLoader passed in the constructor call.\n",
    "        \"\"\"\n",
    "        t1 = time.perf_counter()\n",
    "        self.__loop(epochs=epochs,steps=steps, define_all=define_all, no_cast=no_cast, \n",
    "                    no_float=no_float, no_print=True, no_progress=no_progress, \n",
    "                    train_dl=train_dl, valid_dl=valid_dl, profiler=True)\n",
    "        if self.test_dl is not None or test_dl is not None:\n",
    "            self.__loop(steps=steps, define_all=define_all, no_cast=no_cast, \n",
    "                        no_float=no_float, no_print=True, no_progress=no_progress, profiler=True, \n",
    "                        test_dl=test_dl, is_test=True)\n",
    "        st = ptime(time.perf_counter() - t1)\n",
    "        if print_outcome:\n",
    "            self.print_time_profile()\n",
    "            time_profile = self.time_profile\n",
    "            self.time_profile = {}\n",
    "            print(f\"\\ntotal time: {st}\")\n",
    "            return time_profile\n",
    "        else:\n",
    "            return self.time_profile\n",
    "    \n",
    "    def run_sanity_check(self, epochs:int=1, \n",
    "            steps:int=3, print_every:int=1, use_test_dl=False,\n",
    "            display_metrics:Optional[Union[str,List[str]]]=None,\n",
    "            continue_loop:int=0, define_all:bool=False,  \n",
    "            no_print:bool=False, no_cast:bool=False,\n",
    "            no_float:bool=False, no_progress:bool=False,\n",
    "            train_dl:Optional[DataLoader]=None,\n",
    "            valid_dl:Optional[DataLoader]=None,\n",
    "            test_dl:Optional[DataLoader]=None\n",
    "           ) -> None:\n",
    "        \"\"\"\n",
    "        Runs the loop in sanity check mode, ie all three (train, valid, test) phases \n",
    "        (if set) for given number of epochs and steps.\n",
    "        Criteria based checkpointing is not run, ie best_model and best_score are not saved.\n",
    "        Model state is not altered (it's reloaded) if the sanity check is not interrupted.\n",
    "        ----\n",
    "        PARAMETERS\n",
    "         - epochs : should be a non negative integer\n",
    "         - steps : number of batches to run in each phase [train,valid] \n",
    "             for check if everything is working, if None all batches are iterated over.\n",
    "         - use_test_dl : If False will use the validation DataLoader for the test phase,\n",
    "             else will use the test DataLoader.\n",
    "         - print_every : if 0 will not print, else will print at given epoch\n",
    "         - display_metrics : List of metrics returned in the epoch_end stage rdict that has to be \n",
    "             displayed, if None (default) all the returned metrics are displayed.\n",
    "         -  continue_loop : Will ask whether to continue training after `continue` epochs, should\n",
    "             be a positive integer.\n",
    "         - define_all : If True then `torch.set_grad_enabled`, `optimizer.zero_grad` and model mode \n",
    "             ie [train,eval] have to be called where required (usually in the `train_step` function).\n",
    "         - no_print : If True will suppress all print statements, can be used when custom logging is\n",
    "             used in the stage functions.\n",
    "         - no_cast : True, if data casting has to be manually set in the stage functions\n",
    "         - no_float : True don't apply float conversion to returned metrics.\n",
    "         - no_progress : True, don't show the progress bar.\n",
    "         - train_dl : Will use this instead of DatLoader passed in the constructor call.\n",
    "         - valid_dl : Will use this instead of DatLoader passed in the constructor call.\n",
    "         - test_dl : Will use this instead of DatLoader passed in the constructor call.\n",
    "        \"\"\"\n",
    "    \n",
    "        \n",
    "        print(f\"RUNNING SANITY CHECK: TRAIN LOOP - {epochs} EPOCH(s), {steps} STEP(s)\")\n",
    "        self.__loop(epochs=epochs, steps=steps, print_every=print_every, \n",
    "                    display_metrics=display_metrics, continue_loop=continue_loop,\n",
    "                    define_all=define_all, no_print=no_print, no_cast=no_cast, \n",
    "                    no_float=no_float, no_progress=no_progress, \n",
    "                    train_dl=train_dl,valid_dl=valid_dl,\n",
    "                    is_sanity_check=True)\n",
    "        if self.test_dl is not None or test_dl is not None:\n",
    "            print()\n",
    "            print(f\"RUNNING SANITY CHECK: TEST LOOP - {steps} STEP(s)\")\n",
    "            self.__loop(use_test_dl=use_test_dl, epochs=epochs, steps=steps, print_every=print_every, \n",
    "                        display_metrics=display_metrics, continue_loop=continue_loop,\n",
    "                        define_all=define_all, no_print=no_print, no_cast=no_cast, \n",
    "                        no_float=no_float, no_progress=no_progress, \n",
    "                        test_dl=test_dl,\n",
    "                        is_sanity_check=True, is_test=True)\n",
    "    \n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    SECTION: 4\n",
    "    \n",
    "    Functions to preserve the model state.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __save_model(self, typ:str) -> None:\n",
    "        \"\"\"\n",
    "        Save model to object or to the disk.\n",
    "        \"\"\"\n",
    "        name = self.best_model_name if typ == self._BS else self.pretrained_model_name\n",
    "        path = self.save_path/ name\n",
    "        state_dict = deepcopy(self.model.state_dict())\n",
    "        if self.save_to_disk:\n",
    "            torch.save(state_dict, path)\n",
    "            \n",
    "        elif typ == self._BS:\n",
    "            self.best_model_state_dict = state_dict\n",
    "        elif typ == self._PR:\n",
    "            self.pretrained_model_state_dict = state_dict\n",
    "        else:\n",
    "            logging.warning(\"model save failed\")\n",
    "        \n",
    "    def __load_model(self, typ:str):\n",
    "        \"\"\"\n",
    "        Load model from the object or from the disk.\n",
    "        \"\"\"\n",
    "        name = self.best_model_name if typ == self._BS else self.pretrained_model_name\n",
    "        path = self.save_path/ name\n",
    "        if self.save_to_disk:\n",
    "            state_dict = torch.load(path, map_location=self.device)\n",
    "        elif typ == self._BS:\n",
    "            state_dict = self.best_model_state_dict\n",
    "        else:\n",
    "            state_dict = self.pretrained_model_state_dict\n",
    "        self.model.load_state_dict(state_dict)\n",
    "        if self.configure_optimizer is None:\n",
    "            print(\"please reconfigure FitLoop.optimizer before training\")\n",
    "        else:\n",
    "            self.configure_optimizer(self)\n",
    "    \n",
    "    def reset(self, reset_model:bool=True) -> None:\n",
    "        \"\"\"\n",
    "        Resets FitLoop to initial state.\n",
    "        Parameters reset:\n",
    "            - model, to pretrained state if `reset_model`\n",
    "            - epoch_num, to 0\n",
    "            - best_score to ∓inf\n",
    "        FitLoop.optimizer param groups will have to be set again\n",
    "        \"\"\"\n",
    "        if reset_model:\n",
    "            self.__load_model(self._PR)\n",
    "        self.epoch_num = 0\n",
    "        self.best_score = self.criteria_direction * float('-inf')\n",
    "        self.metrics = None\n",
    "        \n",
    "        \n",
    "    # ---------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    SECTION: 5\n",
    "    \n",
    "    Functions to preserve the FitLoop object state so that training can be resumed.\n",
    "    \"\"\"\n",
    "    \n",
    "    def save(self, path, only_model=False):\n",
    "        \"\"\"\n",
    "        TODO : save the FitLoop state, if only_model then save only model.\n",
    "        \"\"\"\n",
    "        print(\"NOT IMPLEMENTED YET\")\n",
    "        pass\n",
    "    \n",
    "    def load(self, path):\n",
    "        \"\"\"\n",
    "        TODO : load the FitLoop state, if only model then load the model \n",
    "            state dict.\n",
    "        \"\"\"\n",
    "        print(\"NOT IMPLEMENTED YET\")\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    SECTION: 6\n",
    "    \n",
    "    Functions to delete stored model weights.\n",
    "    \"\"\"\n",
    "    \n",
    "    def del_pretrained(self) -> None:\n",
    "        \"\"\"\n",
    "        Deletes the pretrianed model state dict from the disk if \n",
    "        `save_to_disk` else states attribute to None\n",
    "        \"\"\"\n",
    "        if self.save_to_disk:\n",
    "            (self.save_path/self.pretrained_model_name).unlink()\n",
    "        else:\n",
    "            self.pretrained_model_state_dict = None\n",
    "        \n",
    "    def del_best_model(self) -> None:\n",
    "        \"\"\"\n",
    "        Deletes the best model state dict from the disk if \n",
    "        `save_to_disk` else states attribute to None\n",
    "        \"\"\"\n",
    "        if self.save_to_disk:\n",
    "            (self.save_path/self.best_model_name).unlink()\n",
    "        else:\n",
    "            self.best_model_state_dict = None\n",
    "            \n",
    "    # ---------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    SECTION: 7\n",
    "    \n",
    "    Getters for metrics\n",
    "    \"\"\"\n",
    "    @property\n",
    "    def M(self):\n",
    "        return self.metrics\n",
    "    \n",
    "    @property\n",
    "    def train_metrics(self):\n",
    "        return self.metrics.train\n",
    "    \n",
    "    @property\n",
    "    def valid_metrics(self):\n",
    "        return self.metrics.valid\n",
    "    \n",
    "    @property\n",
    "    def test_metrics(self):\n",
    "        return self.metrics.test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T00:30:42.867191Z",
     "start_time": "2020-05-07T00:30:42.862331Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim, nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.datasets import FakeData\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Stage Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Batch Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T05:17:38.943815Z",
     "start_time": "2020-05-07T05:17:38.930490Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def common_step(state):\n",
    "#     print(f\"{state.phase}_step, bn: {state.batch_num} en: {state.epoch_num}, \",end=\"\")\n",
    "    X, y = state.batch\n",
    "    y_ = state.model(X)\n",
    "    loss = state.loss_function(y_,y)\n",
    "#     print(\"loss\",loss.item())\n",
    "    r_loss = loss.item() * state.batch_size\n",
    "    r_corr = (y_.argmax(dim=1) == y).sum().item()\n",
    "    return loss, r_loss, r_corr\n",
    "\n",
    "def train_step(state):\n",
    "    loss, r_loss, r_corr = common_step(state)\n",
    "    loss.backward()\n",
    "    state.optimizer.step()\n",
    "    return {'r_loss':r_loss,'r_corr':r_corr}\n",
    "\n",
    "def valid_step(state):\n",
    "    loss, r_loss, r_corr = common_step(state)\n",
    "    return {'r_loss':r_loss,'r_corr':r_corr}\n",
    "\n",
    "def test_step(state):\n",
    "    loss, r_loss, r_corr = common_step(state)\n",
    "    return {'r_loss':r_loss,'r_corr':r_corr}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Epoch Start Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T02:30:23.312588Z",
     "start_time": "2020-05-07T02:30:23.298636Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def common_epoch_start(state):\n",
    "#     print(f\"\\n{state.phase}_epoch_start, # {state.epoch_num}\")\n",
    "    return {'dummy':'dict'}\n",
    "    \n",
    "def train_epoch_start(state):\n",
    "    return common_epoch_start(state)\n",
    "\n",
    "def valid_epoch_start(state):\n",
    "    return common_epoch_start(state)\n",
    "\n",
    "def test_epoch_start(state):\n",
    "    return common_epoch_start(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Epoch End Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T02:30:24.831128Z",
     "start_time": "2020-05-07T02:30:24.819422Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def common_epoch_end(state):\n",
    "#     print(f\"{state.phase}_epoch_end, # {state.epoch_num}\")\n",
    "    r_loss = state['r_loss']\n",
    "    r_corr = state['r_corr']\n",
    "    \n",
    "#     print('r_loss len',len(r_loss))\n",
    "#     print('r_corr len',len(r_corr))\n",
    "    \n",
    "    e_loss = r_loss.sum()/state.size\n",
    "    e_accu = r_corr.sum()/state.size\n",
    "    \n",
    "#     print('loss',e_loss)\n",
    "#     print('accu',e_accu)\n",
    "    \n",
    "    return {'loss':e_loss, 'accu':e_accu}\n",
    "    \n",
    "def train_epoch_end(state):\n",
    "    return common_epoch_end(state)\n",
    "\n",
    "def valid_epoch_end(state):\n",
    "    return common_epoch_end(state)\n",
    "\n",
    "def test_epoch_end(state):\n",
    "    return common_epoch_end(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FitLoop - Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1461,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T19:26:17.574444Z",
     "start_time": "2020-05-08T19:26:09.931432Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Basic setup with FakeData for testing.\n",
    "\"\"\"\n",
    "\n",
    "def get_dl(batch_size=4, sz=[100,20,30]):\n",
    "    sets = ['train','valid','test']\n",
    "    TR, VA, TE = sets\n",
    "    class_names = ['a','b','c','d']\n",
    "    num_classes = 4\n",
    "\n",
    "    sz = {s:z for s, z in zip(sets,sz)} # a multiple of batch size\n",
    "    ds = {s:FakeData(size=sz[s], transform=ToTensor(), num_classes=num_classes) for s in sets}\n",
    "    dl = {s:DataLoader(ds[s],batch_size=batch_size) for s in ds}\n",
    "    return dl\n",
    "\n",
    "model = resnet18()\n",
    "in_features = model.fc.in_features\n",
    "model.fc = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    \n",
    "dummy = nn.Sequential(\n",
    "    nn.Conv2d(3,1,3,10),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(529,4)\n",
    ")\n",
    "\n",
    "model = dummy\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# ----------------------------------------------\n",
    "\n",
    "def configure_optimizer(self):\n",
    "    parameters = self.model.parameters()\n",
    "    self.optimizer.param_groups.clear()\n",
    "    self.optimizer.add_param_group({'params': parameters})\n",
    "    \n",
    "dl = get_dl(batch_size=5,sz=[523,121,232])\n",
    "fl_dict = {\n",
    "    \"model\": model,\n",
    "    \"optimizer\": optimizer,\n",
    "    \"loss_function\": loss_function,\n",
    "    \"train_dl\":dl[TR],\n",
    "    \"valid_dl\":dl[VA],\n",
    "    \"test_dl\":dl[TE],\n",
    "    \"train_step\":train_step,\n",
    "    \"valid_step\":valid_step,\n",
    "    \"test_step\":test_step,\n",
    "    \"train_epoch_start\":train_epoch_start,\n",
    "    \"valid_epoch_start\":valid_epoch_start,\n",
    "    \"test_epoch_start\":test_epoch_start,\n",
    "    \"train_epoch_end\":train_epoch_end,\n",
    "    \"valid_epoch_end\":valid_epoch_end,\n",
    "    \"test_epoch_end\":test_epoch_end,\n",
    "    \"configure_optimizer\":configure_optimizer,\n",
    "    \"criteria\": \"accu\"\n",
    "}\n",
    "\n",
    "trainer = FitLoop(**fl_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1423,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T16:39:41.429869Z",
     "start_time": "2020-05-08T16:39:35.503681Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ceea2bea51d4316a48ae3d68fc0b027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='EPOCH :', layout=Layout(flex='2'), max=2.0, style=Progres…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=130.0), HTML(value='')), layout=Layout(di…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/2] - train :: loss: 1.3984 | accu: 0.2428 \n",
      "        valid :: loss: 1.3488 | accu: 0.3719 \n",
      "        epoch time: 02 s 996 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=130.0), HTML(value='')), layout=Layout(di…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/2] - train :: loss: 1.3490 | accu: 0.3881 \n",
      "        valid :: loss: 1.2926 | accu: 0.6198 \n",
      "        epoch time: 02 s 891 ms\n",
      "\n",
      "-----\n",
      "total time: 05 s 922 ms\n",
      "best score: 0.6198\n"
     ]
    }
   ],
   "source": [
    "# trainer.fit(epochs=3, train_dl=dl[TR], valid_dl=dl[VA])\n",
    "trainer.fit(epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T04:47:36.506922Z",
     "start_time": "2020-05-08T04:47:36.501882Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer.metrics.test.epoch_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1367,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T16:20:32.233221Z",
     "start_time": "2020-05-08T16:20:31.078400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dff6039b9d25420cad0406e01b83cdde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=47.0), HTML(value='')), layout=Layout(dis…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test :: loss: 1.2006 | accu: 0.7414 \n",
      "\n",
      "-----\n",
      "total time: 01 s 151 ms\n"
     ]
    }
   ],
   "source": [
    "trainer.test(test_dl=dl[TE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1383,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T16:25:29.315033Z",
     "start_time": "2020-05-08T16:25:25.202483Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING SANITY CHECK: TRAIN LOOP - 1 EPOCH(s), None STEP(s)\n",
      "{'train': <torch.utils.data.dataloader.DataLoader object at 0x1b6e70690>, 'valid': <torch.utils.data.dataloader.DataLoader object at 0x1b6e70210>}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5865a6bb43645349f973805c2fc87a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='EPOCH :', layout=Layout(flex='2'), max=1.0, style=Progres…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=130.0), HTML(value='')), layout=Layout(di…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1] - train :: loss: 1.4023 | accu: 0.2256 \n",
      "        valid :: loss: 1.3285 | accu: 0.4793 \n",
      "        epoch time: 03 s 019 ms\n",
      "\n",
      "-----\n",
      "total time: 03 s 058 ms\n",
      "best score: -inf\n",
      "\n",
      "RUNNING SANITY CHECK: TEST LOOP - None STEP(s)\n",
      "{'test': <torch.utils.data.dataloader.DataLoader object at 0x1b6e70650>}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b5433adc1be47139a5b06fefe2dfb1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=47.0), HTML(value='')), layout=Layout(dis…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test :: loss: 1.3985 | accu: 0.2457 \n",
      "\n",
      "-----\n",
      "total time: 01 s 046 ms\n"
     ]
    }
   ],
   "source": [
    "# trainer.run_sanity_check(use_test_dl=True,steps=None, train_dl=dl[TR], valid_dl=dl[VA], test_dl=dl[TE])\n",
    "trainer.run_sanity_check(use_test_dl=True,steps=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1392,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T16:27:49.975067Z",
     "start_time": "2020-05-08T16:27:45.747724Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING PROFILER: TRAIN LOOP 1 EPOCH(s)\n",
      "  train dl :: batches:  105 batch_size:    5 last_batch:    3 dataset_size:    523\n",
      "  valid dl :: batches:   25 batch_size:    5 last_batch:    1 dataset_size:    121\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cd800120ebb4efaacc61b9806c5bd7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='EPOCH :', layout=Layout(flex='2'), max=1.0, style=Progres…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=130.0), HTML(value='')), layout=Layout(di…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RUNNING PROFILER: TEST LOOP \n",
      "  test  dl :: batches:   47 batch_size:    5 last_batch:    2 dataset_size:    232\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973f84dc75c54b7e9e029261a2a95d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=47.0), HTML(value='')), layout=Layout(dis…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AVERAGE TIMES\n",
      "1. initialize:       00 s 006 ms 197 us\n",
      "2. train\n",
      "  1. epoch_start:    00 s 206 ms 300 us\n",
      "  2. step:           00 s 001 ms 730 us\n",
      "  3. batch_inner:    00 s 002 ms 055 us\n",
      "  4. batch_loop:     02 s 566 ms 521 us\n",
      "  5. epoch_end:      00 s 000 ms 113 us\n",
      "  6. phase_inner:    02 s 566 ms 750 us\n",
      "3. valid\n",
      "  1. epoch_start:    00 s 688 ms 999 us\n",
      "  2. step:           00 s 000 ms 541 us\n",
      "  3. batch_inner:    00 s 000 ms 847 us\n",
      "  4. batch_loop:     00 s 528 ms 868 us\n",
      "  5. epoch_end:      00 s 000 ms 273 us\n",
      "  6. phase_inner:    00 s 529 ms 221 us\n",
      "4. phase loop:       03 s 127 ms 263 us\n",
      "5. epoch\n",
      "  1. inner:          03 s 127 ms 738 us\n",
      "  2. loop:           03 s 159 ms 707 us\n",
      "  3. inner_t:        01 s 052 ms 351 us\n",
      "  4. loop_t:         01 s 052 ms 542 us\n",
      "6. restore model:    00 s 000 ms 183 us\n",
      "7. total:            03 s 166 ms 137 us\n",
      "8. initialize_t:     00 s 001 ms 523 us\n",
      "9. test\n",
      "  1. epoch_start_t:  00 s 236 ms 700 us\n",
      "  2. step_t:         00 s 000 ms 526 us\n",
      "  3. batch_inner_t:  00 s 000 ms 950 us\n",
      "  4. batch_loop_t:   01 s 013 ms 210 us\n",
      "  5. epoch_end_t:    00 s 183 ms 999 us\n",
      "  6. phase_inner_t:  01 s 013 ms 387 us\n",
      "10. phase loop_t:    01 s 049 ms 890 us\n",
      "11. restore model_t: 00 s 000 ms 137 us\n",
      "12. total_t:         01 s 054 ms 236 us\n",
      "\n",
      "total time: 04 s 220 ms 728 us\n"
     ]
    }
   ],
   "source": [
    "# _ = trainer.run_profiler(epochs=2, steps=None, train_dl=dl[TR], valid_dl=dl[VA], test_dl=dl[TE])\n",
    "_ = trainer.run_profiler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FitLoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bare minimum to structure and push to Pypi:\n",
    "    - Test on console.\n",
    "    - Metrics plot\n",
    "    - FitLoopDefaults (really simple ones)\n",
    "    - Onchanging model update parameters appropriately\n",
    "    - ✅All stage functions except for `train_step` should be optional that's the only one that is required for training the model, rest all are for metric keeping.\n",
    "    - ✅Make it easy to train, validate, test with some other DataLoader that is not attached to the object.\n",
    "    - ✅Fix No Continue\n",
    "    - ✅Metrics\n",
    "- `FitLoop.save(path:str)` to save the model and training state somehow even the FitLoop state.\n",
    "- `FitLoop.load(path:str)` to load the FitLoop state from given path.\n",
    "- ✅`FitLoop.profiler()` mode to capture all stage timings and maybe even CPU, GPU, RAM usage to check for bottlenecks and usage spikes, to be used with timed_test.\n",
    "- ✅if `FitLoop.fit(define_all:bool=False)` the zero_grad and the context manager are not auto set.\n",
    "- ✅should keep track of epochs that have been completed\n",
    "- ✅epoch_number can be reset \n",
    "- ✅`FitLoop.metrics.set_name.loop_stage['metric_name']` to access the metric\n",
    "- ✅`FitLoop.store_pretrained:bool` arg to store the pretrained weights before training\n",
    "    if path then store at given path else store in memory.\n",
    "- ✅`FitLoop.reset(reset_model:bool)` to clear metrics, epoch_num and to reset the model, to pretrained state\n",
    "    will load the weight from passed path else from memory.\n",
    "- ✅`FitLoop.fit(continue_loop:int=0)` ask after `int` whether to continue training or to end.\n",
    "- ✅`FitLoop.fit(profiler:bool=False)` mode to capture all stage timings and maybe even CPU, GPU, RAM usage to check for bottlenecks and usage spikes, to be used with timed_test.\n",
    "- ✅Functionality to view the metrics.\n",
    "- ✅Model score should be a loop instance so that the best model may not be erased.\n",
    "- ✅Time keeping/ metric keeping:\n",
    "    - General\n",
    "        - ✅Metrics returned in the batch step\n",
    "        - ✅Metrics returned in the end step\n",
    "        - ✅Progress bar for epoch\n",
    "        - ✅Progress bar for batch that disappears after complete\n",
    "        - ✅Epoch timing (for both phases when training)\n",
    "        - ✅Total timing \n",
    "        \n",
    "    - Profiler Mode:\n",
    "        - ✅Individual Stage Timings\n",
    "        \n",
    "- ✅Check with uneven batchsizes.\n",
    " \n",
    "#### Later Incremental Addons\n",
    "- Profiler Mode:\n",
    "    - Use an actual profiler ie: `cProfile`\n",
    "    - Individual Stage CPU Usage\n",
    "    - Individual Stage GPU Usage\n",
    "    - Individual Stage RAM Usage\n",
    "- Use better logging (maybe) `warnings` and `logging`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FitLoopDefaults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Basic functions for\n",
    "    - train_step\n",
    "    - valid_step\n",
    "    - test_step\n",
    "- Basic funtions for \n",
    "    - train_epoch_end\n",
    "    - valid_epoch_end\n",
    "    - test_epoch_end\n",
    "\n",
    "FitLoopDefaults shouldn't be a class, it should be a module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics/MetricsAggregator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Value access\n",
    "Fitloop.metrics.train['loss']                 # returns all losses from epoch end\n",
    "Fitloop.metrics.train.epoch_end['loss']       # ✅returns all losses from epoch end\n",
    "Fitloop.metrics.train.epoch_start['loss']     # ✅returns all losses from epoch start\n",
    "Fitloop.metrics.train.batch_step['loss']      # ✅returns all losses from batch step\n",
    " \n",
    "Fitloop.metrics.train['loss'][0]              # returns losses for run 0 from epoch end\n",
    "Fitloop.metrics.valid.batch_step['accu'][3]   # ✅returns all validation accuracies for batch step from run 3\n",
    " \n",
    "# Value visualization \n",
    "Fitloop.metrics.plot()                        # plots validation criteria against training criteria (eg accuracy)\n",
    "                                              # if criteria not available, then first key from rdict.\n",
    "Fitloop.metrics.train.plot()                  # if loss then loss else, plots first value from rdict\n",
    "Fitloop.metrics.train.plot('loss')            # plots loss \n",
    "```\n",
    "- ✅metrics can be cleared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### ✅LoopState "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- ✅should cast the batch to device before passing it using `state.batch()`\n",
    "- ✅should get the batch num `state.batch_num` and epoch num `state.epoch_num`\n",
    "- ✅the model, optimizer, loss_function, lr_scheduler should be available\n",
    "    `state.model`, `state.optimizer`, `state.loss_function`, `state.lr_scheduler`\n",
    "- ✅should return the batch metrics as float tensors using square bracket indexing\n",
    "    `state['loss']` \n",
    "    - every step function hook receives the LoopState object.\n",
    "- ✅The loop state object should have a copy of all the values returned from the function hook\n",
    "- ✅example the below returned dict values should be avialable in the LoopState object\n",
    "\n",
    "```python\n",
    "def train_step(state):\n",
    "    X,y = state() # should device cast automatically\n",
    "    y_ = state.model(X)\n",
    "    loss = state.loss_function(y_, y)\n",
    "    \n",
    "    state.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    state.optimizer.step()\n",
    "    state.lr_scheduler.step() \n",
    "    \n",
    "    loss = loss.item()\n",
    "    batch_loss = loss * y.size()\n",
    "    batch_corr = (y_.argmax(dim=0) == y).sum().float().item()\n",
    "    \n",
    "    return {'loss':loss,'batch_loss':batch_loss:'batch_corr'}\n",
    "```\n",
    "- ✅The LoopState object should be cleared of the above values at the start \n",
    "  of the next epoch.\n",
    "- ✅The returned values should be available through the FitLoop object\n",
    "  Eg: `FitLoop.metrics.train.batch['loss']`\n",
    "- ✅The (above statement) returned value should be optionally available by setting the flag \n",
    "  `track_batch_metrics`\n",
    "\n",
    "```python\n",
    "def train_epoch_end(state):\n",
    "    loss = state['loss']\n",
    "    batch_loss = state['batch_loss']\n",
    "    batch_corr = state['batch_corr']\n",
    "    \n",
    "    size = state.size\n",
    "    \n",
    "    epoch_loss = batch_loss.sum().item()/size\n",
    "    epoch_accu = batch_corr.sum().item()/size\n",
    "    \n",
    "    return {\"loss\":epoch_loss,\"accu\":epoch_accu}\n",
    "```\n",
    "\n",
    "- ✅The returned values should be available through the FitLoop object\n",
    "  Eg: `FitLoop.metrics.train.epoch['loss']`\n",
    "- ✅For each phase a different LoopState obect is maintained.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
