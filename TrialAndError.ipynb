{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Grounds - Fit Loop\n",
    "Creating a library that helps with the pytorch looping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T17:18:14.466907Z",
     "start_time": "2020-05-06T17:18:14.460137Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "from uuid import uuid4\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from tqdm.autonotebook import tqdm\n",
    "from typing import Union, List, Callable, Optional, Any, Dict, Tuple\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import _LRScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T00:26:27.425435Z",
     "start_time": "2020-05-06T00:26:27.418596Z"
    }
   },
   "outputs": [],
   "source": [
    "class A():\n",
    "    def __init__(self, f):\n",
    "        self.__f = f\n",
    "        self.a = 22\n",
    "    \n",
    "    def __getitem__(self, name):\n",
    "        return name\n",
    "        \n",
    "    def __getattr__(self,name):\n",
    "        return getattr(self.__f,name)\n",
    "class B():\n",
    "    def __init__(self):\n",
    "        self.b = 33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LoopState\n",
    "- ✅should cast the batch to device before passing it using `state.batch()`\n",
    "- ✅should get the batch num `state.batch_num` and epoch num `state.epoch_num`\n",
    "- ✅the model, optimizer, loss_function, lr_scheduler should be available\n",
    "    `state.model`, `state.optimizer`, `state.loss_function`, `state.lr_scheduler`\n",
    "- ✅should return the batch metrics as float tensors using square bracket indexing\n",
    "    `state['loss']` \n",
    "    - every step function hook receives the LoopState object.\n",
    "- ✅The loop state object should have a copy of all the values returned from the function hook\n",
    "- ✅example the below returned dict values should be avialable in the LoopState object\n",
    "\n",
    "```python\n",
    "def train_step(state):\n",
    "    X,y = state() # should device cast automatically\n",
    "    y_ = state.model(X)\n",
    "    loss = state.loss_function(y_, y)\n",
    "    \n",
    "    state.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    state.optimizer.step()\n",
    "    state.lr_scheduler.step() \n",
    "    \n",
    "    loss = loss.item()\n",
    "    batch_loss = loss * y.size()\n",
    "    batch_corr = (y_.argmax(dim=0) == y).sum().float().item()\n",
    "    \n",
    "    return {'loss':loss,'batch_loss':batch_loss:'batch_corr'}\n",
    "```\n",
    "- The LoopState object should be cleared of the above values at the start \n",
    "  of the next epoch.\n",
    "- The returned values should be available through the FitLoop object\n",
    "  Eg: `FitLoop.train.batch['loss']`\n",
    "- The (above statement) returned value should be optionally available by setting the flag \n",
    "  `store_batch_metrics`\n",
    "\n",
    "```python\n",
    "def train_epoch_end(state):\n",
    "    loss = state['loss']\n",
    "    batch_loss = state['batch_loss']\n",
    "    batch_corr = state['batch_corr']\n",
    "    \n",
    "    size = state.size\n",
    "    \n",
    "    epoch_loss = batch_loss.sum().item()/size\n",
    "    epoch_accu = batch_corr.sum().item()/size\n",
    "    \n",
    "    return {\"loss\":epoch_loss,\"accu\":epoch_accu}\n",
    "```\n",
    "\n",
    "- The returned values should be available through the FitLoop object\n",
    "  Eg: `FitLoop.train.epoch['loss']`\n",
    "- ✅For each phase a different LoopState obect is maintained.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T17:21:22.927136Z",
     "start_time": "2020-05-06T17:21:22.923214Z"
    }
   },
   "outputs": [],
   "source": [
    "class FitLoop:\n",
    "    # Dummy\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T00:44:16.153669Z",
     "start_time": "2020-05-07T00:44:16.148062Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = []\n",
    "\n",
    "a.append(2)\n",
    "a.append(3)\n",
    "\n",
    "a[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T00:49:31.559480Z",
     "start_time": "2020-05-07T00:49:31.528276Z"
    }
   },
   "outputs": [],
   "source": [
    "class LoopState:\n",
    "    \"\"\"\n",
    "    Maintains train/valid/test loop state for a single run of \n",
    "    a certain number of epochs, does not used to preserve state \n",
    "    between runs.\n",
    "    \"\"\"\n",
    "    _stages = ['batch','epoch_start','epoch_end']\n",
    "    _batch_step, _epoch_start, _epoch_end = _stages\n",
    "    def __init__(self, phase:str, floop:FitLoop, no_cast:bool, no_float:bool, is_train:bool, is_test:bool):\n",
    "        \"\"\"\n",
    "        phase : phase name 'train', 'valid' or 'test'\n",
    "        floop : the calling FitLoop object\n",
    "        \"\"\"\n",
    "        self.__batch = ()\n",
    "        self.__floop = floop\n",
    "        self._no_cast = no_cast\n",
    "        self._no_float = no_float\n",
    "        self.phase = phase\n",
    "        self.batch_num = 0\n",
    "        self.epoch_num = 0\n",
    "        self.metrics = {s:{} for s in self._stages}\n",
    "        self.is_train = is_train\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        # For easy access\n",
    "        dl = getattr(floop, f'{phase}_dl')\n",
    "        bs = dl.batch_size\n",
    "        dr = dl.drop_last\n",
    "        sz = len(dl.dataset)\n",
    "        bt = sz / bs\n",
    "        \n",
    "        # Gives dataset size and batch count\n",
    "        self.size = sz\n",
    "        self.batches = math.floor(bt) if dr else math.ceil(bt)\n",
    "        self.batch_size = 0\n",
    "    \n",
    "    def __getattr__(self, name:str) -> Any:\n",
    "        # To get attributes from the FitLoop object \n",
    "        # for use in the stage functions.\n",
    "        return getattr(self.__floop, name)\n",
    "    \n",
    "    def __getitem__(self, metric_name:str):\n",
    "        # To get the metrics stored in the batch step stage\n",
    "        metric_value = self.metrics[self._batch_step][metric_name]\n",
    "        try:\n",
    "            return torch.tensor(metric_value).float()\n",
    "        except:\n",
    "            return metric_value\n",
    "    \n",
    "    \"\"\"\n",
    "    Getter and setter for the current batch\n",
    "    \"\"\"\n",
    "    @property\n",
    "    def batch(self) -> Tuple[Tensor,...]:\n",
    "        if self._no_cast:\n",
    "            return self.__batch\n",
    "        \n",
    "        return (\n",
    "            d.to(device=self.device,dtype=self.dtype) \n",
    "            if d.is_floating_point() \n",
    "            else d.to(device=self.device,dtype=torch.long) \n",
    "            for d in self.__batch\n",
    "        )\n",
    "    \n",
    "    @batch.setter\n",
    "    def batch(self, current_batch:Tuple[Tensor,...]) -> None:\n",
    "        self.__batch = current_batch\n",
    "        \n",
    "    \"\"\"\n",
    "    Functions to append rdict values to self.metrics\n",
    "    \"\"\"\n",
    "    def _append(self, rdict:Dict[str, float], stage:str) -> None:\n",
    "        #  Append metrics to the specific stage.\n",
    "        if rdict is None:\n",
    "            if stage == self._epoch_end:\n",
    "                print(f\"no rdict returned from: f{self.phase}_{stage}\")\n",
    "            \"\"\"\n",
    "            TODO: Add warning if rdict of stage is None\n",
    "            \"\"\"\n",
    "            return\n",
    "        \n",
    "        for key in rdict:\n",
    "            if key not in self.metrics[stage]:\n",
    "                self.metrics[stage][key] = []\n",
    "            self.metrics[stage][key].append(rdict[key])\n",
    "            \n",
    "    def _append_batch_step(self, rdict:Dict[str, float]) -> None:\n",
    "        # Called after batch step rdict is returned\n",
    "        self._append(rdict, self._batch_step)\n",
    "        \n",
    "    def _append_epoch_start(self, rdict:Dict[str, float]) -> None:\n",
    "        # Called before epoch start\n",
    "        self._append(rdict, self._epoch_start)\n",
    "        \n",
    "    def _append_epoch_end(self, rdict:Dict[str, float]) -> None:\n",
    "        # Called after epoch end step rdict is returned\n",
    "        self._append(rdict, self._epoch_end)\n",
    "    \n",
    "        \n",
    "    \"\"\"\n",
    "    Functions to clear rdict values from self.metrics\n",
    "    \"\"\"\n",
    "    def _clear(self, stage:str) -> None:\n",
    "        # Clear the batch metrics at the end of the batch.\n",
    "        for mlist in self.metrics[stage]:\n",
    "            self.metrics[stage][mlist].clear()\n",
    "            \n",
    "    def _clear_batch_step(self) -> None:\n",
    "        # Called before epoch start\n",
    "        self._clear(self._batch_step)\n",
    "        \n",
    "    def _clear_epoch_start(self) -> None:\n",
    "        # Called ??\n",
    "        self._clear(self._epoch_start)\n",
    "        \n",
    "    def _clear_epoch_end(self) -> None:\n",
    "        # Called after loop end\n",
    "        self._clear(self._epoch_end)\n",
    "    \n",
    "    \"\"\"\n",
    "    State updates before epoch start and batch step stages\n",
    "    \"\"\"\n",
    "    def _pre_epoch_start_update(self, epoch_num:int) -> None:\n",
    "        self._clear_batch_step()\n",
    "        self.batch_num = 0\n",
    "        self.epoch_num = epoch_num\n",
    "    \n",
    "    def _pre_batch_step_update(self, current_batch):\n",
    "        self.batch_size = current_batch[0].size(0)\n",
    "        self.batch_num += 1\n",
    "        self.batch = current_batch\n",
    "    \n",
    "    \"\"\"\n",
    "    Functions to get various metrics at different stages \n",
    "    \"\"\"\n",
    "    def _get_epoch_metric(self, criteria:str) -> float:\n",
    "        # Last added metric that is to be used as a model \n",
    "        # selection criteria\n",
    "        metric = self.metrics[self._epoch_end][criteria][-1]\n",
    "        if self._no_float:\n",
    "            return metric\n",
    "        else:\n",
    "            return float(metric)\n",
    "    \n",
    "    def _get_epoch_metrics(self, \n",
    "                display_metrics:Optional[Union[str,List[str]]]=None\n",
    "                ) -> Dict[str,float]:\n",
    "        # Return the last saved epoch metrics\n",
    "        if isinstance(display_metrics, str):\n",
    "            return {display_metricss:self._get_epoch_metric(display_metrics)}\n",
    "        elif isinstance(display_metrics, list):\n",
    "            return {\n",
    "                metric:self._get_epoch_metric(metric)\n",
    "                for metric in display_metrics\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                metric: self._get_epoch_metric(metric)\n",
    "                for metric in self.metrics[self._epoch_end]\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FitLoop\n",
    "- all stage functions except for `train_step` should be optional that's the only one that is required for training the model, rest all are for metric keeping.\n",
    "- ✅if `FitLoop.fit(define_all:bool=False)` the zero_grad and the context manager are not auto set.\n",
    "- ✅should keep track of epochs that have been completed\n",
    "- ✅epoch_number can be reset \n",
    "- metrics can be cleared\n",
    "- `FitLoop.set_name.loop_stage['metric_name']` to access the metric\n",
    "- ✅`FitLoop.store_pretrained:bool` arg to store the pretrained weights before training\n",
    "    if path then store at given path else store in memory.\n",
    "- ✅`FitLoop.reset(reset_model:bool)` to clear metrics, epoch_num and to reset the model, to pretrained state\n",
    "    will load the weight from passed path else from memory.\n",
    "- `FitLoop.save(path:str)` to save the model and training state somehow even the fitloop state.\n",
    "- `FitLoop.load(path:str)` to load the FitLoop state from given path.\n",
    "- Some basic step should be used such that one can use it without defining step functions.\n",
    "- ✅`FitLoop.fit(continue_loop:int=0)` ask after `int` whether to continue training or to end.\n",
    "- ✅`FitLoop.fit(profiler:bool=False)` mode to capture all stage timings and maybe even CPU, GPU, RAM usage to check for bottlenecks and usage spikes, to be used with timed_test.\n",
    "- Make it easy to train,validate, test with some other DataLoader that is not attached to the object.\n",
    "- `FitLoop.fit(no_print:bool=False)` mode to capture all stage timings and maybe even CPU, GPU, RAM usage to check for bottlenecks and usage spikes, to be used with timed_test.\n",
    "- Use a loading bar for epoch and a disappearing one for batch.\n",
    "- Functionality to view the metrics.\n",
    "- ✅Model score should be a loop instance so that the best model may not be erased.\n",
    "- Time keeping/ metric keeping:\n",
    "    - Profiler:\n",
    "        - ✅Individual Stage Timings\n",
    "        - Individual Stage CPU Usage\n",
    "        - Individual Stage GPU Usage\n",
    "        - Individual Stage RAM Usage\n",
    "        - For these Stages:\n",
    "            - Batch step for each phase\n",
    "            - Epoch start step for each phase\n",
    "            - Epoch end step for each phase\n",
    "    - General\n",
    "        - Metrics returned in the batch step\n",
    "        - Metrics returned in the end step\n",
    "        - Progress bar for epoch\n",
    "        - Progress bar for batch that disappears after complete\n",
    "        - ✅ Epoch timing (for both phases when training)\n",
    "        - ✅ Total timing \n",
    "- ✅Check with uneven batchsizes.\n",
    "- TQDM \n",
    "- Metrics\n",
    "- FitLoopDefaults\n",
    "- Fix No Continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T17:48:33.456334Z",
     "start_time": "2020-05-06T17:48:33.451212Z"
    }
   },
   "outputs": [],
   "source": [
    "class FitLoopDefault:\n",
    "    def train_step(state):\n",
    "        print(\"default train_step\")\n",
    "        return {}\n",
    "\n",
    "    def valid_step(state):\n",
    "        \n",
    "        print(\"default valid_step\")\n",
    "        return {}\n",
    "\n",
    "    def test_step(state):\n",
    "        print(\"default test_step\")\n",
    "        return {}\n",
    "        \n",
    "    def train_epoch_end(state):\n",
    "        print(\"default train_epoch_end\")\n",
    "        return {}\n",
    "\n",
    "    def valid_epoch_end(state):\n",
    "        print(\"default valid_epoch_end\")\n",
    "        return {}\n",
    "\n",
    "    def test_epoch_end(state):\n",
    "        print(\"default test_epoch_end\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T18:02:54.574702Z",
     "start_time": "2020-05-06T18:02:54.572167Z"
    }
   },
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "    \"\"\"\n",
    "    Class to keep track of all the metrics and should have\n",
    "    visualization for the metrics.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1028,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T00:22:29.652941Z",
     "start_time": "2020-05-08T00:22:25.646511Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t2 cont\n",
      "\t5 cont\n",
      "\t8 cont\n",
      "\t11 cont\n",
      "\t14 cont\n",
      "\t17 cont\n",
      "\t20 cont\n"
     ]
    }
   ],
   "source": [
    "coninue_loop = 3\n",
    "for e in range(22):\n",
    "    if (e+1) % coninue_loop == 0:\n",
    "        input(f'\\t{e} cont')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FitLoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T05:41:41.920640Z",
     "start_time": "2020-05-07T05:41:41.913668Z"
    }
   },
   "outputs": [],
   "source": [
    "def ftime(t1:float,t2:float)->str:\n",
    "    t = t2-t1\n",
    "    gm = time.gmtime(t)\n",
    "    try:\n",
    "        ms = str(t).split('.')[1][:3]\n",
    "    except:\n",
    "        ms = \"\"\n",
    "    if gm.tm_hour > 0:\n",
    "        fstr = \"%H h %M m %S s\"\n",
    "    elif gm.tm_min > 0:\n",
    "        fstr = \"%M m %S s\"\n",
    "    else:\n",
    "        fstr = \"%S s\"\n",
    "    return time.strftime(fstr, gm) + f\" {ms} ms\"\n",
    "\n",
    "def ptime(t:float)->str:\n",
    "    gm = time.gmtime(t)\n",
    "    try:\n",
    "        fs = str(t).split('.')[1]\n",
    "        ms = fs[:3]\n",
    "        us = fs[3:6]\n",
    "    except:\n",
    "        ms = \"\"\n",
    "    if gm.tm_hour > 0:\n",
    "        fstr = \"%H h %M m %S s\"\n",
    "    elif gm.tm_min > 0:\n",
    "        fstr = \"%M m %S s\"\n",
    "    else:\n",
    "        fstr = \"%S s\"\n",
    "    return time.strftime(fstr, gm) + f\" {ms} ms {us} us\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1037,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T00:26:40.510514Z",
     "start_time": "2020-05-08T00:26:40.385523Z"
    }
   },
   "outputs": [],
   "source": [
    "class FitLoop:\n",
    "    \"\"\"\n",
    "    FitLoop trains Pytorch models.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    SECTION: 0 \n",
    "    \n",
    "    Initialization\n",
    "    \"\"\"\n",
    "    _sets = ['train','valid','test']\n",
    "    _TR, _VA, _TE = _sets\n",
    "    \n",
    "    _model_type = ['pretrained','best']\n",
    "    _PR, _BS = _model_type\n",
    "    def __init__(self, \n",
    "                 # Basic Blocks\n",
    "                 model: Module, \n",
    "                 optimizer: Union[Optimizer,List[Optimizer]], \n",
    "                 loss_function: Callable[[Tensor,Tensor],Tensor], \n",
    "                 \n",
    "                 # DataLoader\n",
    "                 train_dl: DataLoader, \n",
    "                 valid_dl: Optional[DataLoader]=None, \n",
    "                 test_dl: Optional[DataLoader]=None, \n",
    "                 \n",
    "                 # Batch Step\n",
    "                 train_step: Callable[[LoopState],Dict[str, Any]]=FitLoopDefault.train_step,\n",
    "                 valid_step: Optional[Callable[[LoopState],Dict[str, Any]]]=FitLoopDefault.valid_step,\n",
    "                 test_step: Optional[Callable[[LoopState],Dict[str, Any]]]=FitLoopDefault.test_step,\n",
    "                 \n",
    "                 # Epoch Start Step\n",
    "                 train_epoch_start: Optional[Callable[[LoopState],Dict[str, Any]]]=None,\n",
    "                 valid_epoch_start: Optional[Callable[[LoopState],Dict[str, Any]]]=None,\n",
    "                 test_epoch_start: Optional[Callable[[LoopState],Dict[str, Any]]]=None,\n",
    "                 \n",
    "                 # Epoch End Step\n",
    "                 train_epoch_end: Callable[[LoopState],Dict[str, Any]]=FitLoopDefault.train_epoch_end,\n",
    "                 valid_epoch_end: Optional[Callable[[LoopState],Dict[str, Any]]]=FitLoopDefault.valid_epoch_end,\n",
    "                 test_epoch_end: Optional[Callable[[LoopState],Dict[str, Any]]]=FitLoopDefault.test_epoch_end,\n",
    "                 \n",
    "                 # Other Stage Functions\n",
    "                 preloop: Optional[Callable[[dict],None]]=None,\n",
    "                 postloop: Optional[Callable[[dict],None]]=None,\n",
    "                 \n",
    "                 # Other Args\n",
    "                 lr_scheduler: Optional[Union[_LRScheduler, Any, List[Union[_LRScheduler,Any]]]]=None,\n",
    "                 device: torch.device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'), \n",
    "                 configure_optimizer:Callable[[FitLoop],None]=None,\n",
    "                 dtype: torch.dtype=torch.float32,\n",
    "                 \n",
    "                 # Model Evaluation\n",
    "                 criteria: Optional[str]=None,\n",
    "                 criteria_direction: int=1,\n",
    "                 \n",
    "                 # Preserving Model State\n",
    "                 save_to_disk: bool=False,\n",
    "                 save_path: str=\"models\",\n",
    "                 pretrained_model_name: Optional[str]=None,\n",
    "                 best_model_name: Optional[str]=None,\n",
    "                ) -> None:\n",
    "        \"\"\"\n",
    "        FitLoop constructor\n",
    "        ----\n",
    "        Parameters:\n",
    "        # Basic Blocks\n",
    "            The bare minimum required along with train_dl.\n",
    "            - model : nn.Module model that has to be trained\n",
    "            - optimizer : an optimizer from torch.optim\n",
    "            - loss_function : function to compute loss\n",
    "         \n",
    "        # DataLoader\n",
    "            - train_dl : training DataLoader\n",
    "            - valid_dl : validation DataLoader, if None validation will be ignored\n",
    "            - test_dl : testing DataLoader, if None `.test()` will not run\n",
    "         \n",
    "        # Batch Step\n",
    "            Functions that take in a LoopState object to perform \n",
    "            required calculations, functions should return a dict with values\n",
    "            to be used in the epoch end step.\n",
    "            - train_step : portion of the loop where forward and backward \n",
    "                passes take place.\n",
    "            - valid_step : validation portion of the loop.\n",
    "            - test_step : called when `FitLoop.test()` is called.\n",
    "        \n",
    "        # Epoch Start Step\n",
    "            - train_epoch_start : Train phase stage function in the epoch loop at the start.\n",
    "            - valid_epoch_start : Valid phase stage function in the epoch loop at the start.\n",
    "            - test_epoch_start : Test phase stage function in the epoch loop at the start.\n",
    "        \n",
    "        # Epoch End Step\n",
    "            Functions that take in a LoopState object to perform \n",
    "            required calculations, functions should return a dict with values\n",
    "            that are to be returned when the loop is over.\n",
    "            - train_epoch_end : after training epoch has ended.\n",
    "            - valid_epoch_end : after validation epoch has ended.\n",
    "            - test_epoch_end : called when the test loop is done, one iteration\n",
    "                over all batches in the test dataloader.\n",
    "                \n",
    "        # Other Stage Functions\n",
    "            - preloop : function that is called before the epoch loop runs, it is passed\n",
    "                all the loop variables (local()) in a dict.\n",
    "            - postloop : function that is called after the epoch loop runs, it is passed\n",
    "                all the loop variables (local()) in a dict.\n",
    "        \n",
    "        # Other Args\n",
    "            - lr_scheduler : scheduler from torch.optim.lr_scheduler\n",
    "            - device : torch.device model will be cast to device this prior to the loop\n",
    "            - configure_optimizer : function that configures the optimizer, will be called\n",
    "                whenever the model weights have to be restored.\n",
    "            - dtype : floating point dtype to cast model and data to\n",
    "            \n",
    "        # Model Evaluation\n",
    "            - criteria : model evaluation metric that is returned in the dict of the\n",
    "                `valid_epoch_end` stage function if None (default) best model and \n",
    "                best score are not tracked.\n",
    "            - criteria_direction : whether more is better (1) or less is better (-1) \n",
    "                for model score criteria.\n",
    "        \n",
    "        # Preserving Model State\n",
    "            - save_to_disk : True then save pretrained and best_model to the disk, else it is \n",
    "                stored as an attribute.\n",
    "            - save_path : location where the initial and pretrained models are to be saved\n",
    "            - pretrained_model_name : Name to save the pretrained model by\n",
    "            - best_model_name : Name to save the best model by\n",
    "        \"\"\"\n",
    "        # Basic Blocks\n",
    "        self.model = model.to(device=device, dtype=dtype)\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_function = loss_function\n",
    "        \n",
    "        # DataLoaders\n",
    "        self.train_dl = train_dl\n",
    "        self.valid_dl = valid_dl\n",
    "        self.test_dl = test_dl\n",
    "        \n",
    "        # Batch Step\n",
    "        self.train_step = train_step\n",
    "        self.valid_step = valid_step\n",
    "        self.test_step = test_step\n",
    "        \n",
    "        # Epoch Start Step\n",
    "        self.train_epoch_start = train_epoch_start\n",
    "        self.valid_epoch_start = valid_epoch_start\n",
    "        self.test_epoch_start = test_epoch_start\n",
    "        \n",
    "        # Epoch End Step\n",
    "        self.train_epoch_end = train_epoch_end\n",
    "        self.valid_epoch_end = valid_epoch_end\n",
    "        self.test_epoch_end = test_epoch_end\n",
    "        \n",
    "        # Other Stage Functions\n",
    "        self.preloop = preloop\n",
    "        self.postloop = postloop\n",
    "        \n",
    "        # Other Args\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.device = device\n",
    "        self.configure_optimizer = configure_optimizer\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        # Model Evaluation\n",
    "        self.criteria = criteria\n",
    "        self.criteria_direction = criteria_direction\n",
    "        \n",
    "        # Preserving Model State\n",
    "        if pretrained_model_name is None:\n",
    "            u = str(uuid4()).split('-')[1]\n",
    "            pretrained_model_name = f\"pretrained_{u}.pt\"\n",
    "        if best_model_name is None:\n",
    "            u = str(uuid4()).split('-')[1]\n",
    "            best_model_name = f\"best_{u}.pt\"\n",
    "        self.pretrained_model_name = pretrained_model_name\n",
    "        self.best_model_name = best_model_name\n",
    "        self.save_to_disk = save_to_disk\n",
    "        self.save_path = Path(save_path)\n",
    "        \n",
    "        # INITIALIZE NON ARGS\n",
    "        self.__save_model(self._PR)\n",
    "        self.epoch_num = 0\n",
    "        self.best_score = self.criteria_direction * float('-inf')\n",
    "        self.time_profile = {}\n",
    "        self.pretrained_model_state_dict = None\n",
    "        self.best_model_state_dict = None\n",
    "            \n",
    "            \n",
    "    # ---------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    SECTION: 1\n",
    "    \n",
    "    Helper functions used in `__loop`\n",
    "    \"\"\"\n",
    "        \n",
    "    \n",
    "    def __call_batch_step(self, state:LoopState) -> None:\n",
    "        step_funcs = [self.train_step, self.valid_step, self.test_step]\n",
    "        step_funcs = {s:f for s,f in zip(self._sets, step_funcs)}\n",
    "        step_func = step_funcs[state.phase]\n",
    "        \n",
    "        if step_func is None:\n",
    "            raise AttributeError(f\"{phase}_step not assigned\")\n",
    "        rdict = step_func(state)\n",
    "        if isinstance(rdict,dict):\n",
    "            state._append_batch_step(rdict)\n",
    "        \n",
    "    def __call_epoch_start_step(self, state:LoopState) -> None:\n",
    "        step_funcs = [self.train_epoch_start,self.valid_epoch_start,self.test_epoch_start]\n",
    "        step_funcs = {s:f for s,f in zip(self._sets, step_funcs)}\n",
    "        step_func = step_funcs[state.phase]\n",
    "        \n",
    "        if step_func is None:\n",
    "            return None\n",
    "        rdict = step_func(state)\n",
    "        if isinstance(rdict,dict):\n",
    "            state._append_epoch_start(rdict)\n",
    "        \n",
    "    def __call_epoch_end_step(self, state:LoopState) -> None:\n",
    "        step_funcs = [self.train_epoch_end,self.valid_epoch_end,self.test_epoch_end]\n",
    "        step_funcs = {s:f for s,f in zip(self._sets, step_funcs)}\n",
    "        step_func = step_funcs[state.phase]\n",
    "        \n",
    "        if step_func is None:\n",
    "            raise AttributeError(f\"{phase}_end_step not assigned\")\n",
    "        rdict = step_func(state)\n",
    "        if isinstance(rdict,dict):\n",
    "            state._append_epoch_end(rdict)\n",
    "        \n",
    "    def __get_dl(self, is_test:bool, use_test_dl:Optional[bool]=None)-> Dict[str,DataLoader]:\n",
    "        if is_test:\n",
    "            if use_test_dl is not None and not use_test_dl:\n",
    "                if self.valid_dl is None:\n",
    "                    raise AttributeError(\"valid_dl not assigned\")\n",
    "                return {self._TE:self.valid_dl}\n",
    "            elif self.test_dl is None:\n",
    "                raise AttributeError(\"test_dl not assigned\")\n",
    "            return {self._TE:self.test_dl}\n",
    "        \n",
    "        va_dl = self.valid_dl is not None\n",
    "        if  va_dl:\n",
    "            return {self._TR:self.train_dl, self._VA:self.valid_dl}\n",
    "        else:\n",
    "            return {self._TR:self.train_dl}\n",
    "    \n",
    "    def __profile_time(self, t1, t2, name, is_test):\n",
    "        if t1 is None or t2 is None:\n",
    "            return\n",
    "        else:\n",
    "            t = t2 - t1\n",
    "            a,*b =  name.split('_')\n",
    "            if len(b) == 0:\n",
    "                a += \"_t\" if is_test else \"\"\n",
    "                if a not in self.time_profile:\n",
    "                    self.time_profile[a] = []\n",
    "                self.time_profile[a].append(t)\n",
    "            else:\n",
    "                b = '_'.join(b)\n",
    "                b += \"_t\" if is_test else \"\"\n",
    "                if a not in self.time_profile:\n",
    "                    self.time_profile[a] = {}\n",
    "                if b not in self.time_profile[a]:\n",
    "                    self.time_profile[a][b] = []\n",
    "                self.time_profile[a][b].append(t)\n",
    "                \n",
    "    def print_time_profile(self):\n",
    "        if len(self.time_profile) == 0:\n",
    "            print(\"please run FitLoop.run_profiler(print_outcome=False) first\")\n",
    "        else:\n",
    "            print(\"AVERAGE TIMES\")\n",
    "            for i,m in enumerate(self.time_profile):\n",
    "                if isinstance(self.time_profile[m], list):\n",
    "                    temp = torch.tensor(self.time_profile[m]).mean().item()\n",
    "                    prf = f\"{i+1}. {m}:\".ljust(20)\n",
    "                    print(f\"{prf} {ptime(temp)}\")\n",
    "                else:\n",
    "                    print(f\"{i+1}. {m}\")\n",
    "                    \n",
    "                    for j,n in enumerate(self.time_profile[m]):\n",
    "                        temp = torch.tensor(self.time_profile[m][n]).mean().item()\n",
    "                        prf = f\"{j+1}. {n}:\".ljust(18)\n",
    "                        print(f\"  {prf} {ptime(temp)}\")\n",
    "            \n",
    "    \n",
    "    def __profile_other(self,val,name):\n",
    "        # TODO: Profiler for other metrics: CPU, GPU, RAM usages.\n",
    "        pass\n",
    "        \n",
    "        \n",
    "    # ---------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    SECTION: 2\n",
    "    \n",
    "    The main loop function __loop \n",
    "    \"\"\"\n",
    "    \n",
    "    def __loop(self, \n",
    "            epochs:int=1,  print_every:int=1, \n",
    "            steps: Optional[int]=None, load_best:bool=False, \n",
    "            profiler:bool=False, is_test:bool=False,\n",
    "            track_batch_metrics:bool=True, define_all:bool=False,\n",
    "            continue_loop:int=0, no_print:bool=False, no_cast:bool=False,\n",
    "            display_metrics:Optional[Union[str,List[str]]]=None, no_float:bool=False,\n",
    "            no_progress:bool=False,is_sanity_check:bool=False, use_test_dl:Optional[bool]=None\n",
    "           ) -> None:\n",
    "        \"\"\"\n",
    "        Runs the training loop for `epochs`\n",
    "        ----\n",
    "        PARAMETERS\n",
    "         - epochs : should be a non negative integer\n",
    "         - print_every : if 0 will not print, else will print at given epoch\n",
    "         - steps : number of batches to run in each phase [train,valid] \n",
    "             for check if everything is working.\n",
    "         - load_best : whether to load the best model after training, works only if validation\n",
    "             parameters are defined `valid_dl`, `valid_step`, `valid_epoch_end`\n",
    "         - profiler : whether to keep track of time taken by various sections\n",
    "         - is_test : whether it is a model testing loop or training/validation loop\n",
    "         - track_batch_metrics : whether to store the values returned in the batch steps\n",
    "         - define_all : If True then `torch.set_grad_enabled`, `optimizer.zero_grad` and model mode \n",
    "             ie [train,eval] have to be called where required (usually in the `train_step` function).\n",
    "         - continue_loop : Will ask whether to continue training after `continue_loop` epochs, should\n",
    "             be a positive integer.\n",
    "         - no_print : If True will suppress all print statements, can be used when custom logging is\n",
    "             used in the stage functions.\n",
    "         - no_cast : True, if data casting has to be manually set in the stage functions\n",
    "         - display_metrics : List of metrics returned in the epoch_end stage rdict that has to be \n",
    "             displayed, if None (default) all the returned metrics are displayed.\n",
    "         - no_float : True don't apply float conversion to returned metrics.\n",
    "         - no_progress : False don't show the progress bars.\n",
    "         - is_sanity_check : For sanity check mode.\n",
    "         - use_test_dl : For use with sanity check, to use valid dl or test dl.\n",
    "        \n",
    "        \"\"\"\n",
    "        time_ = lambda p : time.perf_counter() if p else None\n",
    "        tpe = lambda : time_(print_every != 0) # Returns the time \n",
    "        tpr = lambda : time_(profiler) # Times keeping used by profiler\n",
    "        \n",
    "        prof_total = tpr() # ⏳\n",
    "        total_time_start = tpe()\n",
    "        \n",
    "        # INITILIZING VARIABLES -----\n",
    "        is_train = not(is_test or is_sanity_check or profiler)\n",
    "        pre = self.preloop is not None\n",
    "        post = self.postloop is not None\n",
    "        \n",
    "        # Storage\n",
    "        prof_time = {}\n",
    "        dl = self.__get_dl(is_test, use_test_dl)\n",
    "        sz = { k : len(dl[k].dataset) for k in dl }\n",
    "        phases = [ph for ph in dl]\n",
    "        state = {ph: LoopState(ph,self,no_cast,no_float,is_train, is_test) for ph in phases}\n",
    "\n",
    "        # Markers\n",
    "        self.__save_model(self._BS)\n",
    "        \n",
    "        # TQDM Progressbar\n",
    "        tot_size = torch.tensor([len(dl[d]) for d in dl]).sum().long().item()\n",
    "        if steps is not None:\n",
    "            tot_size = torch.tensor([len(dl[d]) if len(dl[d]) < steps else steps for d in dl])\\\n",
    "                .sum().long().item()\n",
    "                                 \n",
    "        l_bar='{desc}: {percentage:3.0f}%|' \n",
    "        r_bar='| [ {n_fmt}/{total_fmt} ] :: [ {elapsed} < {remaining} ] :: [ {rate_fmt} ] ' \n",
    "        bar_format = f'{l_bar}'+'{bar}'+f'{r_bar}'\n",
    "        etqdm = lambda e: tqdm(range(e),desc=\"EPOCH :\", disable=no_progress or is_test, \\\n",
    "                               bar_format=bar_format, unit=\"epoch\",dynamic_ncols=True)\n",
    "        btqdm = lambda : tqdm(range(tot_size),leave=False or is_test,disable=no_progress,\\\n",
    "                               bar_format=bar_format,unit=\"batch\",dynamic_ncols=True)\n",
    "         \n",
    "        # PROFILER STATEMENT ---------\n",
    "        if profiler:\n",
    "            print(f\"RUNNING PROFILER: {'TEST' if is_test else 'TRAIN'} LOOP\" , (\"\" if is_test else f\"{epochs} EPOCH(s)\"))\n",
    "            for dlo in dl: \n",
    "                dlo_b = len(dl[dlo])\n",
    "                dlo_s = len(dl[dlo].dataset)\n",
    "                bs = dl[dlo].batch_size\n",
    "                if steps is not None and dlo_b > steps:\n",
    "                    dlo_b = steps\n",
    "                lb = dlo_s % bs\n",
    "                lb = lb if lb > 0 else bs\n",
    "                print(f\"  {dlo.ljust(5)} dl :: batches: {dlo_b:4} batch_size: {dl[dlo].batch_size:4} last_batch: {lb:4} dataset_size: {dlo_s:6}\")\n",
    "            print()\n",
    "        \n",
    "        \n",
    "        # CONVENIENCE FUNCTIONS ------\n",
    "        \n",
    "        # Function to get formatted epochs (from 1 not 0)\n",
    "        r_just_val = len(str(epochs))*2 + 3\n",
    "        estr = lambda e: f\"[{e + 1}/{epochs}]\".rjust(r_just_val)\n",
    "\n",
    "        # Function to print every `print_every` epochs.\n",
    "        def eprint(e,st):\n",
    "            if not no_print:\n",
    "                if (e == 0) and (print_every != 0):\n",
    "                    print(st,end=\"\")\n",
    "                elif (e + 1) % print_every == 0:\n",
    "                    print(st,end=\"\")\n",
    "\n",
    "        # Function for phase strings.\n",
    "        def statstr(phase, epoch_metrics, rjust=True):\n",
    "            mt = ' | '.join([f\"{m}: {epoch_metrics[m]:0.4f}\"for m in epoch_metrics])\n",
    "            st =  f\"{phase} :: {mt} \\n\"\n",
    "            if rjust:\n",
    "                return st.rjust(r_just_val + len(st) + 3)\n",
    "            else:\n",
    "                return st\n",
    "            \n",
    "        # To set is_test\n",
    "        def _profile_time(t1,t2,name):\n",
    "            self.__profile_time(t1,t2,name,is_test=is_test)\n",
    "            \n",
    "        prof_preloop = tpr()\n",
    "        pre and self.preloop(locals())\n",
    "        pre and profiler and _profile_time(prof_total_start, tpr(), 'preloop') # ⏳\n",
    "            \n",
    "        profiler and _profile_time(prof_total, tpr(), 'initialize') # ⏳\n",
    "        \n",
    "        # EPOCH LOOP - START -----------\n",
    "        prof_epoch_loop = tpr()\n",
    "        for e in etqdm(epochs):\n",
    "            prof_epoch_inner = tpr() # ⏳\n",
    "            epoch_time_start = tpe()\n",
    "            \n",
    "            # UPDATE: epoch_num\n",
    "            if not is_sanity_check and not profiler and not is_test:\n",
    "                self.epoch_num += 1\n",
    "            \n",
    "            # PHASE LOOP [TRAIN|VALID,TEST] - START\n",
    "            prof_phase_loop = tpr() # ⏳\n",
    "            prog_bar_phase = btqdm()\n",
    "            for phase in phases:\n",
    "                prof_phase_inner = tpr() # ⏳\n",
    "                prog_bar_phase.desc = phase.upper().ljust(5)+\" :\"\n",
    "                \n",
    "                # EPOCH START STEP - START \n",
    "                prof_epoch_start = tpr() # ⏳\n",
    "                self.__call_epoch_start_step(state[phase])\n",
    "                profiler and _profile_time(prof_epoch_start,tpr(),f'{phase}_epoch_start') # ⏳\n",
    "                # EPOCH START STEP - END \n",
    "                \n",
    "                is_tr = phase == self._TR\n",
    "                if is_tr:\n",
    "                    eprint(e,estr(e)+f\" - \")\n",
    "                \n",
    "                # UPDATE: batch_num, metrics['batch'], epoch_num\n",
    "                state[phase]._pre_epoch_start_update(e)\n",
    "                \n",
    "                \n",
    "                if not define_all:\n",
    "                    if is_tr:\n",
    "                          model.train()\n",
    "                    else:\n",
    "                          model.eval()\n",
    "                            \n",
    "                # BATCH LOOP - START \n",
    "                prof_batch_loop = tpr() # ⏳\n",
    "                for step, batch in enumerate(dl[phase]):\n",
    "                    prof_batch_inner = tpr() # ⏳\n",
    "                    \n",
    "                    if steps is not None and step == steps: break\n",
    "                    \n",
    "                    # Update LoopState: batch_num, batch and batch_size\n",
    "                    state[phase]._pre_batch_step_update(batch)\n",
    "                    \n",
    "                    # BATCH STEP - START \n",
    "                    prof_batch_step = tpr() # ⏳\n",
    "                    if define_all:\n",
    "                        self.__call_batch_step(state[phase])\n",
    "                    else:\n",
    "                        if isinstance(self.optimizer,list):\n",
    "                            for opt in self.optimizer:opt.zero_grad()\n",
    "                        else:\n",
    "                            self.optimizer.zero_grad()\n",
    "                        with torch.set_grad_enabled(is_tr):\n",
    "                            self.__call_batch_step(state[phase])\n",
    "                    profiler and _profile_time(prof_batch_step,tpr(),f'{phase}_step') # ⏳\n",
    "                    # BATCH STEP - END \n",
    "                    prog_bar_phase.update(1)\n",
    "                    \n",
    "                    profiler and _profile_time(prof_batch_inner,tpr(),f'{phase}_batch_inner') # ⏳\n",
    "                    \n",
    "                profiler and _profile_time(prof_batch_loop,tpr(),f'{phase}_batch_loop') # ⏳\n",
    "                # BATCH LOOP - END \n",
    "                \n",
    "                # EPOCH END STEP - START \n",
    "                prof_epoch_end = tpr()\n",
    "                self.__call_epoch_end_step(state[phase])\n",
    "                profiler and _profile_time(prof_epoch_end,tpr(),f'{phase}_epoch_end') # ⏳\n",
    "                # EPOCH END STEP - END \n",
    "                \n",
    "                # UPDATE MARKERS\n",
    "                if not (is_tr or is_test or profiler or is_sanity_check) and self.criteria is not None:\n",
    "                    score = state[phase]._get_epoch_metric(self.criteria)\n",
    "                    direc = self.criteria_direction > 0\n",
    "                    is_better = (score > self.best_score) if direc else (score < self.best_score)\n",
    "                    if is_better:\n",
    "                        self.best_score = score\n",
    "                        self.__save_model(self._BS)\n",
    "\n",
    "                # PRINT EPOCH[PHASE] METRICS\n",
    "                epoch_metrics = state[phase]._get_epoch_metrics(display_metrics)\n",
    "                if is_tr or is_test:\n",
    "                    eprint(e,statstr(phase, epoch_metrics,False))\n",
    "                else:\n",
    "                    eprint(e,statstr(phase, epoch_metrics))\n",
    "                    \n",
    "                profiler and _profile_time(prof_phase_inner,tpr(),f'{phase}_phase_inner') # ⏳\n",
    "                \n",
    "            profiler and _profile_time(prof_phase_loop,tpr(),f'phase loop') # ⏳\n",
    "            # PHASE LOOP [TRAIN|VALID,TEST] - END\n",
    "            \n",
    "            # PRINT EPOCH TIMES\n",
    "            epoch_time_end = tpe()\n",
    "            epoch_time = ftime(epoch_time_start, epoch_time_end)\n",
    "            epoch_time = f\"epoch time: {epoch_time}\" + (\"\\n\" if no_progress else \"\")\n",
    "            not is_test and eprint(e,epoch_time.rjust(len(epoch_time) + r_just_val + 3)+\"\\n\")\n",
    "            \n",
    "            prog_bar_phase.close()\n",
    "            # CONTINUE LOOP ?\n",
    "            if continue_loop > 0 and \\\n",
    "                not (is_sanity_check or is_test or profiler) and \\\n",
    "                ((e + 1) % continue_loop == 0) and (e + 1 != epochs):\n",
    "                cont = input(\"continue loop ([y]/n): \")\n",
    "                not no_print and print()\n",
    "                if cont == 'n':\n",
    "                    break\n",
    "            \n",
    "            profiler and _profile_time(prof_epoch_inner,tpr(),f'epoch_inner') # ⏳\n",
    "\n",
    "        profiler and _profile_time(prof_epoch_loop,tpr(),f'epoch_loop') # ⏳\n",
    "        # EPOCH LOOP - END \n",
    "        \n",
    "        prof_postloop = tpr()\n",
    "        post and self.postloop(locals())\n",
    "        post and profiler and _profile_time(prof_postloop, tpr(), 'postloop') # ⏳\n",
    "        \n",
    "        # PRINT FINAL METRICS\n",
    "        eprint(0,\"-\"*r_just_val)\n",
    "        total_time_end = tpe()\n",
    "        total_time = ftime(total_time_start,total_time_end)\n",
    "        eprint(0, f\"\\ntotal time: {total_time}\\n\")\n",
    "        if self.criteria is not None and not is_test:\n",
    "            eprint(0, f\"best score: {self.best_score:0.4f}\\n\")\n",
    "\n",
    "        # RESTORE BEST MODEL\n",
    "        prof_restore_model = tpr() # ⏳\n",
    "        if load_best or profiler or is_sanity_check:\n",
    "            self.__load_model(self._BS)\n",
    "        profiler and _profile_time(prof_restore_model,tpr(),f'restore model') # ⏳\n",
    "\n",
    "        profiler and _profile_time(prof_total,tpr(),f'total',) # ⏳\n",
    "        # __loop - END \n",
    "\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    SECTION: 3 - A\n",
    "    \n",
    "    Loop methods for training and testing of the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def fit(self, \n",
    "            epochs:int=1, print_every:int=1,\n",
    "            display_metrics:Optional[Union[str,List[str]]]=None,\n",
    "            track_batch_metrics:bool=True, load_best:bool=True,\n",
    "            continue_loop:int=0, define_all:bool=False,  \n",
    "            no_print:bool=False, no_cast:bool=False,\n",
    "            no_float:bool=False, no_progress:bool=False\n",
    "           ) -> None:\n",
    "        \"\"\"\n",
    "        Runs the training loop for `epochs`\n",
    "        ----\n",
    "        PARAMETERS\n",
    "         - epochs : should be a non negative integer\n",
    "         - print_every : if 0 will not print, else will print at given epoch\n",
    "         - display_metrics : List of metrics returned in the epoch_end stage rdict that has to be \n",
    "             displayed, if None (default) all the returned metrics are displayed.\n",
    "         - track_batch_metrics : whether to store the values returned in the batch steps\n",
    "         - load_best : whether to load the best model after training, works only if validation\n",
    "             parameters are defined `valid_dl`, `valid_step`, `valid_epoch_end`\n",
    "         -  continue_loop : Will ask whether to continue training after `continue` epochs; should\n",
    "             be a positive integer.\n",
    "         - define_all : If True then `torch.set_grad_enabled`, `optimizer.zero_grad` and model mode \n",
    "             ie [train,eval] have to be called where required (usually in the `train_step` function).\n",
    "         - no_print : If True will suppress all print statements, can be used when custom logging is\n",
    "             used in the stage functions.\n",
    "         - no_cast : True, if data casting has to be manually set in the stage functions\n",
    "         - no_float : True, don't apply float conversion to returned metrics.\n",
    "         - no_progress : True, don't show the progress bar.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.__loop(epochs=epochs, print_every=print_every,\n",
    "                   display_metrics=display_metrics, track_batch_metrics=track_batch_metrics,\n",
    "                   load_best=load_best, continue_loop=continue_loop, define_all=define_all,\n",
    "                   no_print=no_print, no_cast=no_cast, no_float=no_float, no_progress=no_progress)\n",
    "    \n",
    "    def train(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Alias for FitLoop.fit\n",
    "        \"\"\"\n",
    "        self.fit(*args, **kwargs)\n",
    "    \n",
    "    def test(self, no_print:bool=False, no_cast:bool=False, no_float:bool=False) -> None:\n",
    "        \"\"\"\n",
    "        For model testing. Runs loop for one epoch using test DataLoader and test stage functions.\n",
    "        ----\n",
    "        PARAMETERS\n",
    "         - no_print : If True will suppress all print statements, can be used when custom logging is\n",
    "             used in the stage functions.\n",
    "         - no_cast : True, if model and data casting has to be manually set in the stage functions\n",
    "         - no_float : True don't apply float conversion to returned metrics.\n",
    "         - no_progress : True, don't show the progress bar.\n",
    "        \"\"\"\n",
    "        self.__loop(is_test=True, no_print=no_print, no_cast=no_cast, no_float=no_float)\n",
    "        \n",
    "    \"\"\"\n",
    "    SECTION: 3 - B\n",
    "    \n",
    "    Loop methods for (sort of) unit testing and timing of components.\n",
    "    \"\"\"\n",
    "    def run_profiler(self,\n",
    "            epochs:Optional[int]=1, steps: Optional[int]=None, define_all:bool=False,\n",
    "            no_cast:bool=False, no_float:bool=False, no_progress:bool=False, print_outcome:bool=True\n",
    "            ) -> Dict[str,Union[Dict[str,List[float]],List[float]]]:\n",
    "        \"\"\"\n",
    "        Runs the loop in profiler mode, ie run all three (train, valid, test) phases \n",
    "        (if set) for given number of epochs and steps and print the average time taken \n",
    "        at different stages, loop output is not printed.\n",
    "        \n",
    "        Returns the time_profile dict.\n",
    "        \n",
    "        Criteria based checkpointing is not run, ie best_model and best_score are not saved.\n",
    "        Model state is not altered (it's reloaded) if the profiler is not interrupted.\n",
    "        ----\n",
    "        PARAMETERS\n",
    "         - epochs : should be a non negative integer\n",
    "         - steps : number of batches to iterate over in each phase [train,valid,test] \n",
    "             to check if everything is working as expected, if None then all batches are\n",
    "             iterated over.\n",
    "         - define_all : If True then `torch.set_grad_enabled`, `optimizer.zero_grad` and model mode \n",
    "             ie [train,eval] have to be called where required (usually in the `train_step` function).\n",
    "         - no_cast : True, if data casting has to be manually set in the stage functions\n",
    "         - no_float : True don't apply float conversion to returned metrics.\n",
    "         - no_progress : True, don't show the progress bar.\n",
    "         - print_outcome : If False won't print profiler outcome, can be accesed from FitLoop.time_profile\n",
    "        \"\"\"\n",
    "        t1 = time.perf_counter()\n",
    "        self.__loop(epochs=epochs,steps=steps, define_all=define_all, no_cast=no_cast, \n",
    "                    no_float=no_float, no_print=True, no_progress=no_progress, profiler=True)\n",
    "        if self.test_dl is not None:\n",
    "            self.__loop(steps=steps, define_all=define_all, no_cast=no_cast, \n",
    "                        no_float=no_float, no_print=True, no_progress=no_progress, profiler=True, \n",
    "                        is_test=True)\n",
    "        st = ptime(time.perf_counter() - t1)\n",
    "        if print_outcome:\n",
    "            self.print_time_profile()\n",
    "            time_profile = self.time_profile\n",
    "            self.time_profile = {}\n",
    "            print(f\"\\ntotal time: {st}\")\n",
    "            return time_profile\n",
    "        else:\n",
    "            return self.time_profile\n",
    "    \n",
    "    def run_sanity_check(self, epochs:int=1, \n",
    "            steps:int=3, print_every:int=1, use_test_dl=False,\n",
    "            display_metrics:Optional[Union[str,List[str]]]=None,\n",
    "            continue_loop:int=0, define_all:bool=False,  \n",
    "            no_print:bool=False, no_cast:bool=False,\n",
    "            no_float:bool=False, no_progress:bool=False\n",
    "           ) -> None:\n",
    "        \"\"\"\n",
    "        Runs the loop in sanity check mode, ie all three (train, valid, test) phases \n",
    "        (if set) for given number of epochs and steps.\n",
    "        Criteria based checkpointing is not run, ie best_model and best_score are not saved.\n",
    "        Model state is not altered (it's reloaded) if the sanity check is not interrupted.\n",
    "        ----\n",
    "        PARAMETERS\n",
    "         - epochs : should be a non negative integer\n",
    "         - steps : number of batches to run in each phase [train,valid] \n",
    "             for check if everything is working, if None all batches are iterated over.\n",
    "         - use_test_dl : If False will use the validation DataLoader for the test phase,\n",
    "             else will use the test DataLoader.\n",
    "         - print_every : if 0 will not print, else will print at given epoch\n",
    "         - display_metrics : List of metrics returned in the epoch_end stage rdict that has to be \n",
    "             displayed, if None (default) all the returned metrics are displayed.\n",
    "         -  continue_loop : Will ask whether to continue training after `continue` epochs, should\n",
    "             be a positive integer.\n",
    "         - define_all : If True then `torch.set_grad_enabled`, `optimizer.zero_grad` and model mode \n",
    "             ie [train,eval] have to be called where required (usually in the `train_step` function).\n",
    "         - no_print : If True will suppress all print statements, can be used when custom logging is\n",
    "             used in the stage functions.\n",
    "         - no_cast : True, if data casting has to be manually set in the stage functions\n",
    "         - no_float : True don't apply float conversion to returned metrics.\n",
    "         - no_progress : True, don't show the progress bar.\n",
    "        \"\"\"\n",
    "    \n",
    "        \n",
    "        print(f\"RUNNING SANITY CHECK: TRAIN LOOP - {epochs} EPOCH(s), {steps} STEP(s)\")\n",
    "        self.__loop(epochs=epochs, steps=steps, print_every=print_every, \n",
    "                    display_metrics=display_metrics, continue_loop=continue_loop,\n",
    "                    define_all=define_all, no_print=no_print, no_cast=no_cast, \n",
    "                    no_float=no_float, no_progress=no_progress, is_sanity_check=True)\n",
    "        if self.test_dl is not None:\n",
    "            print()\n",
    "            print(f\"RUNNING SANITY CHECK: TEST LOOP - {steps} STEP(s)\")\n",
    "            self.__loop(use_test_dl=use_test_dl, epochs=epochs, steps=steps, print_every=print_every, \n",
    "                        display_metrics=display_metrics, continue_loop=continue_loop,\n",
    "                        define_all=define_all, no_print=no_print, no_cast=no_cast, \n",
    "                        no_float=no_float, no_progress=no_progress, is_sanity_check=True, is_test=True)\n",
    "    \n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    SECTION: 4\n",
    "    \n",
    "    Functions to preserve the model state.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __save_model(self, typ:str) -> None:\n",
    "        \"\"\"\n",
    "        Save model to object or to the disk.\n",
    "        \"\"\"\n",
    "        name = self.best_model_name if typ == self._BS else self.pretrained_model_name\n",
    "        path = self.save_path/ name\n",
    "        state_dict = self.model.state_dict()\n",
    "        if self.save_to_disk:\n",
    "            torch.save(state_dict,path)\n",
    "        elif typ == self._BS:\n",
    "            self.best_model_state_dict = deepcopy(state_dict)\n",
    "        else:\n",
    "            self.pretrained_model_state_dict = deepcopy(state_dict)\n",
    "        \n",
    "    def __load_model(self, typ:str):\n",
    "        \"\"\"\n",
    "        Load model from the object or from the disk.\n",
    "        \"\"\"\n",
    "        name = self.best_model_name if typ == self._BS else self.pretrained_model_name\n",
    "        path = self.save_path/ name\n",
    "        if self.save_to_disk:\n",
    "            state_dict = torch.load(path, map_location=self.device)\n",
    "        elif typ == self._BS:\n",
    "            state_dict = self.best_model_state_dict\n",
    "        else:\n",
    "            state_dict = self.pretrained_model_state_dict\n",
    "        self.model.load_state_dict(state_dict)\n",
    "        if self.configure_optimizer is None:\n",
    "            print(\"please reconfigure FitLoop.optimizer before training\")\n",
    "        else:\n",
    "            self.configure_optimizer(self)\n",
    "    \n",
    "    def reset(self, reset_model:bool=True) -> None:\n",
    "        \"\"\"\n",
    "        Resets FitLoop to initial state.\n",
    "        Parameters reset:\n",
    "            - model, to pretrained state if `reset_model`\n",
    "            - epoch_num, to 0\n",
    "            - best_score to ∓inf\n",
    "        FitLoop.optimizer param groups will have to be set again\n",
    "        \"\"\"\n",
    "        if reset_model:\n",
    "            self.__load_model(self, self._PR)\n",
    "        self.epoch_num = 0\n",
    "        self.best_score = self.criteria_direction * float('-inf')\n",
    "        \n",
    "        \n",
    "    # ---------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    SECTION: 5\n",
    "    \n",
    "    Functions to preserve the FitLoop object state so that training can be resumed.\n",
    "    \"\"\"\n",
    "    \n",
    "    def save(self, path, only_model=False):\n",
    "        \"\"\"\n",
    "        TODO : save the FitLoop state, if only_model then save only model.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def load(self, path):\n",
    "        \"\"\"\n",
    "        TODO : load the FitLoop state, if only model then load the model \n",
    "            state dict.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    SECTION: 6\n",
    "    \n",
    "    Functions to delete stored model weights.\n",
    "    \"\"\"\n",
    "    \n",
    "    def del_pretrained(self) -> None:\n",
    "        \"\"\"\n",
    "        Deletes the pretrianed model state dict from the disk if \n",
    "        `save_to_disk` else states attribute to None\n",
    "        \"\"\"\n",
    "        if self.save_to_disk:\n",
    "            (self.save_path/self.pretrained_model_name).unlink()\n",
    "        else:\n",
    "            self.pretrained_model_state_dict = None\n",
    "        \n",
    "    def del_best_model(self) -> None:\n",
    "        \"\"\"\n",
    "        Deletes the best model state dict from the disk if \n",
    "        `save_to_disk` else states attribute to None\n",
    "        \"\"\"\n",
    "        if self.save_to_disk:\n",
    "            (self.save_path/self.best_model_name).unlink()\n",
    "        else:\n",
    "            self.best_model_state_dict = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T00:30:42.867191Z",
     "start_time": "2020-05-07T00:30:42.862331Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim, nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.datasets import FakeData\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 964,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T23:58:48.421406Z",
     "start_time": "2020-05-07T23:58:48.214385Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Stage Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Batch Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T05:17:38.943815Z",
     "start_time": "2020-05-07T05:17:38.930490Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def common_step(state):\n",
    "#     print(f\"{state.phase}_step, bn: {state.batch_num} en: {state.epoch_num}, \",end=\"\")\n",
    "    X, y = state.batch\n",
    "    y_ = state.model(X)\n",
    "    loss = state.loss_function(y_,y)\n",
    "#     print(\"loss\",loss.item())\n",
    "    r_loss = loss.item() * state.batch_size\n",
    "    r_corr = (y_.argmax(dim=1) == y).sum().item()\n",
    "    return loss, r_loss, r_corr\n",
    "\n",
    "def train_step(state):\n",
    "    loss, r_loss, r_corr = common_step(state)\n",
    "    loss.backward()\n",
    "    state.optimizer.step()\n",
    "    return {'r_loss':r_loss,'r_corr':r_corr}\n",
    "\n",
    "def valid_step(state):\n",
    "    loss, r_loss, r_corr = common_step(state)\n",
    "    return {'r_loss':r_loss,'r_corr':r_corr}\n",
    "\n",
    "def test_step(state):\n",
    "    loss, r_loss, r_corr = common_step(state)\n",
    "    return {'r_loss':r_loss,'r_corr':r_corr}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Epoch Start Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T02:30:23.312588Z",
     "start_time": "2020-05-07T02:30:23.298636Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def common_epoch_start(state):\n",
    "#     print(f\"\\n{state.phase}_epoch_start, # {state.epoch_num}\")\n",
    "    return {'dummy':'dict'}\n",
    "    \n",
    "def train_epoch_start(state):\n",
    "    return common_epoch_start(state)\n",
    "\n",
    "def valid_epoch_start(state):\n",
    "    return common_epoch_start(state)\n",
    "\n",
    "def test_epoch_start(state):\n",
    "    return common_epoch_start(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Epoch End Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T02:30:24.831128Z",
     "start_time": "2020-05-07T02:30:24.819422Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def common_epoch_end(state):\n",
    "#     print(f\"{state.phase}_epoch_end, # {state.epoch_num}\")\n",
    "    r_loss = state['r_loss']\n",
    "    r_corr = state['r_corr']\n",
    "    \n",
    "#     print('r_loss len',len(r_loss))\n",
    "#     print('r_corr len',len(r_corr))\n",
    "    \n",
    "    e_loss = r_loss.sum()/state.size\n",
    "    e_accu = r_corr.sum()/state.size\n",
    "    \n",
    "#     print('loss',e_loss)\n",
    "#     print('accu',e_accu)\n",
    "    \n",
    "    return {'loss':e_loss, 'accu':e_accu}\n",
    "    \n",
    "def train_epoch_end(state):\n",
    "    return common_epoch_end(state)\n",
    "\n",
    "def valid_epoch_end(state):\n",
    "    return common_epoch_end(state)\n",
    "\n",
    "def test_epoch_end(state):\n",
    "    return common_epoch_end(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FitLoop - Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1038,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T00:26:46.421944Z",
     "start_time": "2020-05-08T00:26:46.253612Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Basic setup with FakeData for testing.\n",
    "\"\"\"\n",
    "\n",
    "def get_dl(batch_size=4, sz=[100,20,30]):\n",
    "    sets = ['train','valid','test']\n",
    "    TR, VA, TE = sets\n",
    "    class_names = ['a','b','c','d']\n",
    "    num_classes = 4\n",
    "\n",
    "    sz = {s:z for s, z in zip(sets,sz)} # a multiple of batch size\n",
    "    ds = {s:FakeData(size=sz[s], transform=ToTensor(), num_classes=num_classes) for s in sets}\n",
    "    dl = {s:DataLoader(ds[s],batch_size=batch_size) for s in ds}\n",
    "    return dl\n",
    "\n",
    "model = resnet18()\n",
    "in_features = model.fc.in_features\n",
    "model.fc = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    \n",
    "dummy = nn.Sequential(\n",
    "    nn.Conv2d(3,1,3,10),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(529,4)\n",
    ")\n",
    "\n",
    "model = dummy\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# ----------------------------------------------\n",
    "\n",
    "def configure_optimizer(self):\n",
    "    parameters = self.model.parameters()\n",
    "    self.optimizer.param_groups.clear()\n",
    "    self.optimizer.add_param_group({'params': parameters})\n",
    "    \n",
    "dl = get_dl(batch_size=5,sz=[523,121,232])\n",
    "fl_dict = {\n",
    "    \"model\": model,\n",
    "    \"optimizer\": optimizer,\n",
    "    \"loss_function\": loss_function,\n",
    "    \"train_dl\":dl[TR],\n",
    "    \"valid_dl\":dl[VA],\n",
    "    \"test_dl\":dl[TE],\n",
    "    \"train_step\":train_step,\n",
    "    \"valid_step\":valid_step,\n",
    "    \"test_step\":test_step,\n",
    "    \"train_epoch_start\":train_epoch_start,\n",
    "    \"valid_epoch_start\":valid_epoch_start,\n",
    "    \"test_epoch_start\":test_epoch_start,\n",
    "    \"train_epoch_end\":train_epoch_end,\n",
    "    \"valid_epoch_end\":valid_epoch_end,\n",
    "    \"test_epoch_end\":test_epoch_end,\n",
    "    \"configure_optimizer\":configure_optimizer,\n",
    "    \"criteria\": \"accu\"\n",
    "}\n",
    "\n",
    "trainer = FitLoop(**fl_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1041,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T00:27:10.546181Z",
     "start_time": "2020-05-08T00:27:03.472528Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e708ee88e75499bbfcbd018eeaa4157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='EPOCH :', layout=Layout(flex='2'), max=10.0, style=Progre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=130.0), HTML(value='')), layout=Layout(di…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [1/10] - train :: loss: 1.2757 | accu: 0.6252 \n",
      "          valid :: loss: 1.1897 | accu: 0.7769 \n",
      "          epoch time: 02 s 957 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=130.0), HTML(value='')), layout=Layout(di…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [2/10] - train :: loss: 1.1874 | accu: 0.7189 \n",
      "          valid :: loss: 1.0747 | accu: 0.8182 \n",
      "          epoch time: 02 s 781 ms\n",
      "continue loop ([y]/n): n\n",
      "\n",
      "\n",
      "-------\n",
      "total time: 07 s 068 ms\n",
      "best score: 0.8182\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(epochs=10, no_progress=False, continue_loop=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1048,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T00:27:57.349059Z",
     "start_time": "2020-05-08T00:27:57.320598Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 1048,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.epoch_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1043,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T00:27:15.512017Z",
     "start_time": "2020-05-08T00:27:14.400323Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bdf0e56081d40548166bbf9988a7ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=47.0), HTML(value='')), layout=Layout(dis…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test :: loss: 1.0798 | accu: 0.8103 \n",
      "\n",
      "-----\n",
      "total time: 01 s 103 ms\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1045,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T00:27:35.751782Z",
     "start_time": "2020-05-08T00:27:31.942902Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING SANITY CHECK: TRAIN LOOP - 1 EPOCH(s), None STEP(s)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "992da525a45b4f4bb5927646d36ada0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='EPOCH :', layout=Layout(flex='2'), max=1.0, style=Progres…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=130.0), HTML(value='')), layout=Layout(di…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1] - train :: loss: 1.0679 | accu: 0.7591 \n",
      "        valid :: loss: 0.9327 | accu: 0.8595 \n",
      "        epoch time: 02 s 804 ms\n",
      "\n",
      "-----\n",
      "total time: 02 s 836 ms\n",
      "best score: 0.8182\n",
      "\n",
      "RUNNING SANITY CHECK: TEST LOOP - None STEP(s)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e4a7cb49bdd4a6bbeca8f4f70aa3631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=47.0), HTML(value='')), layout=Layout(dis…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test :: loss: 1.0798 | accu: 0.8103 \n",
      "\n",
      "-----\n",
      "total time: 00 s 968 ms\n"
     ]
    }
   ],
   "source": [
    "trainer.run_sanity_check(use_test_dl=True,steps=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1047,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T00:27:50.080507Z",
     "start_time": "2020-05-08T00:27:42.823989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING PROFILER: TRAIN LOOP 2 EPOCH(s)\n",
      "  train dl :: batches:  105 batch_size:    5 last_batch:    3 dataset_size:    523\n",
      "  valid dl :: batches:   25 batch_size:    5 last_batch:    1 dataset_size:    121\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2949c31f95148eca2e6adcb466cc610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='EPOCH :', layout=Layout(flex='2'), max=2.0, style=Progres…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=130.0), HTML(value='')), layout=Layout(di…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=130.0), HTML(value='')), layout=Layout(di…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RUNNING PROFILER: TEST LOOP \n",
      "  test  dl :: batches:   47 batch_size:    5 last_batch:    2 dataset_size:    232\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34241839dc374bc4ae3f49aed9f0635d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=47.0), HTML(value='')), layout=Layout(dis…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AVERAGE TIMES\n",
      "1. initialize:       00 s 002 ms 202 us\n",
      "2. train\n",
      "  1. epoch_start:    00 s 830 ms 650 us\n",
      "  2. step:           00 s 001 ms 666 us\n",
      "  3. batch_inner:    00 s 002 ms 184 us\n",
      "  4. batch_loop:     02 s 600 ms 802 us\n",
      "  5. epoch_end:      00 s 000 ms 217 us\n",
      "  6. phase_inner:    02 s 601 ms 177 us\n",
      "3. valid\n",
      "  1. epoch_start:    00 s 756 ms 505 us\n",
      "  2. step:           00 s 000 ms 503 us\n",
      "  3. batch_inner:    00 s 000 ms 762 us\n",
      "  4. batch_loop:     00 s 466 ms 777 us\n",
      "  5. epoch_end:      00 s 000 ms 155 us\n",
      "  6. phase_inner:    00 s 467 ms 036 us\n",
      "4. phase loop:       03 s 093 ms 361 us\n",
      "5. epoch\n",
      "  1. inner:          03 s 094 ms 777 us\n",
      "  2. loop:           06 s 222 ms 683 us\n",
      "  3. inner_t:        01 s 014 ms 415 us\n",
      "  4. loop_t:         01 s 014 ms 661 us\n",
      "6. restore model:    00 s 000 ms 836 us\n",
      "7. total:            06 s 225 ms 782 us\n",
      "8. initialize_t:     00 s 001 ms 633 us\n",
      "9. test\n",
      "  1. epoch_start_t:  00 s 007 ms 899 us\n",
      "  2. step_t:         00 s 000 ms 555 us\n",
      "  3. batch_inner_t:  00 s 001 ms 003 us\n",
      "  4. batch_loop_t:   00 s 974 ms 546 us\n",
      "  5. epoch_end_t:    00 s 252 ms 600 us\n",
      "  6. phase_inner_t:  00 s 974 ms 725 us\n",
      "10. phase loop_t:    01 s 012 ms 246 us\n",
      "11. restore model_t: 00 s 000 ms 370 us\n",
      "12. total_t:         01 s 016 ms 713 us\n",
      "\n",
      "total time: 07 s 243 ms 636 us\n"
     ]
    }
   ],
   "source": [
    "_ = trainer.run_profiler(epochs=2, steps=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test other stuff here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T22:51:14.379217Z",
     "start_time": "2020-05-07T22:51:14.373735Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1001,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T00:11:58.376688Z",
     "start_time": "2020-05-08T00:11:56.831198Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fdf932ea4ac4721bbed83f747310724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='EPOCH :', layout=Layout(flex='2'), max=5.0, style=Progres…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91f67a6eba3447f98c00271ac79ad3d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=5.0), HTML(value='')), layout=Layout(disp…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test :: loss: 0.120 | accu: 0.744\n",
      "\n",
      "\tepoch time: 00 s 295 ms \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d0dc277321f4d5382b8690a0faf118d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=5.0), HTML(value='')), layout=Layout(disp…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test :: loss: 0.120 | accu: 0.744\n",
      "\n",
      "\tepoch time: 00 s 307 ms \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b159ca367ee44cc78785c5610373fecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=5.0), HTML(value='')), layout=Layout(disp…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test :: loss: 0.120 | accu: 0.744\n",
      "\n",
      "\tepoch time: 00 s 298 ms \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3191a94d385b45db84b7a780006c931d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=5.0), HTML(value='')), layout=Layout(disp…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test :: loss: 0.120 | accu: 0.744\n",
      "\n",
      "\tepoch time: 00 s 295 ms \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1067c18ea4e4fbeb42da260c0aaf5c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=5.0), HTML(value='')), layout=Layout(disp…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test :: loss: 0.120 | accu: 0.744\n",
      "\n",
      "\tepoch time: 00 s 298 ms \n",
      "\n",
      "-----\n",
      "best score: 0.744\n",
      "total time: 01 s 533 ms\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "# phases = ['train','valid']\n",
    "phases = ['test']\n",
    "batches = {\n",
    "    'train':20,\n",
    "    'valid':4,\n",
    "    'test':5\n",
    "}\n",
    "tot = 5\n",
    "def dl_(phase):\n",
    "    for i in range(batches[phase]):\n",
    "        yield [3,3,3,3],i\n",
    "        \n",
    "to_print = True\n",
    "no_progress = False\n",
    "\n",
    "def eprint(*args,**kwargs):\n",
    "    if to_print:\n",
    "#         tqdm.write(*args,**kwargs)\n",
    "        print(*args,**kwargs)\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "Specify a custom bar string formatting. May impact performance. \n",
    "[default: '{l_bar}{bar}{r_bar}']  \n",
    "l_bar='{desc}: {percentage:3.0f}%|' \n",
    "r_bar='| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}{postfix}]' \n",
    "\n",
    "Possible vars: \n",
    "    l_bar, bar, r_bar, n, n_fmt, total, total_fmt, \n",
    "    percentage, elapsed, elapsed_s, ncols, nrows, desc, \n",
    "    unit, rate, rate_fmt, rate_noinv, rate_noinv_fmt, \n",
    "    rate_inv, rate_inv_fmt, postfix, unit_divisor, \n",
    "    remaining, remaining_s. \n",
    "    \n",
    "Note that a trailing \": \" is automatically removed after {desc} if the latter is empty.\n",
    "\"\"\"\n",
    "\n",
    "le_bar='{desc}: {percentage:3.0f}%|' \n",
    "lb_bar='{desc}: {percentage:3.0f}%|' \n",
    "# r_bar='| [ {n_fmt}/{total_fmt} ] {elapsed}<{remaining}, ' '{rate_fmt} s/epoch' \n",
    "re_bar='| [ {n_fmt}/{total_fmt} ] :: [ {rate_fmt} ] :: [ {elapsed} < {remaining} ]' \n",
    "rb_bar='| [ {n_fmt}/{total_fmt} ] :: [ {rate_fmt} ] :: [ {elapsed} < {remaining} ]' \n",
    "e_bar_format = f'{le_bar}'+'{bar}'+f'{re_bar}'\n",
    "b_bar_format = f'{lb_bar}'+'{bar}'+f'{rb_bar}'\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "l_bar='{desc}: {percentage:3.0f}%|' \n",
    "r_bar='| [ {n_fmt}/{total_fmt} ] :: [ {rate_fmt} ] :: [ {elapsed} < {remaining} ]' \n",
    "b_bar_format = f'{l_bar}'+'{bar}'+f'{r_bar}'\n",
    "etqdm = lambda e: tqdm(range(e),desc=\"EPOCH :\", disable=no_progress, bar_format=e_bar_format, unit=\"epoch\",dynamic_ncols=True)\n",
    "btqdm = lambda b: tqdm(range(b),leave=True,disable=no_progress, bar_format=b_bar_format,unit=\"batch\",dynamic_ncols=True)\n",
    "\n",
    "# EPOCH LOOP\n",
    "# for i in tqdm(range(epochs),desc=\"EPOCH :\", disable=disable, bar_format=e_bar_format):\n",
    "for i in etqdm(epochs):\n",
    "    \n",
    "    t2 = time.time()\n",
    "    \n",
    "    # PHASE LOOP\n",
    "    \n",
    "    pbar = btqdm(tot)\n",
    "    for phase in phases:\n",
    "        pbar.desc = phase.upper().ljust(5)+\" :\"\n",
    "        \n",
    "        if phase == 'train':\n",
    "            eprint(f\"[{i+1}/{epochs}] - \",end=\"\")\n",
    "        if phase=='valid':\n",
    "            eprint(f'\\t{phase} :: loss: 0.222 | accu: 0.344')\n",
    "        else:\n",
    "            eprint(f'{phase} :: loss: 0.120 | accu: 0.744')\n",
    "            \n",
    "        # BATCH LOOP\n",
    "        for b in dl_(phase):\n",
    "            time.sleep(0.05)\n",
    "            pbar.update(1)\n",
    "    pbar.close()\n",
    "    eprint(f\"\\tepoch time: {ftime(t2,time.time())}\",\"\\n\"if no_progress else \"\") \n",
    "    \n",
    "eprint(\"-\"*5)\n",
    "eprint(f\"best score: 0.744\") \n",
    "eprint(f\"total time: {ftime(t1,time.time())}\") \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Portions of the loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- **Single step** Portion of the loop that receives the batch and will cal forward pass on it.\n",
    "    - *Set* zero grad\n",
    "    - **Train Step** \n",
    "        - *Set* \n",
    "            - enable gradients\n",
    "            - model.train\n",
    "        - *Define*: \n",
    "            - compute loss\n",
    "            - call backward \n",
    "            - update gradients and maybe scheduler step\n",
    "            - loss and whatever metrics will be saved in a list.\n",
    "    - **Valid Step**\n",
    "        - *Set*\n",
    "            - disable gradients\n",
    "            - model.eval\n",
    "        - *Define*: \n",
    "            - compute loss\n",
    "            - loss and whatever metrics will be saved in a list.\n",
    "    - **Test Step** Only in test loop\n",
    "        - *Set*\n",
    "            - disable gradients\n",
    "            - model.eval\n",
    "        - *Define*: \n",
    "            - compute loss\n",
    "            - loss and whatever metrics will be saved in a list.\n",
    "- **Pre Epoch**\n",
    "- **Post Epoch**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Function to calculate a time string from\n",
    "# 2 instances of time.time\n",
    "def ftime(t1,t2):\n",
    "    t = t2-t1\n",
    "    s, ms = str(t).split('.')\n",
    "    ms = ms[:3]\n",
    "    s = str(int(s)%60).rjust(2)\n",
    "    m = int(t//60)\n",
    "    h = str(m//60)\n",
    "    m = str(m % 60).rjust(2)\n",
    "    u = ['h','m','s','ms']\n",
    "    v = [h,m,s,ms]\n",
    "    t = filter(lambda x:int(x[0]) > 0,zip(v,u))\n",
    "    return ' '.join([' '.join(x) for x in t]).ljust(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def fit(model, dl_train, dl_valid, optim, loss_function, epochs, sched=None, should_pass=False, print_every=1, load_best=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    # model\n",
    "      - pytorch model to be trained\n",
    "      \n",
    "    # dl_train\n",
    "      - Dataloader for the train set\n",
    "      \n",
    "    # dl_valid\n",
    "      - Dataloader for the valid set\n",
    "      \n",
    "    # optimizer\n",
    "      - optimizer attached to model params from torch.optim\n",
    "      \n",
    "    # loss_function\n",
    "      - function to calculate the loss; loss_function(model(X),y)\n",
    "      \n",
    "    # epochs\n",
    "      - number of epochs to train for\n",
    "      \n",
    "    # device\n",
    "      - torch.device to load the model to \n",
    "      \n",
    "    # sched\n",
    "      - scheduler for the optimizer learning rate\n",
    "      \n",
    "    # should_pass\n",
    "      - if loss should be passed to the scheduler.\n",
    "      \n",
    "    # print_every\n",
    "      - to print the loss every n epochs\n",
    "    \n",
    "    # load_best \n",
    "      - whether to load the best model as determined by accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    total_time_start = time.time()\n",
    "    # ----------------------------\n",
    "    phases = [TR,VA]\n",
    "    dl = {TR:dl_train, VA:dl_valid}\n",
    "    sizes = {k:len(dl[k].dataset) for k in dl}\n",
    "    \n",
    "    # To contain metrics to plot later\n",
    "    losses_all = {TR:[],VA:[]}\n",
    "    losses_epo = {TR:[],VA:[]}\n",
    "    accuracy_epo = {TR:[],VA:[]}\n",
    "    \n",
    "    # Markers\n",
    "    least_loss = float('inf')\n",
    "    best_accuracy = 0\n",
    "    best_model = deepcopy(model.state_dict())\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Convenience functions (that won't be used elsewhere)\n",
    "    \n",
    "    # Function to get formatted epochs (from 1 not 0)\n",
    "    r_just_val = len(str(epochs))*2 + 3\n",
    "    estr = lambda e: f\"[{e + 1}/{epochs}]\".rjust(r_just_val)\n",
    "    \n",
    "    # Convenience function to print every `print_every` epochs.\n",
    "    def eprint(e,st):\n",
    "        if ((e + 1) % print_every == 0):\n",
    "            print(st,end=\"\")\n",
    "\n",
    "    def grad_times(t):\n",
    "        t = t*1000\n",
    "        return f\"{t:0.3f} ms\".rjust(10)\n",
    "    \n",
    "    # Convenience function for phase strings.\n",
    "    def statstr(phase,loss,accu,time,infr, rjust=True):\n",
    "        infr = grad_times(infr)\n",
    "        st =  f\"{phase} :: loss: {loss:0.4f} | accu: {accu:0.4f} | infr: {infr}  | time: {time} \\n\"\n",
    "        if rjust:\n",
    "            return st.rjust(r_just_val + len(st) + 3)\n",
    "        else:\n",
    "            return st\n",
    "\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Training \n",
    "    model.to(device)\n",
    "    for e in range(epochs):\n",
    "        epoch_time_start = time.time()\n",
    "        \n",
    "        # Training or validation phase\n",
    "        for phase in phases:\n",
    "            phase_time_start = time.time()\n",
    "\n",
    "            # Running metrics\n",
    "            running_loss = 0\n",
    "            running_corr = 0\n",
    "\n",
    "            is_tr = phase == TR\n",
    "            if is_tr:\n",
    "                  model.train()\n",
    "            else:\n",
    "                  model.eval()\n",
    "                  \n",
    "            # Keeping track of computation times.\n",
    "            inference_times = []\n",
    "            dl_p = dl[phase]\n",
    "            dl_l = len(dl_p)\n",
    "            if is_tr:\n",
    "                eprint(e,estr(e)+f\" - \")\n",
    "            for b,batch in enumerate(dl_p):\n",
    "                X,y = batch\n",
    "                bs = y.size(0)\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                \n",
    "                optim.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(is_tr):\n",
    "                    # Forward pass and log times \n",
    "                    inference_time_start = time.time()\n",
    "                    y_ = model(X)\n",
    "                    inference_time_end = time.time()\n",
    "                    inference_times.append((inference_time_end-inference_time_start)/bs)\n",
    "                    \n",
    "                    loss = loss_function(y_,y)\n",
    "                    if is_tr:\n",
    "                        # Backprop (grad calc and param update)\n",
    "                        loss.backward()\n",
    "                        optim.step()\n",
    "                        \n",
    "                    elif not is_tr and sched is not None:\n",
    "                        # Scheduler step\n",
    "                        if should_pass:\n",
    "                            sched.step(loss.item())\n",
    "                        else:\n",
    "                            sched.step()\n",
    "                    \n",
    "                running_loss += loss.item() * bs\n",
    "                running_corr += (y_.argmax(dim=1) == y).sum().item()\n",
    "                losses_all[phase].append(loss.item())\n",
    "             \n",
    "            # Calculate phase losses and scores\n",
    "            phase_loss = running_loss / sizes[phase]\n",
    "            phase_accu = running_corr / sizes[phase]\n",
    "                  \n",
    "            # Calculate timings\n",
    "            mean_inf = np.array(inference_times).mean()\n",
    "            # Log scores and losses\n",
    "            losses_epo[phase].append(phase_loss)\n",
    "            accuracy_epo[phase].append(phase_accu)\n",
    "                \n",
    "            # Update markers save best model\n",
    "            if not is_tr:\n",
    "                if phase_accu > best_accuracy:\n",
    "                    best_accuracy = phase_accu\n",
    "                    best_model = deepcopy(model.state_dict())\n",
    "                if phase_loss < least_loss:\n",
    "                    least_loss = phase_loss\n",
    "            \n",
    "            # Logging phase time\n",
    "            phase_time_end = time.time()\n",
    "            phase_time = ftime(phase_time_start,phase_time_end)\n",
    "            if is_tr:\n",
    "                eprint(e,statstr(phase, phase_loss,phase_accu, phase_time, mean_inf, False))\n",
    "            else:\n",
    "                eprint(e,statstr(phase, phase_loss,phase_accu, phase_time, mean_inf))\n",
    "        \n",
    "        # Logging epoch times\n",
    "        epoch_time_end = time.time()\n",
    "        epoch_time = f\"epoch time: {ftime(epoch_time_start, epoch_time_end)}\\n\"\n",
    "        eprint(e,epoch_time.rjust(len(epoch_time)+r_just_val+3)+\"\\n\")\n",
    "    \n",
    "    if load_best:\n",
    "        model.load_state_dict(best_model)\n",
    "    \n",
    "    # ----------------------------\n",
    "    total_time_end = time.time()\n",
    "    total_time = ftime(total_time_start,total_time_end)\n",
    "    print(f\"least loss: {least_loss:0.4f} | best accu: {best_accuracy:0.4f} | total time taken: {total_time}\")\n",
    "    \n",
    "    return {\"losses_epo\":losses_epo,\"accuracy_epo\":accuracy_epo,\"losses_all\":losses_all}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
