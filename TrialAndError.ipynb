{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Grounds - Fit Loop\n",
    "Creating a library that helps with the pytorch looping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T18:42:01.944048Z",
     "start_time": "2020-05-05T18:42:01.940135Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim, nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.datasets import FakeData\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T18:35:09.458596Z",
     "start_time": "2020-05-05T18:35:09.437906Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'typing' from '/Users/alan/opt/anaconda3/envs/data_sci/lib/python3.7/typing.py'>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T18:19:19.138293Z",
     "start_time": "2020-05-05T18:19:18.920323Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Basic setup \n",
    "\"\"\"\n",
    "\n",
    "sets = ['train','valid','test']\n",
    "TR, VA, TE = sets\n",
    "class_names = ['a','b','c','d']\n",
    "num_classes = 4\n",
    "\n",
    "sz = {s:z for s, z in zip(sets,[1024,128,256])}\n",
    "ds = {s:FakeData(size=sz[s], transform=ToTensor(), num_classes=num_classes) for s in sets}\n",
    "dl = {s:DataLoader(ds[s],batch_size=16) for s in ds}\n",
    "\n",
    "model = resnet18()\n",
    "in_features = model.fc.in_features\n",
    "model.fc = nn.Linear(in_features, num_classes)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T18:20:29.302782Z",
     "start_time": "2020-05-05T18:20:29.222507Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Batch for testing purposes\n",
    "\"\"\"\n",
    "batch = next(iter(dl[TR]))\n",
    "X, y = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T23:58:31.206152Z",
     "start_time": "2020-05-05T23:58:31.185037Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from typing import Union, List, Callable, Optional, Any, Dict, Tuple\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import _LRScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T18:42:42.403644Z",
     "start_time": "2020-05-05T18:42:42.400202Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dict passed from from one of the states should be accesible from the loop state through `__getitem__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T23:41:37.126686Z",
     "start_time": "2020-05-05T23:41:37.117359Z"
    }
   },
   "outputs": [],
   "source": [
    "def somefunc():\n",
    "    return 22\n",
    "class A:\n",
    "    def __init__(self):\n",
    "        self.s = somefunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoopMetrics:\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T23:45:32.798697Z",
     "start_time": "2020-05-05T23:45:32.793232Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'asdf' in {'asdf':22}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T00:03:12.925870Z",
     "start_time": "2020-05-06T00:03:12.902985Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- every step function hook receives the LoopState object.\n",
    "- The loop state object should have a copy of all the values returned from the function hook\n",
    "- example the below returned dict values should be avialable in the LoopState object\n",
    "\n",
    "```python\n",
    "def train_step(state):\n",
    "    X,y = state() # should device cast automatically\n",
    "    y_ = state.model(X)\n",
    "    loss = state.loss_function(y_, y)\n",
    "    \n",
    "    state.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    state.optimizer.step()\n",
    "    state.lr_scheduler.step() \n",
    "    \n",
    "    loss = loss.item()\n",
    "    batch_loss = loss * y.size()\n",
    "    batch_corr = (y_.argmax(dim=0) == y).sum().float().item()\n",
    "    \n",
    "    return {'loss':loss,'batch_loss':batch_loss:'batch_corr'}\n",
    "```\n",
    "- The LoopState object should be cleared of the above values at the start \n",
    "  of the next epoch.\n",
    "- The returned values should be available through the FitLoop object\n",
    "  Eg: `FitLoop.train.batch['loss']`\n",
    "- The returned value should be optionally available by setting the flag \n",
    "  `store_batch_metrics`\n",
    "\n",
    "```python\n",
    "def train_epoch_end(state):\n",
    "    loss = state['loss']\n",
    "    batch_loss = state['batch_loss']\n",
    "    batch_corr = state['batch_corr']\n",
    "    \n",
    "    size = state.sz\n",
    "    \n",
    "    epoch_loss = batch_loss.sum().item()/size\n",
    "    epoch_accu = batch_corr.sum().item()/size\n",
    "    \n",
    "    return {\"loss\":epoch_loss,\"accu\":epoch_accu}\n",
    "```\n",
    "\n",
    "- The returned values should be available through the FitLoop object\n",
    "  Eg: `FitLoop.train.epoch['loss']`\n",
    "- For each phase a different LoopState obect is maintained.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T19:13:24.960941Z",
     "start_time": "2020-05-05T19:13:24.957841Z"
    }
   },
   "outputs": [],
   "source": [
    "class LoopState:\n",
    "    __batch = 'batch'\n",
    "    __epoch = 'epoch'\n",
    "    def __init__(self, phase:str, floop:FitLoop):\n",
    "        self.metrics = {}\n",
    "        self._batch = ()\n",
    "        \n",
    "        # Passed down attributes\n",
    "        self.model = floop.model\n",
    "        self.optimizer = floop.optimizer\n",
    "        self.loss_function = floop.loss_function\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    def batch(self) -> Tuple[...]:\n",
    "        return (d.to(self.device) for d in self._batch)\n",
    "    \n",
    "    @batch.setter\n",
    "    def batch(self, current_batch:Tuple[...]) -> None:\n",
    "        self._batch = current_batch\n",
    "    \n",
    "    def append(self, rdict:Dict[str, float], stage:str) -> None:\n",
    "        #  Append metrics to the specific stage.\n",
    "        if stage not in self.metrics:\n",
    "            self.metrics[stage] = {}\n",
    "            \n",
    "        for key in rdict:\n",
    "            if key not in self.metrics[stage]:\n",
    "                self.metrics[stage][key] = []\n",
    "            self.metrics[stage][key].append(rdict[key])\n",
    "    \n",
    "    def clear(self, stage:str) -> None:\n",
    "        # Clear the batch metrics at the end of the batch.\n",
    "        for m in self.metrics[stage]:\n",
    "            m.clear()\n",
    "            \n",
    "    def append_batch(self, rdict:Dict[str, float]) -> None\n",
    "        self.append(rdict, self.__batch)\n",
    "        \n",
    "    def append_epoch(self, rdict:Dict[str, float]) -> None\n",
    "        self.append(rdict, self.__epoch)\n",
    "            \n",
    "    def clear_batch(self) -> None\n",
    "        self.clear(self.__batch)\n",
    "        \n",
    "    def clear_epoch(self) -> None\n",
    "        self.clear(self.__epoch)\n",
    "    \n",
    "    def \n",
    "    \n",
    "    def __getitem__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T20:11:06.599285Z",
     "start_time": "2020-05-05T20:11:06.543248Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "non-default argument follows default argument (<ipython-input-99-b9ca058edd5d>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-99-b9ca058edd5d>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    def __init__(self,\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m non-default argument follows default argument\n"
     ]
    }
   ],
   "source": [
    "class FitLoop:\n",
    "    sets = ['train','valid','test']\n",
    "    TR, VA, TE = sets\n",
    "    def __init__(self, \n",
    "                 # Basic blocks\n",
    "                 model: Module, optimizer: Union[Optimizer,List[Optimizer]], \n",
    "                 loss_function: Callable[[Tensor,Tensor],Tensor], \n",
    "                 \n",
    "                 # DataLoader\n",
    "                 train_dl: DataLoader, \n",
    "                 valid_dl: Optional[DataLoader]=None, \n",
    "                 test_dl: Optional[DataLoader]=None, \n",
    "                 \n",
    "                 # Batch Step\n",
    "                 train_step: Callable[[LoopState],Dict[str, Any]],\n",
    "                 valid_step: Optional[Callable[[LoopState],Dict[str, Any]]]=None,\n",
    "                 test_step: Optional[Callable[[LoopState],Dict[str, Any]]]=None,\n",
    "                 \n",
    "                 # Epoch Start Step\n",
    "                 train_epoch_start: Callable[[LoopState],Dict[str, Any]],\n",
    "                 valid_epoch_start: Optional[Callable[[LoopState],Dict[str, Any]]]=None,\n",
    "                 test_epoch_start: Optional[Callable[[LoopState],Dict[str, Any]]]=None,\n",
    "                 \n",
    "                 # Epoch End Step\n",
    "                 train_epoch_end: Callable[[LoopState],Dict[str, Any]],\n",
    "                 valid_epoch_end: Optional[Callable[[LoopState],Dict[str, Any]]]=None,\n",
    "                 test_epoch_end: Optional[Callable[[LoopState],Dict[str, Any]]]=None,\n",
    "                 \n",
    "                 # Other Args\n",
    "                 lr_scheduler: Optional[_LRScheduler, Any, List[_LRScheduler,Any]]=None,\n",
    "                 device: torch.device=torch.device('cpu'), \n",
    "                 dtype: torch.dtype=torch.float32\n",
    "                ) -> None:\n",
    "        \"\"\"\n",
    "        FitLoop constructor\n",
    "        ----\n",
    "        Parameters:\n",
    "        # Basic Blocks\n",
    "            The bare minimum required along with train_dl, train_step and train_epoch_end.\n",
    "            - model : nn.Module model that has to be trained\n",
    "            - optimizer : an optimizer from torch.optim\n",
    "            - loss_function : function to compute loss\n",
    "         \n",
    "        # DataLoader\n",
    "            - train_dl : training DataLoader\n",
    "            - valid_dl : validation DataLoader, if None validation will be ignored\n",
    "            - test_dl : testing DataLoader, if None `.test()` will not run\n",
    "         \n",
    "        # Batch Step\n",
    "            Functions that take in a LoopState object to perform \n",
    "            required calculations, functions should return a dict with values\n",
    "            to be used in the epoch end step.\n",
    "            - train_step : portion of the loop where forward and backward \n",
    "                passes take place.\n",
    "            - valid_step : validation portion of the loop.\n",
    "            - test_step : called when `FitLoop.test()` is called.\n",
    "        \n",
    "        # Epoch Start Step\n",
    "            TODO: NEED TO IMPLEMENT\n",
    "            - train_epoch_start :\n",
    "            - valid_epoch_start :\n",
    "            - test_epoch_start : \n",
    "        \n",
    "        # Epoch End Step\n",
    "            Functions that take in a LoopState object to perform \n",
    "            required calculations, functions should return a dict with values\n",
    "            that are to be returned when the loop is over.\n",
    "            - train_epoch_end : after training epoch has ended.\n",
    "            - valid_epoch_end : after validation epoch has ended.\n",
    "            - test_epoch_end : called when the test loop is done, one iteration\n",
    "                over all batches in the test dataloader.\n",
    "        \n",
    "        # Other Args\n",
    "            - lr_scheduler : scheduler from torch.optim.lr_scheduler\n",
    "            - device : torch.device model will be cast to device this prior to the loop\n",
    "            - dtype : dtype to cast model and data to\n",
    "        \"\"\"\n",
    "        # Basic Blocks\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_function = loss_function\n",
    "        \n",
    "        # DataLoaders\n",
    "        self.train_dl = train_dl\n",
    "        self.valid_dl = valid_dl\n",
    "        self.test_dl = test_dl\n",
    "        \n",
    "        # Batch Step\n",
    "        self.train_step = train_step\n",
    "        self.valid_step = valid_step\n",
    "        self.test_step = test_step\n",
    "        \n",
    "        # Epoch Start Step\n",
    "        self.train_epoch_start = train_epoch_start\n",
    "        self.valid_epoch_start = valid_epoch_start\n",
    "        self.test_epoch_start = test_epoch_start\n",
    "        \n",
    "        # Epoch End Step\n",
    "        self.train_epoch_end = train_epoch_end\n",
    "        self.valid_epoch_end = valid_epoch_end\n",
    "        self.test_epoch_end = test_epoch_end\n",
    "        \n",
    "        # Other Args\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        # Epoch Num\n",
    "        self.epoch_num = 0\n",
    "        \n",
    "    def __ftime(t1:float,t2:float)->str:\n",
    "        t = t2-t1\n",
    "        s, ms = str(t).split('.')\n",
    "        ms = ms[:3]\n",
    "        s = str(int(s)%60).rjust(2)\n",
    "        m = int(t//60)\n",
    "        h = str(m//60)\n",
    "        m = str(m % 60).rjust(2)\n",
    "        u = ['h','m','s','ms']\n",
    "        v = [h,m,s,ms]\n",
    "        t = filter(lambda x:int(x[0]) > 0,zip(v,u))\n",
    "        return ' '.join([' '.join(x) for x in t]).ljust(20)\n",
    "    \n",
    "    def __time(profiler:bool) -> Optional[float]:\n",
    "        if profiler:\n",
    "            return time.time()\n",
    "    \n",
    "    def __call_batch_step(self, state:LoopState) -> None:\n",
    "        step_funcs = [self.train_step, self.valid_step, self.test_step]\n",
    "        step_funcs = {s:f for s,f in zip(self.sets, step_func)}\n",
    "        step_func = step_funcs[state.phase]\n",
    "        \n",
    "        if step_func is None:\n",
    "            raise AttributeError(f\"{phase}_step not assigned\")\n",
    "        state_dict = step_func(phase)\n",
    "        # To Do Alter LoopState according to the state_dict\n",
    "        \n",
    "        \n",
    "    def __call_epoch_start_step(self, state:LoopState) -> Dict[str,Any]:\n",
    "        step_funcs = [self.train_epoch_start,self.valid_epoch_start,self.test_epoch_start]\n",
    "        step_funcs = {s:f for s,f in zip(self.sets, step_func)}\n",
    "        step_func = step_funcs[state.phase]\n",
    "        \n",
    "        if step_func is None:\n",
    "            raise AttributeError(f\"{phase}_step not assigned\")\n",
    "        state_dict = step_func(state)\n",
    "        # To Do Alter LoopState according to the state_dict\n",
    "        \n",
    "    def __call_epoch_end_step(self, phase:str) -> Dict[str,Any]:\n",
    "        step_funcs = [self.train_epoch_end,self.valid_epoch_end,self.test_epoch_end]\n",
    "        step_funcs = {s:f for s,f in zip(self.sets, step_func)}\n",
    "        step_func = step_funcs[state.phase]\n",
    "        \n",
    "        if step_func is None:\n",
    "            raise AttributeError(f\"{phase}_step not assigned\")\n",
    "        state_dict = step_func(state)\n",
    "        # To Do Alter LoopState according to the state_dict\n",
    "        \n",
    "    def __get_dl(self, is_test:bool)-> Dict[str,DataLoader]:\n",
    "        if is_test:\n",
    "            if self.test_dl is None:\n",
    "                raise AttributeError(\"test_dl not assigned\")\n",
    "            return {self.TE:self.test_dl}\n",
    "        v = self.valid_dl is None\n",
    "        if not v:\n",
    "            return {TR:self.train_dl, VA:self.valid_dl}\n",
    "        else:\n",
    "            return {TR:self.train_dl}\n",
    "    \n",
    "    def __get_phase_list(self, is_test):\n",
    "        if is_test:\n",
    "            return [self.TE]\n",
    "        \n",
    "        va_dl = self.valid_dl is not None\n",
    "        va_st = self.valid_step is not None\n",
    "        \n",
    "        if va_dl and va_st:\n",
    "            return [self.TR, self.VA]\n",
    "        else:\n",
    "            return [self.TR]\n",
    "    \n",
    "    def __loop(self, \n",
    "            epochs:int=1, print_every:int=1, steps: Optional[int],\n",
    "            criteria:Union[str, None]=None, load_best:bool=False, \n",
    "            profiler:bool=False, is_test:bool=False,\n",
    "            track_batch_metrics:bool=True\n",
    "           ):\n",
    "        \"\"\"\n",
    "        Runs the training loop for `epochs`\n",
    "        ----\n",
    "        Parameters\n",
    "         - epochs : should be a non negative integer\n",
    "         - print_every : if 0 will not print, else will print at given epoch\n",
    "         - steps : number of batches to run in each phase [train,valid] \n",
    "             for check if everything is working.\n",
    "         - criteria : value in the validation epoch end return dict\n",
    "             that used is used to define a better model, eg: 'accuracy'\n",
    "         - load_best : whether to load the best model after training for `epochs`\n",
    "         - profiler : whether to keep track of time taken by various sections\n",
    "         - is_test : whether it is a model testing loop or training/validation loop\n",
    "         - track_batch_metrics : whether to store the values returned in the batch steps\n",
    "        \n",
    "        \"\"\"\n",
    "        t = lambda : self.__time(print_every != 0) # Returns the time \n",
    "        \n",
    "        total_time_start = t()\n",
    "        \n",
    "        # ----------------------------\n",
    "        phases = self.__get_phase_list(is_test)\n",
    "        dl = self.__get_dl(is_test)\n",
    "        sz = {k:len(dl[k].dataset) for k in dl}\n",
    "        state = {}\n",
    "\n",
    "        # Markers\n",
    "        least_loss = float('inf')\n",
    "        best_score = float('-inf') # value used save best model\n",
    "        best_model = deepcopy(model.state_dict())\n",
    "\n",
    "        # ----------------------------\n",
    "        # Convenience functions (that aren't be used elsewhere)\n",
    "\n",
    "        # Function to get formatted epochs (from 1 not 0)\n",
    "        r_just_val = len(str(epochs))*2 + 3\n",
    "        estr = lambda e: f\"[{e + 1}/{epochs}]\".rjust(r_just_val)\n",
    "\n",
    "        # Convenience function to print every `print_every` epochs.\n",
    "        def eprint(e,st):\n",
    "            if (e == 0) and (print_every != 0):\n",
    "                print(st,end=\"\")\n",
    "            elif (e + 1) % print_every == 0:\n",
    "                print(st,end=\"\")\n",
    "\n",
    "        def grad_times(t):\n",
    "            t = t*1000\n",
    "            return f\"{t:0.3f} ms\".rjust(10)\n",
    "\n",
    "        # Convenience function for phase strings.\n",
    "        def statstr(phase, loss, accu, time, infr, rjust=True):\n",
    "            infr = grad_times(infr)\n",
    "            st =  f\"{phase} :: loss: {loss:0.4f} | accu: {accu:0.4f} | infr: {infr}  | time: {time} \\n\"\n",
    "            if rjust:\n",
    "                return st.rjust(r_just_val + len(st) + 3)\n",
    "            else:\n",
    "                return st\n",
    "\n",
    "\n",
    "        # ----------------------------\n",
    "        # Training \n",
    "        model = self.model.to(device)\n",
    "        for e in range(epochs):\n",
    "            epoch_time_start = t()\n",
    "            \n",
    "            \"\"\"\n",
    "            TODO : Re-initilize LoopState\n",
    "            \"\"\"\n",
    "            # for st in state do something\n",
    "            \n",
    "            # Training or validation phase\n",
    "            for phase in phases:\n",
    "                phase_time_start = t()\n",
    "                \n",
    "                # EPOCH START STEP START\n",
    "                epoch_start_step = self.__call_epoch_start_step(state[phase])\n",
    "                # EPOCH START STEP END\n",
    "                \n",
    "                is_tr = phase == TR\n",
    "                if is_tr:\n",
    "                      model.train()\n",
    "                else:\n",
    "                      model.eval()\n",
    "\n",
    "                # Keeping track of computation times.\n",
    "                inference_times = []\n",
    "                dl_p = dl[phase]\n",
    "                dl_l = len(dl_p)\n",
    "                \n",
    "                if is_tr:\n",
    "                    eprint(e,estr(e)+f\" - \")\n",
    "                    \n",
    "                for b,batch in enumerate(dl_p):\n",
    "                    # BATCH STEP START\n",
    "                    batch_step = self.__call_batch_step(state[phase])\n",
    "                    # BATCH STEP END\n",
    "                    \n",
    "                    \"\"\"\n",
    "                    Code below goes in to above block\n",
    "                    \"\"\"\n",
    "                    \n",
    "                    ## In the function\n",
    "#                     X,y = batch\n",
    "#                     bs = y.size(0)\n",
    "#                     X = X.to(device)\n",
    "#                     y = y.to(device)\n",
    "\n",
    "                    ## Auto set\n",
    "#                     optim.zero_grad()\n",
    "\n",
    "                    ## Auto set\n",
    "#                     with torch.set_grad_enabled(is_tr):\n",
    "#                         # Forward pass and log times \n",
    "#                         inference_time_start = time.time()\n",
    "#                         y_ = model(X)\n",
    "#                         inference_time_end = time.time()\n",
    "#                         inference_times.append((inference_time_end-inference_time_start)/bs)\n",
    "\n",
    "#                         loss = loss_function(y_,y)\n",
    "#                         if is_tr:\n",
    "#                             # Backprop (grad calc and param update)\n",
    "#                             loss.backward()\n",
    "#                             optim.step()\n",
    "\n",
    "#                         elif not is_tr and sched is not None:\n",
    "#                             # Scheduler step\n",
    "#                             if should_pass:\n",
    "#                                 sched.step(loss.item())\n",
    "#                             else:\n",
    "#                                 sched.step()\n",
    "\n",
    "#                     running_loss += loss.item() * bs\n",
    "#                     running_corr += (y_.argmax(dim=1) == y).sum().item()\n",
    "#                     losses_all[phase].append(loss.item())\n",
    "\n",
    "\n",
    "                # EPOCH END STEP START\n",
    "                self.__call_epoch_end_step(state[phase])\n",
    "                # EPOCH END STEP END\n",
    "            \n",
    "                # Calculate phase losses and scores\n",
    "                phase_loss = running_loss / sizes[phase]\n",
    "                phase_accu = running_corr / sizes[phase]\n",
    "\n",
    "                # Calculate timings\n",
    "                mean_inf = np.array(inference_times).mean()\n",
    "                # Log scores and losses\n",
    "                losses_epo[phase].append(phase_loss)\n",
    "                accuracy_epo[phase].append(phase_accu)\n",
    "\n",
    "                # Update markers save best model\n",
    "                if not is_tr:\n",
    "                    if phase_accu > best_accuracy:\n",
    "                        best_accuracy = phase_accu\n",
    "                        best_model = deepcopy(model.state_dict())\n",
    "                    if phase_loss < least_loss:\n",
    "                        least_loss = phase_loss\n",
    "\n",
    "                # Logging phase time\n",
    "                phase_time_end = time.time()\n",
    "                phase_time = ftime(phase_time_start,phase_time_end)\n",
    "                if is_tr:\n",
    "                    eprint(e,statstr(phase, phase_loss,phase_accu, phase_time, mean_inf, False))\n",
    "                else:\n",
    "                    eprint(e,statstr(phase, phase_loss,phase_accu, phase_time, mean_inf))\n",
    "\n",
    "            # Logging epoch times\n",
    "            epoch_time_end = time.time()\n",
    "            epoch_time = f\"epoch time: {ftime(epoch_time_start, epoch_time_end)}\\n\"\n",
    "            eprint(e,epoch_time.rjust(len(epoch_time)+r_just_val+3)+\"\\n\")\n",
    "\n",
    "        if load_best:\n",
    "            model.load_state_dict(best_model)\n",
    "\n",
    "        # ----------------------------\n",
    "        total_time_end = time.time()\n",
    "        total_time = ftime(total_time_start,total_time_end)\n",
    "        print(f\"least loss: {least_loss:0.4f} | best accu: {best_accuracy:0.4f} | total time taken: {total_time}\")\n",
    "\n",
    "        return {\"losses_epo\":losses_epo,\"accuracy_epo\":accuracy_epo,\"losses_all\":losses_all}\n",
    "    \n",
    "    def fit(self, \n",
    "            epochs:int=1, print_every:int=1, \n",
    "            criteria:Union[str, None]=None, load_best:bool=False, \n",
    "            profiler:bool=False\n",
    "           ):\n",
    "        \"\"\"\n",
    "        Runs the training loop for `epochs`\n",
    "        ----\n",
    "        Parameters\n",
    "         - epochs : should be a non negative integer\n",
    "         \n",
    "         - print_every : if 0 will not print, else will print at given epoch\n",
    "         - criteria : value in the validation epoch end return dict\n",
    "             that used is used to define a better model, eg: 'accuracy'\n",
    "         - load_best : whether to load the best model after training for `epochs`\n",
    "         - profiler : whether to keep track of time taken by various sections\n",
    "        \n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FitLoop\n",
    "- should keep track of epochs that have been completed\n",
    "- epoch_number can be reset \n",
    "- metrics can be cleared\n",
    "- `FitLoop.set_name.loop_stage['metric_name']` to access the metric\n",
    "- `FitLoop.set_name.loop_stage['metric_name']` to access the metric\n",
    "- `FitLoop.store_pretrained:bool` arg to store the pretrained weights before training\n",
    "    if path then store at given path else store in memory.\n",
    "- `FitLoop.reset(reset_model:bool)` to clear metrics, epoch_num and to reset the model, to pretrained state\n",
    "    will load the weight from passed path else from memory.\n",
    "- `FitLoop.save(path:str)` to save the model and training state somehow even the fitloop state.\n",
    "- `FitLoop.load(path:str)` to load the FitLoop state from given path.\n",
    "- Some basic step should be used such that one can use it without defining step functions.\n",
    "- `FitLoop.fit(continue:[bool,int]=False)` ask after `int` whether to continue training or to end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LoopState\n",
    "- should cast the batch to device before passing it using `state.batch()`\n",
    "- should get the batch num `state.batch_num` and epoch num `state.epoch_num`\n",
    "- the model, optimizer, loss_function, lr_scheduler should be available\n",
    "    `state.model`, `state.optimizer`, `state.loss_function`, `state.lr_scheduler`\n",
    "- should return the batch metrics as float tensors using square bracket indexing\n",
    "    `state['loss']` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portions of the loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Single step** Portion of the loop that receives the batch and will cal forward pass on it.\n",
    "    - *Set* zero grad\n",
    "    - **Train Step** \n",
    "        - *Set* \n",
    "            - enable gradients\n",
    "            - model.train\n",
    "        - *Define*: \n",
    "            - compute loss\n",
    "            - call backward \n",
    "            - update gradients and maybe scheduler step\n",
    "            - loss and whatever metrics will be saved in a list.\n",
    "    - **Valid Step**\n",
    "        - *Set*\n",
    "            - disable gradients\n",
    "            - model.eval\n",
    "        - *Define*: \n",
    "            - compute loss\n",
    "            - loss and whatever metrics will be saved in a list.\n",
    "    - **Test Step** Only in test loop\n",
    "        - *Set*\n",
    "            - disable gradients\n",
    "            - model.eval\n",
    "        - *Define*: \n",
    "            - compute loss\n",
    "            - loss and whatever metrics will be saved in a list.\n",
    "- **Pre Epoch**\n",
    "- **Post Epoch**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate a time string from\n",
    "# 2 instances of time.time\n",
    "def ftime(t1,t2):\n",
    "    t = t2-t1\n",
    "    s, ms = str(t).split('.')\n",
    "    ms = ms[:3]\n",
    "    s = str(int(s)%60).rjust(2)\n",
    "    m = int(t//60)\n",
    "    h = str(m//60)\n",
    "    m = str(m % 60).rjust(2)\n",
    "    u = ['h','m','s','ms']\n",
    "    v = [h,m,s,ms]\n",
    "    t = filter(lambda x:int(x[0]) > 0,zip(v,u))\n",
    "    return ' '.join([' '.join(x) for x in t]).ljust(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, dl_train, dl_valid, optim, loss_function, epochs, sched=None, should_pass=False, print_every=1, load_best=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    # model\n",
    "      - pytorch model to be trained\n",
    "      \n",
    "    # dl_train\n",
    "      - Dataloader for the train set\n",
    "      \n",
    "    # dl_valid\n",
    "      - Dataloader for the valid set\n",
    "      \n",
    "    # optimizer\n",
    "      - optimizer attached to model params from torch.optim\n",
    "      \n",
    "    # loss_function\n",
    "      - function to calculate the loss; loss_function(model(X),y)\n",
    "      \n",
    "    # epochs\n",
    "      - number of epochs to train for\n",
    "      \n",
    "    # device\n",
    "      - torch.device to load the model to \n",
    "      \n",
    "    # sched\n",
    "      - scheduler for the optimizer learning rate\n",
    "      \n",
    "    # should_pass\n",
    "      - if loss should be passed to the scheduler.\n",
    "      \n",
    "    # print_every\n",
    "      - to print the loss every n epochs\n",
    "    \n",
    "    # load_best \n",
    "      - whether to load the best model as determined by accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    total_time_start = time.time()\n",
    "    # ----------------------------\n",
    "    phases = [TR,VA]\n",
    "    dl = {TR:dl_train, VA:dl_valid}\n",
    "    sizes = {k:len(dl[k].dataset) for k in dl}\n",
    "    \n",
    "    # To contain metrics to plot later\n",
    "    losses_all = {TR:[],VA:[]}\n",
    "    losses_epo = {TR:[],VA:[]}\n",
    "    accuracy_epo = {TR:[],VA:[]}\n",
    "    \n",
    "    # Markers\n",
    "    least_loss = float('inf')\n",
    "    best_accuracy = 0\n",
    "    best_model = deepcopy(model.state_dict())\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Convenience functions (that won't be used elsewhere)\n",
    "    \n",
    "    # Function to get formatted epochs (from 1 not 0)\n",
    "    r_just_val = len(str(epochs))*2 + 3\n",
    "    estr = lambda e: f\"[{e + 1}/{epochs}]\".rjust(r_just_val)\n",
    "    \n",
    "    # Convenience function to print every `print_every` epochs.\n",
    "    def eprint(e,st):\n",
    "        if ((e + 1) % print_every == 0):\n",
    "            print(st,end=\"\")\n",
    "\n",
    "    def grad_times(t):\n",
    "        t = t*1000\n",
    "        return f\"{t:0.3f} ms\".rjust(10)\n",
    "    \n",
    "    # Convenience function for phase strings.\n",
    "    def statstr(phase,loss,accu,time,infr, rjust=True):\n",
    "        infr = grad_times(infr)\n",
    "        st =  f\"{phase} :: loss: {loss:0.4f} | accu: {accu:0.4f} | infr: {infr}  | time: {time} \\n\"\n",
    "        if rjust:\n",
    "            return st.rjust(r_just_val + len(st) + 3)\n",
    "        else:\n",
    "            return st\n",
    "\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Training \n",
    "    model.to(device)\n",
    "    for e in range(epochs):\n",
    "        epoch_time_start = time.time()\n",
    "        \n",
    "        # Training or validation phase\n",
    "        for phase in phases:\n",
    "            phase_time_start = time.time()\n",
    "\n",
    "            # Running metrics\n",
    "            running_loss = 0\n",
    "            running_corr = 0\n",
    "\n",
    "            is_tr = phase == TR\n",
    "            if is_tr:\n",
    "                  model.train()\n",
    "            else:\n",
    "                  model.eval()\n",
    "                  \n",
    "            # Keeping track of computation times.\n",
    "            inference_times = []\n",
    "            dl_p = dl[phase]\n",
    "            dl_l = len(dl_p)\n",
    "            if is_tr:\n",
    "                eprint(e,estr(e)+f\" - \")\n",
    "            for b,batch in enumerate(dl_p):\n",
    "                X,y = batch\n",
    "                bs = y.size(0)\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                \n",
    "                optim.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(is_tr):\n",
    "                    # Forward pass and log times \n",
    "                    inference_time_start = time.time()\n",
    "                    y_ = model(X)\n",
    "                    inference_time_end = time.time()\n",
    "                    inference_times.append((inference_time_end-inference_time_start)/bs)\n",
    "                    \n",
    "                    loss = loss_function(y_,y)\n",
    "                    if is_tr:\n",
    "                        # Backprop (grad calc and param update)\n",
    "                        loss.backward()\n",
    "                        optim.step()\n",
    "                        \n",
    "                    elif not is_tr and sched is not None:\n",
    "                        # Scheduler step\n",
    "                        if should_pass:\n",
    "                            sched.step(loss.item())\n",
    "                        else:\n",
    "                            sched.step()\n",
    "                    \n",
    "                running_loss += loss.item() * bs\n",
    "                running_corr += (y_.argmax(dim=1) == y).sum().item()\n",
    "                losses_all[phase].append(loss.item())\n",
    "             \n",
    "            # Calculate phase losses and scores\n",
    "            phase_loss = running_loss / sizes[phase]\n",
    "            phase_accu = running_corr / sizes[phase]\n",
    "                  \n",
    "            # Calculate timings\n",
    "            mean_inf = np.array(inference_times).mean()\n",
    "            # Log scores and losses\n",
    "            losses_epo[phase].append(phase_loss)\n",
    "            accuracy_epo[phase].append(phase_accu)\n",
    "                \n",
    "            # Update markers save best model\n",
    "            if not is_tr:\n",
    "                if phase_accu > best_accuracy:\n",
    "                    best_accuracy = phase_accu\n",
    "                    best_model = deepcopy(model.state_dict())\n",
    "                if phase_loss < least_loss:\n",
    "                    least_loss = phase_loss\n",
    "            \n",
    "            # Logging phase time\n",
    "            phase_time_end = time.time()\n",
    "            phase_time = ftime(phase_time_start,phase_time_end)\n",
    "            if is_tr:\n",
    "                eprint(e,statstr(phase, phase_loss,phase_accu, phase_time, mean_inf, False))\n",
    "            else:\n",
    "                eprint(e,statstr(phase, phase_loss,phase_accu, phase_time, mean_inf))\n",
    "        \n",
    "        # Logging epoch times\n",
    "        epoch_time_end = time.time()\n",
    "        epoch_time = f\"epoch time: {ftime(epoch_time_start, epoch_time_end)}\\n\"\n",
    "        eprint(e,epoch_time.rjust(len(epoch_time)+r_just_val+3)+\"\\n\")\n",
    "    \n",
    "    if load_best:\n",
    "        model.load_state_dict(best_model)\n",
    "    \n",
    "    # ----------------------------\n",
    "    total_time_end = time.time()\n",
    "    total_time = ftime(total_time_start,total_time_end)\n",
    "    print(f\"least loss: {least_loss:0.4f} | best accu: {best_accuracy:0.4f} | total time taken: {total_time}\")\n",
    "    \n",
    "    return {\"losses_epo\":losses_epo,\"accuracy_epo\":accuracy_epo,\"losses_all\":losses_all}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToDo\n",
    "- Figure out the portions of the loop.\n",
    "- CPU, GPU, RAM profiling also along with time take to run."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
