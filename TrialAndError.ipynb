{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Grounds - Fit Loop\n",
    "Creating a library that helps with the pytorch looping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T00:18:11.035533Z",
     "start_time": "2020-05-06T00:18:09.166498Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim, nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.datasets import FakeData\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T00:39:04.169062Z",
     "start_time": "2020-05-06T00:39:03.949778Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Basic setup with FakeData for testing.\n",
    "\"\"\"\n",
    "\n",
    "sets = ['train','valid','test']\n",
    "TR, VA, TE = sets\n",
    "class_names = ['a','b','c','d']\n",
    "num_classes = 4\n",
    "\n",
    "sz = {s:z for s, z in zip(sets,[1024,128,256])}\n",
    "ds = {s:FakeData(size=sz[s], transform=ToTensor(), num_classes=num_classes) for s in sets}\n",
    "dl = {s:DataLoader(ds[s],batch_size=16) for s in ds}\n",
    "\n",
    "model = resnet18()\n",
    "in_features = model.fc.in_features\n",
    "model.fc = nn.Linear(in_features, num_classes)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "\"\"\"\n",
    "Batch for testing purposes\n",
    "\"\"\"\n",
    "X, y = next(iter(dl[TR]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T03:04:14.822745Z",
     "start_time": "2020-05-06T03:04:14.818693Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "from uuid import uuid4\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from typing import Union, List, Callable, Optional, Any, Dict, Tuple\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import _LRScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T00:18:21.216895Z",
     "start_time": "2020-05-06T00:18:21.212259Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T00:26:27.425435Z",
     "start_time": "2020-05-06T00:26:27.418596Z"
    }
   },
   "outputs": [],
   "source": [
    "class A():\n",
    "    def __init__(self, f):\n",
    "        self.__f = f\n",
    "        self.a = 22\n",
    "    \n",
    "    def __getitem__(self, name):\n",
    "        return name\n",
    "        \n",
    "    def __getattr__(self,name):\n",
    "        return getattr(self.__f,name)\n",
    "class B():\n",
    "    def __init__(self):\n",
    "        self.b = 33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LoopState\n",
    "- ✅should cast the batch to device before passing it using `state.batch()`\n",
    "- ✅should get the batch num `state.batch_num` and epoch num `state.epoch_num`\n",
    "- ✅the model, optimizer, loss_function, lr_scheduler should be available\n",
    "    `state.model`, `state.optimizer`, `state.loss_function`, `state.lr_scheduler`\n",
    "- ✅should return the batch metrics as float tensors using square bracket indexing\n",
    "    `state['loss']` \n",
    "    - every step function hook receives the LoopState object.\n",
    "- ✅The loop state object should have a copy of all the values returned from the function hook\n",
    "- ✅example the below returned dict values should be avialable in the LoopState object\n",
    "\n",
    "```python\n",
    "def train_step(state):\n",
    "    X,y = state() # should device cast automatically\n",
    "    y_ = state.model(X)\n",
    "    loss = state.loss_function(y_, y)\n",
    "    \n",
    "    state.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    state.optimizer.step()\n",
    "    state.lr_scheduler.step() \n",
    "    \n",
    "    loss = loss.item()\n",
    "    batch_loss = loss * y.size()\n",
    "    batch_corr = (y_.argmax(dim=0) == y).sum().float().item()\n",
    "    \n",
    "    return {'loss':loss,'batch_loss':batch_loss:'batch_corr'}\n",
    "```\n",
    "- The LoopState object should be cleared of the above values at the start \n",
    "  of the next epoch.\n",
    "- The returned values should be available through the FitLoop object\n",
    "  Eg: `FitLoop.train.batch['loss']`\n",
    "- The (above statement) returned value should be optionally available by setting the flag \n",
    "  `store_batch_metrics`\n",
    "\n",
    "```python\n",
    "def train_epoch_end(state):\n",
    "    loss = state['loss']\n",
    "    batch_loss = state['batch_loss']\n",
    "    batch_corr = state['batch_corr']\n",
    "    \n",
    "    size = state.size\n",
    "    \n",
    "    epoch_loss = batch_loss.sum().item()/size\n",
    "    epoch_accu = batch_corr.sum().item()/size\n",
    "    \n",
    "    return {\"loss\":epoch_loss,\"accu\":epoch_accu}\n",
    "```\n",
    "\n",
    "- The returned values should be available through the FitLoop object\n",
    "  Eg: `FitLoop.train.epoch['loss']`\n",
    "- ✅For each phase a different LoopState obect is maintained.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T04:13:04.112915Z",
     "start_time": "2020-05-06T04:13:04.090495Z"
    }
   },
   "outputs": [],
   "source": [
    "class FitLoop:\n",
    "    # Dummy\n",
    "    pass\n",
    "\n",
    "class LoopState:\n",
    "    \"\"\"\n",
    "    Maintains train/valid/test loop state for a single run of \n",
    "    a certain number of epochs, does not used to preserve state \n",
    "    between runs.\n",
    "    \"\"\"\n",
    "    _batch = 'batch'\n",
    "    _epoch = 'epoch'\n",
    "    def __init__(self, phase:str, floop:FitLoop, no_cast:bool):\n",
    "        \"\"\"\n",
    "        phase : phase name 'train', 'valid' or 'test'\n",
    "        floop : the calling FitLoop object\n",
    "        \"\"\"\n",
    "        self.metrics = {}\n",
    "        self.__batch = ()\n",
    "        self.__floop = floop\n",
    "        self.batch_num = 0\n",
    "        self.epoch_num = 0\n",
    "        self.no_cast = no_cast\n",
    "        \n",
    "        # For easy access\n",
    "        dl = getattr(floop, f'{phase}_dl')\n",
    "        bs = dl.batch_size\n",
    "        dr = dl.drop_last\n",
    "        sz = len(dl.dataset)\n",
    "        bt = sz / bs\n",
    "        \n",
    "        # Gives dataset size and batch count\n",
    "        self.size = sz\n",
    "        self.batches = math.floor(bt) if dr else math.ceil(bt)\n",
    "    \n",
    "    def __getattr__(self, name:str) -> Any:\n",
    "        return getattr(self.__floop, name)\n",
    "    \n",
    "    @property\n",
    "    def batch(self) -> Tuple[Tensor,...]:\n",
    "        if no_cast:\n",
    "            return self.__batch\n",
    "        \n",
    "        return (\n",
    "            d.to(device=self.device,dtype=self.dtype) \n",
    "            if d.is_floating_point() \n",
    "            else d.to(device=self.device,dtype=torch.long) \n",
    "            for d in self.__batch\n",
    "        )\n",
    "    \n",
    "    @batch.setter\n",
    "    def batch(self, current_batch:Tuple[Tensor,...]) -> None:\n",
    "        self.__batch = current_batch\n",
    "        \n",
    "    def _get_score(self, criteria:str) -> float:\n",
    "        # Last added metric that is to be used as a model \n",
    "        # selection criteria\n",
    "        return float(self.metrics[self._epoch][-1])\n",
    "    \n",
    "    def _append(self, rdict:Dict[str, float], stage:str) -> None:\n",
    "        #  Append metrics to the specific stage.\n",
    "        if stage not in self.metrics:\n",
    "            self.metrics[stage] = {}\n",
    "            \n",
    "        for key in rdict:\n",
    "            if key not in self.metrics[stage]:\n",
    "                self.metrics[stage][key] = []\n",
    "            self.metrics[stage][key].append(rdict[key])\n",
    "    \n",
    "    def _clear(self, stage:str) -> None:\n",
    "        # Clear the batch metrics at the end of the batch.\n",
    "        for m in self.metrics[stage]:\n",
    "            m.clear()\n",
    "            \n",
    "    def _append_batch(self, rdict:Dict[str, float]) -> None:\n",
    "        self._append(rdict, self._batch)\n",
    "        \n",
    "    def _append_epoch(self, rdict:Dict[str, float]) -> None:\n",
    "        self._append(rdict, self._epoch)\n",
    "            \n",
    "    def _clear_batch(self) -> None:\n",
    "        self._clear(self._batch)\n",
    "        \n",
    "    def _clear_epoch(self) -> None:\n",
    "        self._clear(self._epoch)\n",
    "    \n",
    "    def __getitem__(self, metric_name:str):\n",
    "        metric_value = self.metrics[self._batch][metric]\n",
    "        try:\n",
    "            return torch.tensor(metric_value).float()\n",
    "        except:\n",
    "            return metric_value\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FitLoop\n",
    "- ✅if `FitLoop.fit(define_all:bool=False)` the zero_grad and the context manager are not auto set.\n",
    "- ✅should keep track of epochs that have been completed\n",
    "- ✅epoch_number can be reset \n",
    "- metrics can be cleared\n",
    "- `FitLoop.set_name.loop_stage['metric_name']` to access the metric\n",
    "- `FitLoop.store_pretrained:bool` arg to store the pretrained weights before training\n",
    "    if path then store at given path else store in memory.\n",
    "- ✅`FitLoop.reset(reset_model:bool)` to clear metrics, epoch_num and to reset the model, to pretrained state\n",
    "    will load the weight from passed path else from memory.\n",
    "- `FitLoop.save(path:str)` to save the model and training state somehow even the fitloop state.\n",
    "- `FitLoop.load(path:str)` to load the FitLoop state from given path.\n",
    "- Some basic step should be used such that one can use it without defining step functions.\n",
    "- `FitLoop.fit(continue_loop:int=0)` ask after `int` whether to continue training or to end.\n",
    "- `FitLoop.fit(profiler:bool=False)` mode to capture all stage timings and maybe even CPU, GPU, RAM usage to check for bottlenecks and usage spikes, to be used with timed_test.\n",
    "- Make it easy to train,validate, test with some other DataLoader that is not attached to the object.\n",
    "- `FitLoop.fit(no_print:bool=False)` mode to capture all stage timings and maybe even CPU, GPU, RAM usage to check for bottlenecks and usage spikes, to be used with timed_test.\n",
    "- Use a loading bar for epoch and a disappearing one for batch.\n",
    "- Functionality to view the metrics.\n",
    "- ✅Model score should be a loop instance so that the best model may not be erased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T00:33:07.919907Z",
     "start_time": "2020-05-06T00:33:07.856954Z"
    }
   },
   "outputs": [],
   "source": [
    "class FitLoop:\n",
    "    sets = ['train','valid','test']\n",
    "    TR, VA, TE = sets\n",
    "    \n",
    "    model_typ = ['pretrained','best']\n",
    "    PR, BS = model_type\n",
    "    def __init__(self, \n",
    "                 # Basic blocks\n",
    "                 model: Module, optimizer: Union[Optimizer,List[Optimizer]], \n",
    "                 loss_function: Callable[[Tensor,Tensor],Tensor], \n",
    "                 \n",
    "                 # DataLoader\n",
    "                 train_dl: DataLoader, \n",
    "                 valid_dl: Optional[DataLoader]=None, \n",
    "                 test_dl: Optional[DataLoader]=None, \n",
    "                 \n",
    "                 # Batch Step\n",
    "                 train_step: Callable[[LoopState],Dict[str, Any]],\n",
    "                 valid_step: Optional[Callable[[LoopState],Dict[str, Any]]]=None,\n",
    "                 test_step: Optional[Callable[[LoopState],Dict[str, Any]]]=None,\n",
    "                 \n",
    "                 # Epoch Start Step\n",
    "                 train_epoch_start: Callable[[LoopState],Dict[str, Any]],\n",
    "                 valid_epoch_start: Optional[Callable[[LoopState],Dict[str, Any]]]=None,\n",
    "                 test_epoch_start: Optional[Callable[[LoopState],Dict[str, Any]]]=None,\n",
    "                 \n",
    "                 # Epoch End Step\n",
    "                 train_epoch_end: Callable[[LoopState],Dict[str, Any]],\n",
    "                 valid_epoch_end: Optional[Callable[[LoopState],Dict[str, Any]]]=None,\n",
    "                 test_epoch_end: Optional[Callable[[LoopState],Dict[str, Any]]]=None,\n",
    "                 \n",
    "                 # Other Args\n",
    "                 lr_scheduler: Optional[_LRScheduler, Any, List[_LRScheduler,Any]]=None,\n",
    "                 device: torch.device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'), \n",
    "                 dtype: torch.dtype=torch.float32,\n",
    "                 criteria_direction: int=1\n",
    "                 \n",
    "                 # Preserving Model State\n",
    "                 save_to_disk: bool=False\n",
    "                 save_path: str=\"models\"\n",
    "                 pretrained_name: Optional[str]=None,\n",
    "                 best_model_name: Optional[str]=None,\n",
    "                ) -> None:\n",
    "        \"\"\"\n",
    "        FitLoop constructor\n",
    "        ----\n",
    "        Parameters:\n",
    "        # Basic Blocks\n",
    "            The bare minimum required along with train_dl, train_step and train_epoch_end.\n",
    "            - model : nn.Module model that has to be trained\n",
    "            - optimizer : an optimizer from torch.optim\n",
    "            - loss_function : function to compute loss\n",
    "         \n",
    "        # DataLoader\n",
    "            - train_dl : training DataLoader\n",
    "            - valid_dl : validation DataLoader, if None validation will be ignored\n",
    "            - test_dl : testing DataLoader, if None `.test()` will not run\n",
    "         \n",
    "        # Batch Step\n",
    "            Functions that take in a LoopState object to perform \n",
    "            required calculations, functions should return a dict with values\n",
    "            to be used in the epoch end step.\n",
    "            - train_step : portion of the loop where forward and backward \n",
    "                passes take place.\n",
    "            - valid_step : validation portion of the loop.\n",
    "            - test_step : called when `FitLoop.test()` is called.\n",
    "        \n",
    "        # Epoch Start Step\n",
    "            TODO: NEED TO IMPLEMENT\n",
    "            - train_epoch_start :\n",
    "            - valid_epoch_start :\n",
    "            - test_epoch_start : \n",
    "        \n",
    "        # Epoch End Step\n",
    "            Functions that take in a LoopState object to perform \n",
    "            required calculations, functions should return a dict with values\n",
    "            that are to be returned when the loop is over.\n",
    "            - train_epoch_end : after training epoch has ended.\n",
    "            - valid_epoch_end : after validation epoch has ended.\n",
    "            - test_epoch_end : called when the test loop is done, one iteration\n",
    "                over all batches in the test dataloader.\n",
    "        \n",
    "        # Other Args\n",
    "            - lr_scheduler : scheduler from torch.optim.lr_scheduler\n",
    "            - device : torch.device model will be cast to device this prior to the loop\n",
    "            - dtype : floating point dtype to cast model and data to\n",
    "            - criteria_direction : whether more is better (1) or less is better (-1) \n",
    "                for model score criteria.\n",
    "        \n",
    "        # Preserving Model State\n",
    "            - save_to_disk : True then save pretrained and best_model to the disk, else it is \n",
    "                stored as an attribute.\n",
    "            - save_path : location where the initial and pretrained models are to be saved\n",
    "            - pretrained_name : Name to save the pretrained model by\n",
    "            - best_model_name : Name to save the best model by\n",
    "        \"\"\"\n",
    "        # Basic Blocks\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_function = loss_function\n",
    "        \n",
    "        # DataLoaders\n",
    "        self.train_dl = train_dl\n",
    "        self.valid_dl = valid_dl\n",
    "        self.test_dl = test_dl\n",
    "        \n",
    "        # Batch Step\n",
    "        self.train_step = train_step\n",
    "        self.valid_step = valid_step\n",
    "        self.test_step = test_step\n",
    "        \n",
    "        # Epoch Start Step\n",
    "        self.train_epoch_start = train_epoch_start\n",
    "        self.valid_epoch_start = valid_epoch_start\n",
    "        self.test_epoch_start = test_epoch_start\n",
    "        \n",
    "        # Epoch End Step\n",
    "        self.train_epoch_end = train_epoch_end\n",
    "        self.valid_epoch_end = valid_epoch_end\n",
    "        self.test_epoch_end = test_epoch_end\n",
    "        \n",
    "        # Other Args\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        self.criteria_direction = criteria_direction\n",
    "        \n",
    "        # Preserving Model State\n",
    "        if save_disk and pretrained_name is None:\n",
    "            u = str(uuid4()).split('-')[1]\n",
    "            pretrained_name = f\"pretrained_{u}.pt\"\n",
    "        if save_disk and best_model_name is None:\n",
    "            u = str(uuid4()).split('-')[1]\n",
    "            best_model_name = f\"best_{u}.pt\"\n",
    "        self.pretrained_name = pretrained_name\n",
    "        self.best_model_name = best_model_name\n",
    "        self.save_to_disk = save_to_disk\n",
    "        self.save_path = Path(save_path)\n",
    "        \n",
    "        \n",
    "        # INITIALIZE NON ARGS\n",
    "        self.__save_model(self.PR)\n",
    "        self.epoch_num = 0\n",
    "        self.best_score = self.criteria_direction * float('-inf')\n",
    "            \n",
    "        \n",
    "    def __ftime(t1:float,t2:float)->str:\n",
    "        t = t2-t1\n",
    "        s, ms = str(t).split('.')\n",
    "        ms = ms[:3]\n",
    "        s = str(int(s)%60).rjust(2)\n",
    "        m = int(t//60)\n",
    "        h = str(m//60)\n",
    "        m = str(m % 60).rjust(2)\n",
    "        u = ['h','m','s','ms']\n",
    "        v = [h,m,s,ms]\n",
    "        t = filter(lambda x:int(x[0]) > 0,zip(v,u))\n",
    "        return ' '.join([' '.join(x) for x in t]).ljust(20)\n",
    "    \n",
    "    def __time(profiler:bool) -> Optional[float]:\n",
    "        if profiler:\n",
    "            return time.time()\n",
    "    \n",
    "    def __call_batch_step(self, state:LoopState) -> None:\n",
    "        step_funcs = [self.train_step, self.valid_step, self.test_step]\n",
    "        step_funcs = {s:f for s,f in zip(self.sets, step_func)}\n",
    "        step_func = step_funcs[state.phase]\n",
    "        \n",
    "        if step_func is None:\n",
    "            raise AttributeError(f\"{phase}_step not assigned\")\n",
    "        state_dict = step_func(phase)\n",
    "        # To Do Alter LoopState according to the state_dict\n",
    "        \n",
    "        \n",
    "    def __call_epoch_start_step(self, state:LoopState) -> Dict[str,Any]:\n",
    "        step_funcs = [self.train_epoch_start,self.valid_epoch_start,self.test_epoch_start]\n",
    "        step_funcs = {s:f for s,f in zip(self.sets, step_func)}\n",
    "        step_func = step_funcs[state.phase]\n",
    "        \n",
    "        if step_func is None:\n",
    "            raise AttributeError(f\"{phase}_step not assigned\")\n",
    "        state_dict = step_func(state)\n",
    "        # To Do Alter LoopState according to the state_dict\n",
    "        \n",
    "    def __call_epoch_end_step(self, phase:str) -> Dict[str,Any]:\n",
    "        step_funcs = [self.train_epoch_end,self.valid_epoch_end,self.test_epoch_end]\n",
    "        step_funcs = {s:f for s,f in zip(self.sets, step_func)}\n",
    "        step_func = step_funcs[state.phase]\n",
    "        \n",
    "        if step_func is None:\n",
    "            raise AttributeError(f\"{phase}_step not assigned\")\n",
    "        state_dict = step_func(state)\n",
    "        # To Do Alter LoopState according to the state_dict\n",
    "        \n",
    "    def __get_dl(self, is_test:bool)-> Dict[str,DataLoader]:\n",
    "        if is_test:\n",
    "            if self.test_dl is None:\n",
    "                raise AttributeError(\"test_dl not assigned\")\n",
    "            return {self.TE:self.test_dl}\n",
    "        \n",
    "        va_dl = self.valid_dl is not None\n",
    "        va_st = self.valid_step is not None\n",
    "        if  va_dl and va_st:\n",
    "            return {TR:self.train_dl, VA:self.valid_dl}\n",
    "        else:\n",
    "            return {TR:self.train_dl}\n",
    "    \n",
    "    def __loop(self, \n",
    "            epochs:int=1, print_every:int=1, steps: Optional[int],\n",
    "            criteria:Union[str, None]=None, load_best:bool=False, \n",
    "            profiler:bool=False, is_test:bool=False,\n",
    "            track_batch_metrics:bool=True, define_all:bool=False,\n",
    "            continue_loop:int=0, no_print:bool=False, no_cast:bool=False\n",
    "           ):\n",
    "        \"\"\"\n",
    "        Runs the training loop for `epochs`\n",
    "        ----\n",
    "        Parameters\n",
    "         - epochs : should be a non negative integer\n",
    "         - print_every : if 0 will not print, else will print at given epoch\n",
    "         - steps : number of batches to run in each phase [train,valid] \n",
    "             for check if everything is working.\n",
    "         - criteria : value in the `valid_epoch_end` return dict\n",
    "             that used is used to define a better model, eg: 'accuracy'\n",
    "         - load_best : whether to load the best model after training, works only if validation\n",
    "             parameters are defined `valid_dl`, `valid_step`, `valid_epoch_end`\n",
    "         - profiler : whether to keep track of time taken by various sections\n",
    "         - is_test : whether it is a model testing loop or training/validation loop\n",
    "         - track_batch_metrics : whether to store the values returned in the batch steps\n",
    "         - define_all : If True then `torch.set_grad_enabled`, `optimizer.zero_grad` and model mode \n",
    "             ie [train,eval] have to be called where required (usually in the `train_step` function).\n",
    "         -  continue_loop : Will ask whether to continue training after `continue` epochs, should\n",
    "             be a positive integer.\n",
    "         - no_print : If True will suppress all print statements, can be used when custom logging is\n",
    "             used in the stage functions.\n",
    "         - no_cast : True, if model and data casting has to be manually set in the stage functions\n",
    "        \n",
    "        \"\"\"\n",
    "        t = lambda : self.__time(print_every != 0) # Returns the time \n",
    "        \n",
    "        total_time_start = t()\n",
    "        \n",
    "        # INITILIZING VARIABLES -----\n",
    "        \n",
    "        # Storage\n",
    "        dl = self.__get_dl(is_test)\n",
    "        sz = { k : len(dl[k].dataset) for k in dl }\n",
    "        phases = [ph for ph in dl]\n",
    "        state = {ph: LoopState() for ph in phases}\n",
    "\n",
    "        # Markers\n",
    "        self.__save_model(self.BS)\n",
    "\n",
    "        # ----------------------------\n",
    "        \n",
    "        \n",
    "        # CONVENIENCE FUNCTIONS ------\n",
    "        \n",
    "        # Function to get formatted epochs (from 1 not 0)\n",
    "        r_just_val = len(str(epochs))*2 + 3\n",
    "        estr = lambda e: f\"[{e + 1}/{epochs}]\".rjust(r_just_val)\n",
    "\n",
    "        # Function to print every `print_every` epochs.\n",
    "        def eprint(e,st):\n",
    "            if (e == 0) and (print_every != 0):\n",
    "                print(st,end=\"\")\n",
    "            elif (e + 1) % print_every == 0:\n",
    "                print(st,end=\"\")\n",
    "\n",
    "        def grad_times(t):\n",
    "            t = t*1000\n",
    "            return f\"{t:0.3f} ms\".rjust(10)\n",
    "\n",
    "        # Function for phase strings.\n",
    "        def statstr(phase, loss, accu, time, infr, rjust=True):\n",
    "            infr = grad_times(infr)\n",
    "            st =  f\"{phase} :: loss: {loss:0.4f} | accu: {accu:0.4f} | infr: {infr}  | time: {time} \\n\"\n",
    "            if rjust:\n",
    "                return st.rjust(r_just_val + len(st) + 3)\n",
    "            else:\n",
    "                return st\n",
    "            \n",
    "        # ----------------------------\n",
    "\n",
    "\n",
    "        # THE LOOP - START -----------\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO : Preloop section, to initilize parameters in whichever way, add profiling.\n",
    "            Manual cast for below line\n",
    "        \"\"\"\n",
    "        if not no_cast:\n",
    "            model = self.model.to(device=self.device, dtype=self.dtype)\n",
    "        for e in range(epochs):\n",
    "            epoch_time_start = t()\n",
    "            \n",
    "            # Update FitLoop epoch_num\n",
    "            self.epoch_num += 1\n",
    "            \n",
    "            \"\"\"\n",
    "            TODO : Re-initilize LoopState, add profiling\n",
    "            \"\"\"\n",
    "            # for st in state do something\n",
    "            \n",
    "            # \n",
    "            for phase in phases:\n",
    "                # TODO: set the epoch_num for LoopState and self\n",
    "                phase_time_start = t()\n",
    "                \n",
    "                # Update LoopState epoch_num\n",
    "                state[phase].epoch_num = e\n",
    "                \n",
    "                # EPOCH START STEP - START ---\n",
    "                epoch_start_step = self.__call_epoch_start_step(state[phase])\n",
    "                # EPOCH START STEP - END -----\n",
    "                \n",
    "                is_tr = phase == TR\n",
    "                if not define_all:\n",
    "                    if is_tr:\n",
    "                          model.train()\n",
    "                    else:\n",
    "                          model.eval()\n",
    "                            \n",
    "                # if is_tr:\n",
    "                #    eprint(e,estr(e)+f\" - \")\n",
    "                    \n",
    "                    \n",
    "                # BATCH LOOP - START ---------\n",
    "                for batch in dl[phase]:\n",
    "                    \n",
    "                    # Update LoopState batch_num and set batch\n",
    "                    state[phase].batch_num += 1\n",
    "                    state[phase].batch = batch\n",
    "                    \n",
    "                    # BATCH STEP - START ---------\n",
    "                    if define_all:\n",
    "                        batch_step = self.__call_batch_step(state[phase])\n",
    "                    else:\n",
    "                        optim.zero_grad()\n",
    "                        with torch.set_grad_enabled(is_tr):\n",
    "                            batch_step = self.__call_batch_step(state[phase])\n",
    "                    # BATCH STEP - END -----------\n",
    "                    \n",
    "                # BATCH LOOP - END -----------\n",
    "                \n",
    "                # EPOCH END STEP - START -----\n",
    "                self.__call_epoch_end_step(state[phase])\n",
    "                # EPOCH END STEP - END -------\n",
    "                \n",
    "                # Update markers save best model\n",
    "                if not is_tr and not is_test:\n",
    "                    score = state[phase]._get_score()\n",
    "                    direc = self.criteria_direction > 0\n",
    "                    is_better = score > self.best_score if direc else score < self.best_score:\n",
    "                    if is_better:\n",
    "                        self.best_score = score\n",
    "                        self.__save_model(self.BS)\n",
    "\n",
    "#                 # Logging phase time\n",
    "#                 phase_time_end = time.time()\n",
    "#                 phase_time = ftime(phase_time_start,phase_time_end)\n",
    "#                 if is_tr:\n",
    "#                     eprint(e,statstr(phase, phase_loss,phase_accu, phase_time, mean_inf, False))\n",
    "#                 else:\n",
    "#                     eprint(e,statstr(phase, phase_loss,phase_accu, phase_time, mean_inf))\n",
    "\n",
    "            # Logging epoch times\n",
    "#             epoch_time_end = time.time()\n",
    "#             epoch_time = f\"epoch time: {ftime(epoch_time_start, epoch_time_end)}\\n\"\n",
    "#             eprint(e,epoch_time.rjust(len(epoch_time)+r_just_val+3)+\"\\n\")\n",
    "\n",
    "        # THE LOOP - END -------------\n",
    "    \n",
    "        if load_best:\n",
    "            self.__load_model(self.BS)\n",
    "\n",
    "        # ----------------------------\n",
    "        total_time_end = time.time()\n",
    "#         total_time = ftime(total_time_start,total_time_end)\n",
    "#         print(f\"least loss: {least_loss:0.4f} | best accu: {best_accuracy:0.4f} | total time taken: {total_time}\")\n",
    "\n",
    "#         return {\"losses_epo\":losses_epo,\"accuracy_epo\":accuracy_epo,\"losses_all\":losses_all}\n",
    "        return\n",
    "    \n",
    "    def fit(self, \n",
    "            epochs:int=1, print_every:int=1, \n",
    "            criteria:Union[str, None]=None, load_best:bool=False, \n",
    "            profiler:bool=False\n",
    "           ):\n",
    "        \"\"\"\n",
    "        Runs the training loop for `epochs`\n",
    "        ----\n",
    "        Parameters\n",
    "         - epochs : should be a non negative integer\n",
    "         \n",
    "         - print_every : if 0 will not print, else will print at given epoch\n",
    "         - criteria : value in the validation epoch end return dict\n",
    "             that used is used to define a better model, eg: 'accuracy'\n",
    "         - load_best : whether to load the best model after training for `epochs`\n",
    "         - profiler : whether to keep track of time taken by various sections\n",
    "        \n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def train(self,*args,**kwargs):\n",
    "        \"\"\"\n",
    "        An alias for FitLoop.fit\n",
    "        \"\"\"\n",
    "        self.fit(*args, **kwargs)\n",
    "    \n",
    "    def test(self):\n",
    "        \"\"\"\n",
    "        TODO : Run __loop in test mode for model testing\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def timed_test(self):\n",
    "        \"\"\"\n",
    "        TODO: Runs the model in training mode for one epoch to get\n",
    "            to get the runtime and other stats, will restore model\n",
    "            state after.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def sanity_check(self, use_test_dl=False):\n",
    "        \"\"\"\n",
    "        use_test_dl whether to use test or validation data for the \n",
    "        test loop.\n",
    "        TODO : Run train and test loops for 1 epoch and fewer steps\n",
    "            ie smaller dataset to check if everything is working,\n",
    "            will restore model state after.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def __save_model(self, typ:str) -> None:\n",
    "        \"\"\"\n",
    "        Save model to object or to the disk.\n",
    "        \"\"\"\n",
    "        name = self.best_model_name if typ == self.BS else self.pretrained_name\n",
    "        path = self.save_path/ name\n",
    "        state_dict = self.model.state_dict()\n",
    "        if self.save_to_disk:\n",
    "            torch.save(state_dict,path)\n",
    "        elif typ == self.BS:\n",
    "            self.best_model_state_dict = deepcopy(state_dict)\n",
    "        else:\n",
    "            self.pretrained_state_dict = deepcopy(state_dict)\n",
    "        \n",
    "    def __load_model(self, typ:str):\n",
    "        \"\"\"\n",
    "        Load model from the object or from the disk.\n",
    "        \"\"\"\n",
    "        name = self.best_model_name if typ == self.BS else self.pretrained_name\n",
    "        path = self.save_path/ name\n",
    "        if self.save_to_disk:\n",
    "            state_dict = torch.load(path, map_location=self.device)\n",
    "        elif typ == self.BS:\n",
    "            state_dict = self.best_model_state_dict\n",
    "        else:\n",
    "            state_dict = self.pretrained_state_dict\n",
    "        self.model.load_state_dict(state_dict)\n",
    "        print(\"please reinitilize FitLoop.optimizer param groups before training\")\n",
    "    \n",
    "    def reset(self, reset_model:bool=True) -> None:\n",
    "        \"\"\"\n",
    "        Resets FitLoop to initial state.\n",
    "        Parameters reset:\n",
    "            - model, to pretrained state if rese_mode==True\n",
    "            - epoch_num, to 0\n",
    "            - best_score to ∓inf\n",
    "        FitLoop.optimizer param groups will have to be set again\n",
    "        \"\"\"\n",
    "        if reset_model:\n",
    "            self.__load_model(self, self.PR)\n",
    "        self.epoch_num = 0\n",
    "        self.best_score = self.criteria_direction * float('-inf')\n",
    "    \n",
    "    def save(self, path, only_model=False):\n",
    "        \"\"\"\n",
    "        TODO : save the FitLoop state, if only_model then save only model.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def load(self, path):\n",
    "        \"\"\"\n",
    "        TODO : load the FitLoop state, if only model then load the model \n",
    "            state dict.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def del_pretrained(self):\n",
    "        \"\"\"\n",
    "        TODO : function to delete pretrained model from save path\n",
    "        \"\"\"\n",
    "        \n",
    "    def del_best_model(self):\n",
    "        \"\"\"\n",
    "        TODO : function to delete best model from save path\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portions of the loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Single step** Portion of the loop that receives the batch and will cal forward pass on it.\n",
    "    - *Set* zero grad\n",
    "    - **Train Step** \n",
    "        - *Set* \n",
    "            - enable gradients\n",
    "            - model.train\n",
    "        - *Define*: \n",
    "            - compute loss\n",
    "            - call backward \n",
    "            - update gradients and maybe scheduler step\n",
    "            - loss and whatever metrics will be saved in a list.\n",
    "    - **Valid Step**\n",
    "        - *Set*\n",
    "            - disable gradients\n",
    "            - model.eval\n",
    "        - *Define*: \n",
    "            - compute loss\n",
    "            - loss and whatever metrics will be saved in a list.\n",
    "    - **Test Step** Only in test loop\n",
    "        - *Set*\n",
    "            - disable gradients\n",
    "            - model.eval\n",
    "        - *Define*: \n",
    "            - compute loss\n",
    "            - loss and whatever metrics will be saved in a list.\n",
    "- **Pre Epoch**\n",
    "- **Post Epoch**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate a time string from\n",
    "# 2 instances of time.time\n",
    "def ftime(t1,t2):\n",
    "    t = t2-t1\n",
    "    s, ms = str(t).split('.')\n",
    "    ms = ms[:3]\n",
    "    s = str(int(s)%60).rjust(2)\n",
    "    m = int(t//60)\n",
    "    h = str(m//60)\n",
    "    m = str(m % 60).rjust(2)\n",
    "    u = ['h','m','s','ms']\n",
    "    v = [h,m,s,ms]\n",
    "    t = filter(lambda x:int(x[0]) > 0,zip(v,u))\n",
    "    return ' '.join([' '.join(x) for x in t]).ljust(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, dl_train, dl_valid, optim, loss_function, epochs, sched=None, should_pass=False, print_every=1, load_best=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    # model\n",
    "      - pytorch model to be trained\n",
    "      \n",
    "    # dl_train\n",
    "      - Dataloader for the train set\n",
    "      \n",
    "    # dl_valid\n",
    "      - Dataloader for the valid set\n",
    "      \n",
    "    # optimizer\n",
    "      - optimizer attached to model params from torch.optim\n",
    "      \n",
    "    # loss_function\n",
    "      - function to calculate the loss; loss_function(model(X),y)\n",
    "      \n",
    "    # epochs\n",
    "      - number of epochs to train for\n",
    "      \n",
    "    # device\n",
    "      - torch.device to load the model to \n",
    "      \n",
    "    # sched\n",
    "      - scheduler for the optimizer learning rate\n",
    "      \n",
    "    # should_pass\n",
    "      - if loss should be passed to the scheduler.\n",
    "      \n",
    "    # print_every\n",
    "      - to print the loss every n epochs\n",
    "    \n",
    "    # load_best \n",
    "      - whether to load the best model as determined by accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    total_time_start = time.time()\n",
    "    # ----------------------------\n",
    "    phases = [TR,VA]\n",
    "    dl = {TR:dl_train, VA:dl_valid}\n",
    "    sizes = {k:len(dl[k].dataset) for k in dl}\n",
    "    \n",
    "    # To contain metrics to plot later\n",
    "    losses_all = {TR:[],VA:[]}\n",
    "    losses_epo = {TR:[],VA:[]}\n",
    "    accuracy_epo = {TR:[],VA:[]}\n",
    "    \n",
    "    # Markers\n",
    "    least_loss = float('inf')\n",
    "    best_accuracy = 0\n",
    "    best_model = deepcopy(model.state_dict())\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Convenience functions (that won't be used elsewhere)\n",
    "    \n",
    "    # Function to get formatted epochs (from 1 not 0)\n",
    "    r_just_val = len(str(epochs))*2 + 3\n",
    "    estr = lambda e: f\"[{e + 1}/{epochs}]\".rjust(r_just_val)\n",
    "    \n",
    "    # Convenience function to print every `print_every` epochs.\n",
    "    def eprint(e,st):\n",
    "        if ((e + 1) % print_every == 0):\n",
    "            print(st,end=\"\")\n",
    "\n",
    "    def grad_times(t):\n",
    "        t = t*1000\n",
    "        return f\"{t:0.3f} ms\".rjust(10)\n",
    "    \n",
    "    # Convenience function for phase strings.\n",
    "    def statstr(phase,loss,accu,time,infr, rjust=True):\n",
    "        infr = grad_times(infr)\n",
    "        st =  f\"{phase} :: loss: {loss:0.4f} | accu: {accu:0.4f} | infr: {infr}  | time: {time} \\n\"\n",
    "        if rjust:\n",
    "            return st.rjust(r_just_val + len(st) + 3)\n",
    "        else:\n",
    "            return st\n",
    "\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Training \n",
    "    model.to(device)\n",
    "    for e in range(epochs):\n",
    "        epoch_time_start = time.time()\n",
    "        \n",
    "        # Training or validation phase\n",
    "        for phase in phases:\n",
    "            phase_time_start = time.time()\n",
    "\n",
    "            # Running metrics\n",
    "            running_loss = 0\n",
    "            running_corr = 0\n",
    "\n",
    "            is_tr = phase == TR\n",
    "            if is_tr:\n",
    "                  model.train()\n",
    "            else:\n",
    "                  model.eval()\n",
    "                  \n",
    "            # Keeping track of computation times.\n",
    "            inference_times = []\n",
    "            dl_p = dl[phase]\n",
    "            dl_l = len(dl_p)\n",
    "            if is_tr:\n",
    "                eprint(e,estr(e)+f\" - \")\n",
    "            for b,batch in enumerate(dl_p):\n",
    "                X,y = batch\n",
    "                bs = y.size(0)\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                \n",
    "                optim.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(is_tr):\n",
    "                    # Forward pass and log times \n",
    "                    inference_time_start = time.time()\n",
    "                    y_ = model(X)\n",
    "                    inference_time_end = time.time()\n",
    "                    inference_times.append((inference_time_end-inference_time_start)/bs)\n",
    "                    \n",
    "                    loss = loss_function(y_,y)\n",
    "                    if is_tr:\n",
    "                        # Backprop (grad calc and param update)\n",
    "                        loss.backward()\n",
    "                        optim.step()\n",
    "                        \n",
    "                    elif not is_tr and sched is not None:\n",
    "                        # Scheduler step\n",
    "                        if should_pass:\n",
    "                            sched.step(loss.item())\n",
    "                        else:\n",
    "                            sched.step()\n",
    "                    \n",
    "                running_loss += loss.item() * bs\n",
    "                running_corr += (y_.argmax(dim=1) == y).sum().item()\n",
    "                losses_all[phase].append(loss.item())\n",
    "             \n",
    "            # Calculate phase losses and scores\n",
    "            phase_loss = running_loss / sizes[phase]\n",
    "            phase_accu = running_corr / sizes[phase]\n",
    "                  \n",
    "            # Calculate timings\n",
    "            mean_inf = np.array(inference_times).mean()\n",
    "            # Log scores and losses\n",
    "            losses_epo[phase].append(phase_loss)\n",
    "            accuracy_epo[phase].append(phase_accu)\n",
    "                \n",
    "            # Update markers save best model\n",
    "            if not is_tr:\n",
    "                if phase_accu > best_accuracy:\n",
    "                    best_accuracy = phase_accu\n",
    "                    best_model = deepcopy(model.state_dict())\n",
    "                if phase_loss < least_loss:\n",
    "                    least_loss = phase_loss\n",
    "            \n",
    "            # Logging phase time\n",
    "            phase_time_end = time.time()\n",
    "            phase_time = ftime(phase_time_start,phase_time_end)\n",
    "            if is_tr:\n",
    "                eprint(e,statstr(phase, phase_loss,phase_accu, phase_time, mean_inf, False))\n",
    "            else:\n",
    "                eprint(e,statstr(phase, phase_loss,phase_accu, phase_time, mean_inf))\n",
    "        \n",
    "        # Logging epoch times\n",
    "        epoch_time_end = time.time()\n",
    "        epoch_time = f\"epoch time: {ftime(epoch_time_start, epoch_time_end)}\\n\"\n",
    "        eprint(e,epoch_time.rjust(len(epoch_time)+r_just_val+3)+\"\\n\")\n",
    "    \n",
    "    if load_best:\n",
    "        model.load_state_dict(best_model)\n",
    "    \n",
    "    # ----------------------------\n",
    "    total_time_end = time.time()\n",
    "    total_time = ftime(total_time_start,total_time_end)\n",
    "    print(f\"least loss: {least_loss:0.4f} | best accu: {best_accuracy:0.4f} | total time taken: {total_time}\")\n",
    "    \n",
    "    return {\"losses_epo\":losses_epo,\"accuracy_epo\":accuracy_epo,\"losses_all\":losses_all}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
