{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Grounds - Fit Loop\n",
    "Creating a library that helps with the pytorch looping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T17:18:14.466907Z",
     "start_time": "2020-05-06T17:18:14.460137Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "from uuid import uuid4\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from typing import Union, List, Callable, Optional, Any, Dict, Tuple\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import _LRScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T00:26:27.425435Z",
     "start_time": "2020-05-06T00:26:27.418596Z"
    }
   },
   "outputs": [],
   "source": [
    "class A():\n",
    "    def __init__(self, f):\n",
    "        self.__f = f\n",
    "        self.a = 22\n",
    "    \n",
    "    def __getitem__(self, name):\n",
    "        return name\n",
    "        \n",
    "    def __getattr__(self,name):\n",
    "        return getattr(self.__f,name)\n",
    "class B():\n",
    "    def __init__(self):\n",
    "        self.b = 33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LoopState\n",
    "- ✅should cast the batch to device before passing it using `state.batch()`\n",
    "- ✅should get the batch num `state.batch_num` and epoch num `state.epoch_num`\n",
    "- ✅the model, optimizer, loss_function, lr_scheduler should be available\n",
    "    `state.model`, `state.optimizer`, `state.loss_function`, `state.lr_scheduler`\n",
    "- ✅should return the batch metrics as float tensors using square bracket indexing\n",
    "    `state['loss']` \n",
    "    - every step function hook receives the LoopState object.\n",
    "- ✅The loop state object should have a copy of all the values returned from the function hook\n",
    "- ✅example the below returned dict values should be avialable in the LoopState object\n",
    "\n",
    "```python\n",
    "def train_step(state):\n",
    "    X,y = state() # should device cast automatically\n",
    "    y_ = state.model(X)\n",
    "    loss = state.loss_function(y_, y)\n",
    "    \n",
    "    state.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    state.optimizer.step()\n",
    "    state.lr_scheduler.step() \n",
    "    \n",
    "    loss = loss.item()\n",
    "    batch_loss = loss * y.size()\n",
    "    batch_corr = (y_.argmax(dim=0) == y).sum().float().item()\n",
    "    \n",
    "    return {'loss':loss,'batch_loss':batch_loss:'batch_corr'}\n",
    "```\n",
    "- The LoopState object should be cleared of the above values at the start \n",
    "  of the next epoch.\n",
    "- The returned values should be available through the FitLoop object\n",
    "  Eg: `FitLoop.train.batch['loss']`\n",
    "- The (above statement) returned value should be optionally available by setting the flag \n",
    "  `store_batch_metrics`\n",
    "\n",
    "```python\n",
    "def train_epoch_end(state):\n",
    "    loss = state['loss']\n",
    "    batch_loss = state['batch_loss']\n",
    "    batch_corr = state['batch_corr']\n",
    "    \n",
    "    size = state.size\n",
    "    \n",
    "    epoch_loss = batch_loss.sum().item()/size\n",
    "    epoch_accu = batch_corr.sum().item()/size\n",
    "    \n",
    "    return {\"loss\":epoch_loss,\"accu\":epoch_accu}\n",
    "```\n",
    "\n",
    "- The returned values should be available through the FitLoop object\n",
    "  Eg: `FitLoop.train.epoch['loss']`\n",
    "- ✅For each phase a different LoopState obect is maintained.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T17:21:22.927136Z",
     "start_time": "2020-05-06T17:21:22.923214Z"
    }
   },
   "outputs": [],
   "source": [
    "class FitLoop:\n",
    "    # Dummy\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T00:44:16.153669Z",
     "start_time": "2020-05-07T00:44:16.148062Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = []\n",
    "\n",
    "a.append(2)\n",
    "a.append(3)\n",
    "\n",
    "a[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T00:49:31.559480Z",
     "start_time": "2020-05-07T00:49:31.528276Z"
    }
   },
   "outputs": [],
   "source": [
    "class LoopState:\n",
    "    \"\"\"\n",
    "    Maintains train/valid/test loop state for a single run of \n",
    "    a certain number of epochs, does not used to preserve state \n",
    "    between runs.\n",
    "    \"\"\"\n",
    "    _stages = ['batch','epoch_start','epoch_end']\n",
    "    _batch_step, _epoch_start, _epoch_end = _stages\n",
    "    def __init__(self, phase:str, floop:FitLoop, no_cast:bool, no_float:bool, is_train:bool, is_test:bool):\n",
    "        \"\"\"\n",
    "        phase : phase name 'train', 'valid' or 'test'\n",
    "        floop : the calling FitLoop object\n",
    "        \"\"\"\n",
    "        self.__batch = ()\n",
    "        self.__floop = floop\n",
    "        self._no_cast = no_cast\n",
    "        self._no_float = no_float\n",
    "        self.phase = phase\n",
    "        self.batch_num = 0\n",
    "        self.epoch_num = 0\n",
    "        self.metrics = {s:{} for s in self._stages}\n",
    "        self.is_train = is_train\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        # For easy access\n",
    "        dl = getattr(floop, f'{phase}_dl')\n",
    "        bs = dl.batch_size\n",
    "        dr = dl.drop_last\n",
    "        sz = len(dl.dataset)\n",
    "        bt = sz / bs\n",
    "        \n",
    "        # Gives dataset size and batch count\n",
    "        self.size = sz\n",
    "        self.batches = math.floor(bt) if dr else math.ceil(bt)\n",
    "        self.batch_size = 0\n",
    "    \n",
    "    def __getattr__(self, name:str) -> Any:\n",
    "        # To get attributes from the FitLoop object \n",
    "        # for use in the stage functions.\n",
    "        return getattr(self.__floop, name)\n",
    "    \n",
    "    def __getitem__(self, metric_name:str):\n",
    "        # To get the metrics stored in the batch step stage\n",
    "        metric_value = self.metrics[self._batch_step][metric_name]\n",
    "        try:\n",
    "            return torch.tensor(metric_value).float()\n",
    "        except:\n",
    "            return metric_value\n",
    "    \n",
    "    \"\"\"\n",
    "    Getter and setter for the current batch\n",
    "    \"\"\"\n",
    "    @property\n",
    "    def batch(self) -> Tuple[Tensor,...]:\n",
    "        if self._no_cast:\n",
    "            return self.__batch\n",
    "        \n",
    "        return (\n",
    "            d.to(device=self.device,dtype=self.dtype) \n",
    "            if d.is_floating_point() \n",
    "            else d.to(device=self.device,dtype=torch.long) \n",
    "            for d in self.__batch\n",
    "        )\n",
    "    \n",
    "    @batch.setter\n",
    "    def batch(self, current_batch:Tuple[Tensor,...]) -> None:\n",
    "        self.__batch = current_batch\n",
    "        \n",
    "    \"\"\"\n",
    "    Functions to append rdict values to self.metrics\n",
    "    \"\"\"\n",
    "    def _append(self, rdict:Dict[str, float], stage:str) -> None:\n",
    "        #  Append metrics to the specific stage.\n",
    "        if rdict is None:\n",
    "            if stage == self._epoch_end:\n",
    "                print(f\"no rdict returned from: f{self.phase}_{stage}\")\n",
    "            \"\"\"\n",
    "            TODO: Add warning if rdict of stage is None\n",
    "            \"\"\"\n",
    "            return\n",
    "        \n",
    "        for key in rdict:\n",
    "            if key not in self.metrics[stage]:\n",
    "                self.metrics[stage][key] = []\n",
    "            self.metrics[stage][key].append(rdict[key])\n",
    "            \n",
    "    def _append_batch_step(self, rdict:Dict[str, float]) -> None:\n",
    "        # Called after batch step rdict is returned\n",
    "        self._append(rdict, self._batch_step)\n",
    "        \n",
    "    def _append_epoch_start(self, rdict:Dict[str, float]) -> None:\n",
    "        # Called before epoch start\n",
    "        self._append(rdict, self._epoch_start)\n",
    "        \n",
    "    def _append_epoch_end(self, rdict:Dict[str, float]) -> None:\n",
    "        # Called after epoch end step rdict is returned\n",
    "        self._append(rdict, self._epoch_end)\n",
    "    \n",
    "        \n",
    "    \"\"\"\n",
    "    Functions to clear rdict values from self.metrics\n",
    "    \"\"\"\n",
    "    def _clear(self, stage:str) -> None:\n",
    "        # Clear the batch metrics at the end of the batch.\n",
    "        for mlist in self.metrics[stage]:\n",
    "            self.metrics[stage][mlist].clear()\n",
    "            \n",
    "    def _clear_batch_step(self) -> None:\n",
    "        # Called before epoch start\n",
    "        self._clear(self._batch_step)\n",
    "        \n",
    "    def _clear_epoch_start(self) -> None:\n",
    "        # Called ??\n",
    "        self._clear(self._epoch_start)\n",
    "        \n",
    "    def _clear_epoch_end(self) -> None:\n",
    "        # Called after loop end\n",
    "        self._clear(self._epoch_end)\n",
    "    \n",
    "    \"\"\"\n",
    "    State updates before epoch start and batch step stages\n",
    "    \"\"\"\n",
    "    def _pre_epoch_start_update(self, epoch_num:int) -> None:\n",
    "        self._clear_batch_step()\n",
    "        self.batch_num = 0\n",
    "        self.epoch_num = epoch_num\n",
    "    \n",
    "    def _pre_batch_step_update(self, current_batch):\n",
    "        self.batch_size = current_batch[0].size(0)\n",
    "        self.batch_num += 1\n",
    "        self.batch = current_batch\n",
    "    \n",
    "    \"\"\"\n",
    "    Functions to get various metrics at different stages \n",
    "    \"\"\"\n",
    "    def _get_epoch_metric(self, criteria:str) -> float:\n",
    "        # Last added metric that is to be used as a model \n",
    "        # selection criteria\n",
    "        metric = self.metrics[self._epoch_end][criteria][-1]\n",
    "        if self._no_float:\n",
    "            return metric\n",
    "        else:\n",
    "            return float(metric)\n",
    "    \n",
    "    def _get_epoch_metrics(self, \n",
    "                display_metrics:Optional[Union[str,List[str]]]=None\n",
    "                ) -> Dict[str,float]:\n",
    "        # Return the last saved epoch metrics\n",
    "        if isinstance(display_metrics, str):\n",
    "            return {display_metricss:self._get_epoch_metric(display_metrics)}\n",
    "        elif isinstance(display_metrics, list):\n",
    "            return {\n",
    "                metric:self._get_epoch_metric(metric)\n",
    "                for metric in display_metrics\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                metric: self._get_epoch_metric(metric)\n",
    "                for metric in self.metrics[self._epoch_end]\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FitLoop\n",
    "- all stage functions except for `train_step` should be optional that's the only one that is required for training the model, rest all are for metric keeping.\n",
    "- ✅if `FitLoop.fit(define_all:bool=False)` the zero_grad and the context manager are not auto set.\n",
    "- ✅should keep track of epochs that have been completed\n",
    "- ✅epoch_number can be reset \n",
    "- metrics can be cleared\n",
    "- `FitLoop.set_name.loop_stage['metric_name']` to access the metric\n",
    "- `FitLoop.store_pretrained:bool` arg to store the pretrained weights before training\n",
    "    if path then store at given path else store in memory.\n",
    "- ✅`FitLoop.reset(reset_model:bool)` to clear metrics, epoch_num and to reset the model, to pretrained state\n",
    "    will load the weight from passed path else from memory.\n",
    "- `FitLoop.save(path:str)` to save the model and training state somehow even the fitloop state.\n",
    "- `FitLoop.load(path:str)` to load the FitLoop state from given path.\n",
    "- Some basic step should be used such that one can use it without defining step functions.\n",
    "- `FitLoop.fit(continue_loop:int=0)` ask after `int` whether to continue training or to end.\n",
    "- `FitLoop.fit(profiler:bool=False)` mode to capture all stage timings and maybe even CPU, GPU, RAM usage to check for bottlenecks and usage spikes, to be used with timed_test.\n",
    "- Make it easy to train,validate, test with some other DataLoader that is not attached to the object.\n",
    "- `FitLoop.fit(no_print:bool=False)` mode to capture all stage timings and maybe even CPU, GPU, RAM usage to check for bottlenecks and usage spikes, to be used with timed_test.\n",
    "- Use a loading bar for epoch and a disappearing one for batch.\n",
    "- Functionality to view the metrics.\n",
    "- ✅Model score should be a loop instance so that the best model may not be erased.\n",
    "- Time keeping/ metric keeping:\n",
    "    - Profiler:\n",
    "        - Individual Stage Timings\n",
    "        - Individual Stage CPU Usage\n",
    "        - Individual Stage GPU Usage\n",
    "        - Individual Stage RAM Usage\n",
    "        - For these Stages:\n",
    "            - Batch step for each phase\n",
    "            - Epoch start step for each phase\n",
    "            - Epoch end step for each phase\n",
    "    - General\n",
    "        - Metrics returned in the batch step\n",
    "        - Metrics returned in the end step\n",
    "        - Progress bar for epoch\n",
    "        - Progress bar for batch that disappears after complete\n",
    "        - ✅ Epoch timing (for both phases when training)\n",
    "        - ✅ Total timing \n",
    "- Default Trai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T17:48:33.456334Z",
     "start_time": "2020-05-06T17:48:33.451212Z"
    }
   },
   "outputs": [],
   "source": [
    "class FitLoopDefault:\n",
    "    def train_step(state):\n",
    "        print(\"default train_step\")\n",
    "        return {}\n",
    "\n",
    "    def valid_step(state):\n",
    "        print(\"default valid_step\")\n",
    "        return {}\n",
    "\n",
    "    def test_step(state):\n",
    "        print(\"default test_step\")\n",
    "        return {}\n",
    "        \n",
    "    def train_epoch_end(state):\n",
    "        print(\"default train_epoch_end\")\n",
    "        return {}\n",
    "\n",
    "    def valid_epoch_end(state):\n",
    "        print(\"default valid_epoch_end\")\n",
    "        return {}\n",
    "\n",
    "    def test_epoch_end(state):\n",
    "        print(\"default test_epoch_end\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T18:02:54.574702Z",
     "start_time": "2020-05-06T18:02:54.572167Z"
    }
   },
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "    \"\"\"\n",
    "    Class to keep track of all the metrics and should have\n",
    "    visualization for the metrics.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FitLoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T00:41:23.551220Z",
     "start_time": "2020-05-07T00:41:23.544713Z"
    }
   },
   "outputs": [],
   "source": [
    "def ftime(t1:float,t2:float)->str:\n",
    "    t = t2-t1\n",
    "    s, ms = str(t).split('.')\n",
    "    ms = ms[:3]\n",
    "    s = str(int(s)%60).rjust(2)\n",
    "    m = int(t//60)\n",
    "    h = str(m//60)\n",
    "    m = str(m % 60).rjust(2)\n",
    "    u = ['h','m','s','ms']\n",
    "    v = [h,m,s,ms]\n",
    "    t = filter(lambda x:int(x[0]) > 0,zip(v,u))\n",
    "    return ' '.join([' '.join(x) for x in t]).ljust(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T01:10:51.651179Z",
     "start_time": "2020-05-07T01:10:51.562008Z"
    }
   },
   "outputs": [],
   "source": [
    "class FitLoop:\n",
    "    \"\"\"\n",
    "    class that helps in training pytorch models.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    SECTION: 0 \n",
    "    \n",
    "    Initialization\n",
    "    \"\"\"\n",
    "    _sets = ['train','valid','test']\n",
    "    _TR, _VA, _TE = _sets\n",
    "    \n",
    "    _model_type = ['pretrained','best']\n",
    "    _PR, _BS = _model_type\n",
    "    def __init__(self, \n",
    "                 # Basic Blocks\n",
    "                 model: Module, \n",
    "                 optimizer: Union[Optimizer,List[Optimizer]], \n",
    "                 loss_function: Callable[[Tensor,Tensor],Tensor], \n",
    "                 \n",
    "                 # DataLoader\n",
    "                 train_dl: DataLoader, \n",
    "                 valid_dl: Optional[DataLoader]=None, \n",
    "                 test_dl: Optional[DataLoader]=None, \n",
    "                 \n",
    "                 # Batch Step\n",
    "                 train_step: Callable[[LoopState],Dict[str, Any]]=FitLoopDefault.train_step,\n",
    "                 valid_step: Optional[Callable[[LoopState],Dict[str, Any]]]=FitLoopDefault.valid_step,\n",
    "                 test_step: Optional[Callable[[LoopState],Dict[str, Any]]]=FitLoopDefault.test_step,\n",
    "                 \n",
    "                 # Epoch Start Step\n",
    "                 train_epoch_start: Optional[Callable[[LoopState],Dict[str, Any]]]=None,\n",
    "                 valid_epoch_start: Optional[Callable[[LoopState],Dict[str, Any]]]=None,\n",
    "                 test_epoch_start: Optional[Callable[[LoopState],Dict[str, Any]]]=None,\n",
    "                 \n",
    "                 # Epoch End Step\n",
    "                 train_epoch_end: Callable[[LoopState],Dict[str, Any]]=FitLoopDefault.train_epoch_end,\n",
    "                 valid_epoch_end: Optional[Callable[[LoopState],Dict[str, Any]]]=FitLoopDefault.valid_epoch_end,\n",
    "                 test_epoch_end: Optional[Callable[[LoopState],Dict[str, Any]]]=FitLoopDefault.test_epoch_end,\n",
    "                 \n",
    "                 # Other Args\n",
    "                 lr_scheduler: Optional[Union[_LRScheduler, Any, List[Union[_LRScheduler,Any]]]]=None,\n",
    "                 device: torch.device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'), \n",
    "                 configure_optimizer:Callable[[FitLoop],None]=None,\n",
    "                 dtype: torch.dtype=torch.float32,\n",
    "                 \n",
    "                 # Model Evaluation\n",
    "                 criteria: Optional[str]=None,\n",
    "                 criteria_direction: int=1,\n",
    "                 \n",
    "                 # Preserving Model State\n",
    "                 save_to_disk: bool=False,\n",
    "                 save_path: str=\"models\",\n",
    "                 pretrained_model_name: Optional[str]=None,\n",
    "                 best_model_name: Optional[str]=None,\n",
    "                ) -> None:\n",
    "        \"\"\"\n",
    "        FitLoop constructor\n",
    "        ----\n",
    "        Parameters:\n",
    "        # Basic Blocks\n",
    "            The bare minimum required along with train_dl.\n",
    "            - model : nn.Module model that has to be trained\n",
    "            - optimizer : an optimizer from torch.optim\n",
    "            - loss_function : function to compute loss\n",
    "         \n",
    "        # DataLoader\n",
    "            - train_dl : training DataLoader\n",
    "            - valid_dl : validation DataLoader, if None validation will be ignored\n",
    "            - test_dl : testing DataLoader, if None `.test()` will not run\n",
    "         \n",
    "        # Batch Step\n",
    "            Functions that take in a LoopState object to perform \n",
    "            required calculations, functions should return a dict with values\n",
    "            to be used in the epoch end step.\n",
    "            - train_step : portion of the loop where forward and backward \n",
    "                passes take place.\n",
    "            - valid_step : validation portion of the loop.\n",
    "            - test_step : called when `FitLoop.test()` is called.\n",
    "        \n",
    "        # Epoch Start Step\n",
    "            TODO: NEED TO IMPLEMENT\n",
    "            - train_epoch_start :\n",
    "            - valid_epoch_start :\n",
    "            - test_epoch_start : \n",
    "        \n",
    "        # Epoch End Step\n",
    "            Functions that take in a LoopState object to perform \n",
    "            required calculations, functions should return a dict with values\n",
    "            that are to be returned when the loop is over.\n",
    "            - train_epoch_end : after training epoch has ended.\n",
    "            - valid_epoch_end : after validation epoch has ended.\n",
    "            - test_epoch_end : called when the test loop is done, one iteration\n",
    "                over all batches in the test dataloader.\n",
    "        \n",
    "        # Other Args\n",
    "            - lr_scheduler : scheduler from torch.optim.lr_scheduler\n",
    "            - device : torch.device model will be cast to device this prior to the loop\n",
    "            - configure_optimizer : function that configures the optimizer, will be called\n",
    "                whenever the model weights have to be restored.\n",
    "            - dtype : floating point dtype to cast model and data to\n",
    "            \n",
    "        # Model Evaluation\n",
    "            - criteria : model evaluation metric that is returned in the dict of the\n",
    "                `valid_epoch_end` stage function if None (default) best model and \n",
    "                best score are not tracked.\n",
    "            - criteria_direction : whether more is better (1) or less is better (-1) \n",
    "                for model score criteria.\n",
    "        \n",
    "        # Preserving Model State\n",
    "            - save_to_disk : True then save pretrained and best_model to the disk, else it is \n",
    "                stored as an attribute.\n",
    "            - save_path : location where the initial and pretrained models are to be saved\n",
    "            - pretrained_model_name : Name to save the pretrained model by\n",
    "            - best_model_name : Name to save the best model by\n",
    "        \"\"\"\n",
    "        # Basic Blocks\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_function = loss_function\n",
    "        \n",
    "        # DataLoaders\n",
    "        self.train_dl = train_dl\n",
    "        self.valid_dl = valid_dl\n",
    "        self.test_dl = test_dl\n",
    "        \n",
    "        # Batch Step\n",
    "        self.train_step = train_step\n",
    "        self.valid_step = valid_step\n",
    "        self.test_step = test_step\n",
    "        \n",
    "        # Epoch Start Step\n",
    "        self.train_epoch_start = train_epoch_start\n",
    "        self.valid_epoch_start = valid_epoch_start\n",
    "        self.test_epoch_start = test_epoch_start\n",
    "        \n",
    "        # Epoch End Step\n",
    "        self.train_epoch_end = train_epoch_end\n",
    "        self.valid_epoch_end = valid_epoch_end\n",
    "        self.test_epoch_end = test_epoch_end\n",
    "        \n",
    "        # Other Args\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.device = device\n",
    "        self.configure_optimizer = configure_optimizer\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        # Model Evaluation\n",
    "        self.criteria = criteria\n",
    "        self.criteria_direction = criteria_direction\n",
    "        \n",
    "        # Preserving Model State\n",
    "        if pretrained_model_name is None:\n",
    "            u = str(uuid4()).split('-')[1]\n",
    "            pretrained_model_name = f\"pretrained_{u}.pt\"\n",
    "        if best_model_name is None:\n",
    "            u = str(uuid4()).split('-')[1]\n",
    "            best_model_name = f\"best_{u}.pt\"\n",
    "        self.pretrained_model_name = pretrained_model_name\n",
    "        self.best_model_name = best_model_name\n",
    "        self.save_to_disk = save_to_disk\n",
    "        self.save_path = Path(save_path)\n",
    "        \n",
    "        \n",
    "        # INITIALIZE NON ARGS\n",
    "        self.__save_model(self._PR)\n",
    "        self.epoch_num = 0\n",
    "        self.best_score = self.criteria_direction * float('-inf')\n",
    "        self.profiler = {}\n",
    "        self.pretrained_model_state_dict = None\n",
    "        self.best_model_state_dict = None\n",
    "            \n",
    "            \n",
    "    # ---------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    SECTION: 1\n",
    "    \n",
    "    Helper functions used in `__loop`\n",
    "    \"\"\"\n",
    "        \n",
    "    \n",
    "    def __call_batch_step(self, state:LoopState) -> None:\n",
    "        step_funcs = [self.train_step, self.valid_step, self.test_step]\n",
    "        step_funcs = {s:f for s,f in zip(self._sets, step_funcs)}\n",
    "        step_func = step_funcs[state.phase]\n",
    "        \n",
    "        if step_func is None:\n",
    "            raise AttributeError(f\"{phase}_step not assigned\")\n",
    "        rdict = step_func(state)\n",
    "        if isinstance(rdict,dict):\n",
    "            state._append_batch_step(rdict)\n",
    "        \n",
    "    def __call_epoch_start_step(self, state:LoopState) -> None:\n",
    "        step_funcs = [self.train_epoch_start,self.valid_epoch_start,self.test_epoch_start]\n",
    "        step_funcs = {s:f for s,f in zip(self._sets, step_funcs)}\n",
    "        step_func = step_funcs[state.phase]\n",
    "        \n",
    "        if step_func is None:\n",
    "            return None\n",
    "        rdict = step_func(state)\n",
    "        if isinstance(rdict,dict):\n",
    "            state._append_epoch_start(rdict)\n",
    "        \n",
    "    def __call_epoch_end_step(self, state:LoopState) -> None:\n",
    "        step_funcs = [self.train_epoch_end,self.valid_epoch_end,self.test_epoch_end]\n",
    "        step_funcs = {s:f for s,f in zip(self._sets, step_funcs)}\n",
    "        step_func = step_funcs[state.phase]\n",
    "        \n",
    "        if step_func is None:\n",
    "            raise AttributeError(f\"{phase}_end_step not assigned\")\n",
    "        rdict = step_func(state)\n",
    "        if isinstance(rdict,dict):\n",
    "            state._append_epoch_end(rdict)\n",
    "        \n",
    "    def __get_dl(self, is_test:bool)-> Dict[str,DataLoader]:\n",
    "        if is_test:\n",
    "            if self.test_dl is None:\n",
    "                raise AttributeError(\"test_dl not assigned\")\n",
    "            return {self._TE:self.test_dl}\n",
    "        \n",
    "        va_dl = self.valid_dl is not None\n",
    "        if  va_dl:\n",
    "            return {self._TR:self.train_dl, self._VA:self.valid_dl}\n",
    "        else:\n",
    "            return {self._TR:self.train_dl}\n",
    "    \n",
    "    def __profile_time(self,t1,t2,name):\n",
    "        # TODO: adds to profiler\n",
    "        if t1 is None or t2 is None:\n",
    "            return\n",
    "    \n",
    "    def __profile_other(self,val,name):\n",
    "        # TODO: Profiler for other metrics\n",
    "        pass\n",
    "        \n",
    "        \n",
    "    # ---------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    SECTION: 2\n",
    "    \n",
    "    The main loop function __loop \n",
    "    \"\"\"\n",
    "    \n",
    "    def __loop(self, \n",
    "            epochs:int=1,  print_every:int=1, \n",
    "            steps: Optional[int]=None, load_best:bool=False, \n",
    "            profiler:bool=False, is_test:bool=False,\n",
    "            track_batch_metrics:bool=True, define_all:bool=False,\n",
    "            continue_loop:int=0, no_print:bool=False, no_cast:bool=False,\n",
    "            display_metrics:Optional[Union[str,List[str]]]=None, no_float:bool=False,\n",
    "            is_sanity_check:bool=False \n",
    "           ) -> None:\n",
    "        \"\"\"\n",
    "        Runs the training loop for `epochs`\n",
    "        ----\n",
    "        PARAMETERS\n",
    "         - epochs : should be a non negative integer\n",
    "         - print_every : if 0 will not print, else will print at given epoch\n",
    "         - steps : number of batches to run in each phase [train,valid] \n",
    "             for check if everything is working.\n",
    "         - load_best : whether to load the best model after training, works only if validation\n",
    "             parameters are defined `valid_dl`, `valid_step`, `valid_epoch_end`\n",
    "         - profiler : whether to keep track of time taken by various sections\n",
    "         - is_test : whether it is a model testing loop or training/validation loop\n",
    "         - track_batch_metrics : whether to store the values returned in the batch steps\n",
    "         - define_all : If True then `torch.set_grad_enabled`, `optimizer.zero_grad` and model mode \n",
    "             ie [train,eval] have to be called where required (usually in the `train_step` function).\n",
    "         -  continue_loop : Will ask whether to continue training after `continue` epochs, should\n",
    "             be a positive integer.\n",
    "         - no_print : If True will suppress all print statements, can be used when custom logging is\n",
    "             used in the stage functions.\n",
    "         - no_cast : True, if model and data casting has to be manually set in the stage functions\n",
    "         - display_metrics : List of metrics returned in the epoch_end stage rdict that has to be \n",
    "             displayed, if None (default) all the returned metrics are displayed.\n",
    "         - no_float : True don't apply float conversion to returned metrics.\n",
    "         - is_sanity_check : For sanity check mode.\n",
    "        \n",
    "        \"\"\"\n",
    "        time_ = lambda p : time.time() if p else None\n",
    "        tpe = lambda : time_(print_every != 0) # Returns the time \n",
    "        tpr = lambda : time_(profiler) # Times keeping used by profiler\n",
    "        total_time_start = tpe()\n",
    "        \n",
    "        # INITILIZING VARIABLES -----\n",
    "        is_train = not(is_test or is_sanity_check or profiler)\n",
    "        \n",
    "        # Storage\n",
    "        dl = self.__get_dl(is_test)\n",
    "        sz = { k : len(dl[k].dataset) for k in dl }\n",
    "        phases = [ph for ph in dl]\n",
    "        state = {ph: LoopState(ph,self,no_cast,no_float,is_train, is_test) for ph in phases}\n",
    "\n",
    "        # Markers\n",
    "        self.__save_model(self._BS)\n",
    "\n",
    "        # ----------------------------\n",
    "        \n",
    "        \n",
    "        # CONVENIENCE FUNCTIONS ------\n",
    "        \n",
    "        # Function to get formatted epochs (from 1 not 0)\n",
    "        r_just_val = len(str(epochs))*2 + 3\n",
    "        estr = lambda e: f\"[{e + 1}/{epochs}]\".rjust(r_just_val)\n",
    "\n",
    "        # Function to print every `print_every` epochs.\n",
    "        def eprint(e,st):\n",
    "            if (e == 0) and (print_every != 0):\n",
    "                print(st,end=\"\")\n",
    "            elif (e + 1) % print_every == 0:\n",
    "                print(st,end=\"\")\n",
    "\n",
    "        def grad_times(t):\n",
    "            t = t*1000\n",
    "            return f\"{t:0.3f} ms\".rjust(10)\n",
    "\n",
    "        # Function for phase strings.\n",
    "        def statstr(phase, loss, accu, time, infr, rjust=True):\n",
    "            infr = grad_times(infr)\n",
    "            st =  f\"{phase} :: loss: {loss:0.4f} | accu: {accu:0.4f} | infr: {infr}  | time: {time} \\n\"\n",
    "            if rjust:\n",
    "                return st.rjust(r_just_val + len(st) + 3)\n",
    "            else:\n",
    "                return st\n",
    "            \n",
    "        # ----------------------------\n",
    "\n",
    "\n",
    "        # THE LOOP - START -----------\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO : Preloop section, to initilize parameters in whichever way, add profiling.\n",
    "            Manual cast for below line\n",
    "        \"\"\"\n",
    "        if not no_cast:\n",
    "            model = self.model.to(device=self.device, dtype=self.dtype)\n",
    "        for e in range(epochs):\n",
    "            epoch_time_start = tpe()\n",
    "            \n",
    "            # Update FitLoop epoch_num\n",
    "            if not is_sanity_check and not profiler and not is_test:\n",
    "                self.epoch_num += 1\n",
    "            \n",
    "            for phase in phases:\n",
    "                \n",
    "                # Update LoopState: batch_num, metrics['batch'], epoch_num\n",
    "                state[phase]._pre_epoch_start_update(e)\n",
    "                \n",
    "                # EPOCH START STEP - START \n",
    "                self.__call_epoch_start_step(state[phase])\n",
    "                # EPOCH START STEP - END \n",
    "                \n",
    "                is_tr = phase == self._TR\n",
    "                if not define_all:\n",
    "                    if is_tr:\n",
    "                          model.train()\n",
    "                    else:\n",
    "                          model.eval()\n",
    "                            \n",
    "                # if is_tr:\n",
    "                #    eprint(e,estr(e)+f\" - \")\n",
    "                    \n",
    "                    \n",
    "                # BATCH LOOP - START \n",
    "                for batch in dl[phase]:\n",
    "                    \n",
    "                    # Update LoopState: batch_num, batch and batch_size\n",
    "                    state[phase]._pre_batch_step_update(batch)\n",
    "                    \n",
    "                    # BATCH STEP - START \n",
    "                    if define_all:\n",
    "                        self.__call_batch_step(state[phase])\n",
    "                    else:\n",
    "                        if isinstance(self.optimizer,list):\n",
    "                            for opt in self.optimizer:opt.zero_grad()\n",
    "                        else:\n",
    "                            self.optimizer.zero_grad()\n",
    "                        with torch.set_grad_enabled(is_tr):\n",
    "                            self.__call_batch_step(state[phase])\n",
    "                    # BATCH STEP - END \n",
    "                # BATCH LOOP - END \n",
    "                \n",
    "                # EPOCH END STEP - START \n",
    "                self.__call_epoch_end_step(state[phase])\n",
    "                # EPOCH END STEP - END \n",
    "                \n",
    "                # UPDATE MARKERS\n",
    "                if not (is_tr or is_test or profiler or is_sanity_check) and self.criteria is not None:\n",
    "                    score = state[phase]._get_epoch_metric(self.criteria)\n",
    "                    direc = self.criteria_direction > 0\n",
    "                    is_better = (score > self.best_score) if direc else (score < self.best_score)\n",
    "                    if is_better:\n",
    "                        self.best_score = score\n",
    "                        self.__save_model(self._BS)\n",
    "\n",
    "#                 if is_tr:\n",
    "#                     eprint(e,statstr(phase, phase_loss,phase_accu, phase_time, mean_inf, False))\n",
    "#                 else:\n",
    "#                     eprint(e,statstr(phase, phase_loss,phase_accu, phase_time, mean_inf))\n",
    "\n",
    "                # Display the epoch metrics after the phase end\n",
    "                epoch_metrics = state[phase]._get_epoch_metrics(display_metrics)\n",
    "                # Replace later\n",
    "                print(phase, epoch_metrics)\n",
    "            \n",
    "            # Logging epoch times\n",
    "            epoch_time_end = tpe()\n",
    "            epoch_time = ftime(epoch_time_start, epoch_time_end)\n",
    "            \n",
    "            # Replace later\n",
    "            print('epoch_time:',epoch_time)\n",
    "#             epoch_time = f\"epoch time: {ftime(epoch_time_start, epoch_time_end)}\\n\"\n",
    "#             eprint(e,epoch_time.rjust(len(epoch_time)+r_just_val+3)+\"\\n\")\n",
    "\n",
    "        # THE LOOP - END -------------\n",
    "    \n",
    "        # ----------------------------\n",
    "        total_time_end = tpe()\n",
    "        total_time = ftime(total_time_start,total_time_end)\n",
    "        \n",
    "        # Replace later\n",
    "        print('total_time:',total_time)\n",
    "        \n",
    "#         print(f\"least loss: {least_loss:0.4f} | best accu: {best_accuracy:0.4f} | total time taken: {total_time}\")\n",
    "\n",
    "        if load_best or profiler or is_sanity_check:\n",
    "            self.__load_model(self._BS)\n",
    "\n",
    "\n",
    "        # __loop - END ---------------\n",
    "\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    SECTION: 3\n",
    "    \n",
    "    Loop methods that are called by the FitLoop user.\n",
    "    \"\"\"\n",
    "    \n",
    "    def fit(self, \n",
    "            epochs:int=1, print_every:int=1,\n",
    "            display_metrics:Optional[Union[str,List[str]]]=None,\n",
    "            track_batch_metrics:bool=True, load_best:bool=True,\n",
    "            continue_loop:int=0, define_all:bool=False,  \n",
    "            no_print:bool=False, no_cast:bool=False,\n",
    "            no_float:bool=False\n",
    "           ) -> None:\n",
    "        \"\"\"\n",
    "        Runs the training loop for `epochs`\n",
    "        ----\n",
    "        PARAMETERS\n",
    "         - epochs : should be a non negative integer\n",
    "         - print_every : if 0 will not print, else will print at given epoch\n",
    "         - display_metrics : List of metrics returned in the epoch_end stage rdict that has to be \n",
    "             displayed, if None (default) all the returned metrics are displayed.\n",
    "         - track_batch_metrics : whether to store the values returned in the batch steps\n",
    "         - load_best : whether to load the best model after training, works only if validation\n",
    "             parameters are defined `valid_dl`, `valid_step`, `valid_epoch_end`\n",
    "         -  continue_loop : Will ask whether to continue training after `continue` epochs; should\n",
    "             be a positive integer.\n",
    "         - define_all : If True then `torch.set_grad_enabled`, `optimizer.zero_grad` and model mode \n",
    "             ie [train,eval] have to be called where required (usually in the `train_step` function).\n",
    "         - no_print : If True will suppress all print statements, can be used when custom logging is\n",
    "             used in the stage functions.\n",
    "         - no_cast : True, if model and data casting has to be manually set in the stage functions\n",
    "         - no_float : True don't apply float conversion to returned metrics.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.__loop(epochs=epochs, print_every=print_every,\n",
    "                   display_metrics=display_metrics, track_batch_metrics=track_batch_metrics,\n",
    "                   load_best=load_best, continue_loop=continue_loop, define_all=define_all,\n",
    "                   no_print=no_print, no_cast=no_cast, no_float=no_float)\n",
    "    \n",
    "    def train(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Alias for FitLoop.fit\n",
    "        \"\"\"\n",
    "        self.fit(*args, **kwargs)\n",
    "    \n",
    "    def test(self, no_print:bool=False, no_cast:bool=False, no_float:bool=False) -> None:\n",
    "        \"\"\"\n",
    "        For model testing. Runs loop for one epoch using test DataLoader and test stage functions.\n",
    "        ----\n",
    "        PARAMETERS\n",
    "         - no_print : If True will suppress all print statements, can be used when custom logging is\n",
    "             used in the stage functions.\n",
    "         - no_cast : True, if model and data casting has to be manually set in the stage functions\n",
    "         - no_float : True don't apply float conversion to returned metrics.\n",
    "        \"\"\"\n",
    "        self.__loop(is_test=True, no_print=no_print, no_cast=no_cast, no_float=no_float)\n",
    "    \n",
    "        \n",
    "    def run_profiler(self,\n",
    "            steps: Optional[int]=None, define_all:bool=False,\n",
    "            no_cast:bool=False, no_float:bool=False\n",
    "            ) -> None:\n",
    "        \"\"\"\n",
    "        Runs the loop in profiler mode, ie run all three (train, valid, test) stages \n",
    "        (if set) for a single epoch and given number of steps and profile the time taken \n",
    "        at different stages along with resource utilization.\n",
    "        \n",
    "        Model state is not altered (it's reloaded) if the profiler is not interrupted.\n",
    "        ----\n",
    "        PARAMETERS\n",
    "         - steps : number of batches to iterate over in each phase [train,valid,test] \n",
    "             to check if everything is working as expected, if None then all batches are\n",
    "             iterated over.\n",
    "         - define_all : If True then `torch.set_grad_enabled`, `optimizer.zero_grad` and model mode \n",
    "             ie [train,eval] have to be called where required (usually in the `train_step` function).\n",
    "         - no_cast : True, if model and data casting has to be manually set in the stage functions\n",
    "         - no_float : True don't apply float conversion to returned metrics.\n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO : Implement this in __loop\n",
    "        print(\"NOT IMPLEMENTED\")\n",
    "        return\n",
    "        \n",
    "        self.__loop(steps=steps, define_all=define_all, no_cast=no_cast, \n",
    "                    no_float=no_float, no_print=True, profiler=True)\n",
    "        self.__loop(steps=steps, define_all=define_all, no_cast=no_cast, \n",
    "                    no_float=no_float, no_print=True, profiler=True, is_test=True)\n",
    "    \n",
    "    def sanity_check(self, use_test_dl=False,\n",
    "            epochs:int=1, steps:int=5, print_every:int=1,\n",
    "            display_metrics:Optional[Union[str,List[str]]]=None,\n",
    "            continue_loop:int=0, define_all:bool=False,  \n",
    "            no_print:bool=False, no_cast:bool=False,\n",
    "            no_float:bool=False\n",
    "           ) -> None:\n",
    "        \"\"\"\n",
    "        Runs the loop in sanity check mode, ie all three (train, valid, test) stages \n",
    "        (if set) for given number of epochs and steps.\n",
    "        Model state is not altered (it's reloaded) if the sanity check is not interrupted.\n",
    "        ----\n",
    "        PARAMETERS\n",
    "         - use_test_dl : If False will use the validation DataLoader for the test phase,\n",
    "             else will use the test DataLoader.\n",
    "         - epochs : should be a non negative integer\n",
    "         - steps : number of batches to run in each phase [train,valid] \n",
    "             for check if everything is working.\n",
    "         - print_every : if 0 will not print, else will print at given epoch\n",
    "         - display_metrics : List of metrics returned in the epoch_end stage rdict that has to be \n",
    "             displayed, if None (default) all the returned metrics are displayed.\n",
    "         -  continue_loop : Will ask whether to continue training after `continue` epochs, should\n",
    "             be a positive integer.\n",
    "         - define_all : If True then `torch.set_grad_enabled`, `optimizer.zero_grad` and model mode \n",
    "             ie [train,eval] have to be called where required (usually in the `train_step` function).\n",
    "         - no_print : If True will suppress all print statements, can be used when custom logging is\n",
    "             used in the stage functions.\n",
    "         - no_cast : True, if model and data casting has to be manually set in the stage functions\n",
    "         - no_float : True don't apply float conversion to returned metrics.\n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO : Implement this in __loop\n",
    "        print(\"NOT IMPLEMENTED\")\n",
    "        return\n",
    "    \n",
    "        self.__loop(epochs=epochs, steps=steps, print_every=print_every, \n",
    "                    display_metrics=display_metrics, continue_loop=continue_loop,\n",
    "                    define_all=define_all, no_print=no_print, no_cast=no_cast, \n",
    "                    no_float=no_float, is_sanity_check=True)\n",
    "        self.__loop(use_test_dl=use_test_dl, epochs=epochs, steps=steps, print_every=print_every, \n",
    "                    display_metrics=display_metrics, continue_loop=continue_loop,\n",
    "                    define_all=define_all, no_print=no_print, no_cast=no_cast, \n",
    "                    no_float=no_float, is_sanity_check=True, is_test=True)\n",
    "    \n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    SECTION: 4\n",
    "    \n",
    "    Functions to preserve the model state.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __save_model(self, typ:str) -> None:\n",
    "        \"\"\"\n",
    "        Save model to object or to the disk.\n",
    "        \"\"\"\n",
    "        name = self.best_model_name if typ == self._BS else self.pretrained_model_name\n",
    "        path = self.save_path/ name\n",
    "        state_dict = self.model.state_dict()\n",
    "        if self.save_to_disk:\n",
    "            torch.save(state_dict,path)\n",
    "        elif typ == self._BS:\n",
    "            self.best_model_state_dict = deepcopy(state_dict)\n",
    "        else:\n",
    "            self.pretrained_model_state_dict = deepcopy(state_dict)\n",
    "        \n",
    "    def __load_model(self, typ:str):\n",
    "        \"\"\"\n",
    "        Load model from the object or from the disk.\n",
    "        \"\"\"\n",
    "        name = self.best_model_name if typ == self._BS else self.pretrained_model_name\n",
    "        path = self.save_path/ name\n",
    "        if self.save_to_disk:\n",
    "            state_dict = torch.load(path, map_location=self.device)\n",
    "        elif typ == self._BS:\n",
    "            state_dict = self.best_model_state_dict\n",
    "        else:\n",
    "            state_dict = self.pretrained_model_state_dict\n",
    "        self.model.load_state_dict(state_dict)\n",
    "        if self.configure_optimizer is None:\n",
    "            print(\"please reconfigure FitLoop.optimizer before training\")\n",
    "        else:\n",
    "            self.configure_optimizer(self)\n",
    "    \n",
    "    def reset(self, reset_model:bool=True) -> None:\n",
    "        \"\"\"\n",
    "        Resets FitLoop to initial state.\n",
    "        Parameters reset:\n",
    "            - model, to pretrained state if `reset_model`\n",
    "            - epoch_num, to 0\n",
    "            - best_score to ∓inf\n",
    "        FitLoop.optimizer param groups will have to be set again\n",
    "        \"\"\"\n",
    "        if reset_model:\n",
    "            self.__load_model(self, self._PR)\n",
    "        self.epoch_num = 0\n",
    "        self.best_score = self.criteria_direction * float('-inf')\n",
    "        \n",
    "        \n",
    "    # ---------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    SECTION: 5\n",
    "    \n",
    "    Functions to preserve the FitLoop object state so that training can be resumed.\n",
    "    \"\"\"\n",
    "    \n",
    "    def save(self, path, only_model=False):\n",
    "        \"\"\"\n",
    "        TODO : save the FitLoop state, if only_model then save only model.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def load(self, path):\n",
    "        \"\"\"\n",
    "        TODO : load the FitLoop state, if only model then load the model \n",
    "            state dict.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    SECTION: 6\n",
    "    \n",
    "    Functions to delete stored model weights.\n",
    "    \"\"\"\n",
    "    \n",
    "    def del_pretrained(self) -> None:\n",
    "        \"\"\"\n",
    "        Deletes the pretrianed model state dict from the disk if \n",
    "        `save_to_disk` else states attribute to None\n",
    "        \"\"\"\n",
    "        if self.save_to_disk:\n",
    "            (self.save_path/self.pretrained_model_name).unlink()\n",
    "        else:\n",
    "            self.pretrained_model_state_dict = None\n",
    "        \n",
    "    def del_best_model(self) -> None:\n",
    "        \"\"\"\n",
    "        Deletes the best model state dict from the disk if \n",
    "        `save_to_disk` else states attribute to None\n",
    "        \"\"\"\n",
    "        if self.save_to_disk:\n",
    "            (self.save_path/self.best_model_name).unlink()\n",
    "        else:\n",
    "            self.best_model_state_dict = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T00:30:42.867191Z",
     "start_time": "2020-05-07T00:30:42.862331Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim, nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.datasets import FakeData\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T01:11:00.113567Z",
     "start_time": "2020-05-07T01:10:59.948347Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Basic setup with FakeData for testing.\n",
    "\"\"\"\n",
    "\n",
    "sets = ['train','valid','test']\n",
    "TR, VA, TE = sets\n",
    "class_names = ['a','b','c','d']\n",
    "num_classes = 4\n",
    "batch_size = 4\n",
    "\n",
    "sz = {s:z*batch_size for s, z in zip(sets,[5,2,3])} # a multiple of batch size\n",
    "ds = {s:FakeData(size=sz[s], transform=ToTensor(), num_classes=num_classes) for s in sets}\n",
    "dl = {s:DataLoader(ds[s],batch_size=batch_size) for s in ds}\n",
    "\n",
    "model = resnet18()\n",
    "in_features = model.fc.in_features\n",
    "model.fc = nn.Linear(in_features, num_classes)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T21:27:09.125192Z",
     "start_time": "2020-05-06T21:27:08.453064Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get batch\n",
      "CPU times: user 18.3 ms, sys: 7.25 ms, total: 25.5 ms\n",
      "Wall time: 34.6 ms\n",
      "torch.Size([4, 3, 224, 224]) torch.Size([4])\n",
      "\n",
      "inference\n",
      "CPU times: user 335 ms, sys: 98.4 ms, total: 434 ms\n",
      "Wall time: 609 ms\n",
      "torch.Size([4, 4])\n",
      "\n",
      "loss\n",
      "CPU times: user 1.06 ms, sys: 5.85 ms, total: 6.91 ms\n",
      "Wall time: 7.97 ms\n",
      "tensor(1.4268, grad_fn=<NllLossBackward>)\n",
      "\n",
      "accu\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(\"get batch\")\n",
    "%time X,y = next(iter(dl[TR]))\n",
    "print(X.size(),y.size())\n",
    "\n",
    "print(\"\\ninference\")\n",
    "%time y_ = model(X)\n",
    "print(y_.size())\n",
    "\n",
    "print(\"\\nloss\")\n",
    "%time loss = loss_function(y_,y)\n",
    "print(loss)\n",
    "\n",
    "print(\"\\naccu\")\n",
    "accu = (y_.argmax(dim=0)==y).sum().item()\n",
    "print(accu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T00:35:11.160752Z",
     "start_time": "2020-05-07T00:35:11.150290Z"
    }
   },
   "outputs": [],
   "source": [
    "def common_step(state):\n",
    "    print(f\"{state.phase}_step, bn: {state.batch_num} en: {state.epoch_num}, \",end=\"\")\n",
    "    X, y = state.batch\n",
    "    y_ = state.model(X)\n",
    "    loss = state.loss_function(y_,y)\n",
    "    print(\"loss\",loss.item())\n",
    "    r_loss = loss.item() * state.batch_size\n",
    "    r_corr = (y_.argmax(dim=1) == y).sum().item()\n",
    "    return loss, r_loss, r_corr\n",
    "\n",
    "def train_step(state):\n",
    "    loss, r_loss, r_corr = common_step(state)\n",
    "    loss.backward()\n",
    "    state.optimizer.step()\n",
    "    return {'r_loss':r_loss,'r_corr':r_corr}\n",
    "\n",
    "def valid_step(state):\n",
    "    loss, r_loss, r_corr = common_step(state)\n",
    "    return {'r_loss':r_loss,'r_corr':r_corr}\n",
    "\n",
    "def test_step(state):\n",
    "    loss, r_loss, r_corr = common_step(state)\n",
    "    return {'r_loss':r_loss,'r_corr':r_corr}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epoch Start Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T00:46:47.617155Z",
     "start_time": "2020-05-07T00:46:47.612664Z"
    }
   },
   "outputs": [],
   "source": [
    "def common_epoch_start(state):\n",
    "    print(f\"\\n{state.phase}_epoch_start, # {state.epoch_num}\")\n",
    "    return {'dummy':'dict'}\n",
    "    \n",
    "def train_epoch_start(state):\n",
    "    return common_epoch_start(state)\n",
    "\n",
    "def valid_epoch_start(state):\n",
    "    return common_epoch_start(state)\n",
    "\n",
    "def test_epoch_start(state):\n",
    "    return common_epoch_start(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epoch End Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T00:42:27.134455Z",
     "start_time": "2020-05-07T00:42:27.120256Z"
    }
   },
   "outputs": [],
   "source": [
    "def common_epoch_end(state):\n",
    "    print(f\"{state.phase}_epoch_end, # {state.epoch_num}\")\n",
    "    r_loss = state['r_loss']\n",
    "    r_corr = state['r_corr']\n",
    "    \n",
    "    print('r_loss len',len(r_loss))\n",
    "    print('r_corr len',len(r_corr))\n",
    "    \n",
    "    e_loss = r_loss.sum()/state.size\n",
    "    e_accu = r_corr.sum()/state.size\n",
    "    \n",
    "    print('loss',e_loss)\n",
    "    print('accu',e_accu)\n",
    "    \n",
    "    return {'loss':e_loss, 'accu':e_accu}\n",
    "    \n",
    "def train_epoch_end(state):\n",
    "    return common_epoch_end(state)\n",
    "\n",
    "def valid_epoch_end(state):\n",
    "    return common_epoch_end(state)\n",
    "\n",
    "def test_epoch_end(state):\n",
    "    return common_epoch_end(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FitLoop - Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T01:11:09.659021Z",
     "start_time": "2020-05-07T01:11:09.616842Z"
    }
   },
   "outputs": [],
   "source": [
    "def configure_optimizer(self):\n",
    "    parameters = self.model.parameters()\n",
    "    self.optimizer.param_groups.clear()\n",
    "    self.optimizer.add_param_group({'params': parameters})\n",
    "    \n",
    "fl_dict = {\n",
    "    \"model\": model,\n",
    "    \"optimizer\": optimizer,\n",
    "    \"loss_function\": loss_function,\n",
    "    \"train_dl\":dl[TR],\n",
    "    \"valid_dl\":dl[VA],\n",
    "    \"test_dl\":dl[TE],\n",
    "    \"train_step\":train_step,\n",
    "    \"valid_step\":valid_step,\n",
    "    \"test_step\":test_step,\n",
    "    \"train_epoch_start\":train_epoch_start,\n",
    "    \"valid_epoch_start\":valid_epoch_start,\n",
    "    \"test_epoch_start\":test_epoch_start,\n",
    "    \"train_epoch_end\":train_epoch_end,\n",
    "    \"valid_epoch_end\":valid_epoch_end,\n",
    "    \"test_epoch_end\":test_epoch_end,\n",
    "    \"configure_optimizer\":configure_optimizer,\n",
    "    \"criteria\": \"accu\"\n",
    "}\n",
    "\n",
    "trainer = FitLoop(**fl_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T01:12:20.591318Z",
     "start_time": "2020-05-07T01:12:04.802193Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_epoch_start, # 0\n",
      "train_step, bn: 1 en: 0, loss 1.457542896270752\n",
      "train_step, bn: 2 en: 0, loss 1.6427596807479858\n",
      "train_step, bn: 3 en: 0, loss 1.4610310792922974\n",
      "train_step, bn: 4 en: 0, loss 1.7224218845367432\n",
      "train_step, bn: 5 en: 0, loss 1.293718695640564\n",
      "train_epoch_end, # 0\n",
      "r_loss len 5\n",
      "r_corr len 5\n",
      "loss tensor(1.5155)\n",
      "accu tensor(0.2500)\n",
      "train {'loss': 1.5154948234558105, 'accu': 0.25}\n",
      "\n",
      "valid_epoch_start, # 0\n",
      "valid_step, bn: 1 en: 0, loss 2.981490135192871\n",
      "valid_step, bn: 2 en: 0, loss 4.458968162536621\n",
      "valid_epoch_end, # 0\n",
      "r_loss len 2\n",
      "r_corr len 2\n",
      "loss tensor(3.7202)\n",
      "accu tensor(0.1250)\n",
      "valid {'loss': 3.720229148864746, 'accu': 0.125}\n",
      "epoch_time:  5 s 377 ms         \n",
      "\n",
      "train_epoch_start, # 1\n",
      "train_step, bn: 1 en: 1, loss 0.15699230134487152\n",
      "train_step, bn: 2 en: 1, loss 0.9917818903923035\n",
      "train_step, bn: 3 en: 1, loss 0.024288427084684372\n",
      "train_step, bn: 4 en: 1, loss 1.3614912033081055\n",
      "train_step, bn: 5 en: 1, loss 0.00760624697431922\n",
      "train_epoch_end, # 1\n",
      "r_loss len 5\n",
      "r_corr len 5\n",
      "loss tensor(0.5084)\n",
      "accu tensor(0.8000)\n",
      "train {'loss': 0.5084320306777954, 'accu': 0.800000011920929}\n",
      "\n",
      "valid_epoch_start, # 1\n",
      "valid_step, bn: 1 en: 1, loss 18.245363235473633\n",
      "valid_step, bn: 2 en: 1, loss 12.940299987792969\n",
      "valid_epoch_end, # 1\n",
      "r_loss len 2\n",
      "r_corr len 2\n",
      "loss tensor(15.5928)\n",
      "accu tensor(0.1250)\n",
      "valid {'loss': 15.5928316116333, 'accu': 0.125}\n",
      "epoch_time:  5 s 196 ms         \n",
      "\n",
      "train_epoch_start, # 2\n",
      "train_step, bn: 1 en: 2, loss 0.7341494560241699\n",
      "train_step, bn: 2 en: 2, loss 2.737565517425537\n",
      "train_step, bn: 3 en: 2, loss 0.6417457461357117\n",
      "train_step, bn: 4 en: 2, loss 0.5342032313346863\n",
      "train_step, bn: 5 en: 2, loss 0.710925281047821\n",
      "train_epoch_end, # 2\n",
      "r_loss len 5\n",
      "r_corr len 5\n",
      "loss tensor(1.0717)\n",
      "accu tensor(0.5000)\n",
      "train {'loss': 1.0717177391052246, 'accu': 0.5}\n",
      "\n",
      "valid_epoch_start, # 2\n",
      "valid_step, bn: 1 en: 2, loss 20.674028396606445\n",
      "valid_step, bn: 2 en: 2, loss 11.413057327270508\n",
      "valid_epoch_end, # 2\n",
      "r_loss len 2\n",
      "r_corr len 2\n",
      "loss tensor(16.0435)\n",
      "accu tensor(0.1250)\n",
      "valid {'loss': 16.043542861938477, 'accu': 0.125}\n",
      "epoch_time:  5 s 186 ms         \n",
      "total_time: 15 s 775 ms         \n"
     ]
    }
   ],
   "source": [
    "trainer.fit(epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Portions of the loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- **Single step** Portion of the loop that receives the batch and will cal forward pass on it.\n",
    "    - *Set* zero grad\n",
    "    - **Train Step** \n",
    "        - *Set* \n",
    "            - enable gradients\n",
    "            - model.train\n",
    "        - *Define*: \n",
    "            - compute loss\n",
    "            - call backward \n",
    "            - update gradients and maybe scheduler step\n",
    "            - loss and whatever metrics will be saved in a list.\n",
    "    - **Valid Step**\n",
    "        - *Set*\n",
    "            - disable gradients\n",
    "            - model.eval\n",
    "        - *Define*: \n",
    "            - compute loss\n",
    "            - loss and whatever metrics will be saved in a list.\n",
    "    - **Test Step** Only in test loop\n",
    "        - *Set*\n",
    "            - disable gradients\n",
    "            - model.eval\n",
    "        - *Define*: \n",
    "            - compute loss\n",
    "            - loss and whatever metrics will be saved in a list.\n",
    "- **Pre Epoch**\n",
    "- **Post Epoch**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Function to calculate a time string from\n",
    "# 2 instances of time.time\n",
    "def ftime(t1,t2):\n",
    "    t = t2-t1\n",
    "    s, ms = str(t).split('.')\n",
    "    ms = ms[:3]\n",
    "    s = str(int(s)%60).rjust(2)\n",
    "    m = int(t//60)\n",
    "    h = str(m//60)\n",
    "    m = str(m % 60).rjust(2)\n",
    "    u = ['h','m','s','ms']\n",
    "    v = [h,m,s,ms]\n",
    "    t = filter(lambda x:int(x[0]) > 0,zip(v,u))\n",
    "    return ' '.join([' '.join(x) for x in t]).ljust(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def fit(model, dl_train, dl_valid, optim, loss_function, epochs, sched=None, should_pass=False, print_every=1, load_best=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    # model\n",
    "      - pytorch model to be trained\n",
    "      \n",
    "    # dl_train\n",
    "      - Dataloader for the train set\n",
    "      \n",
    "    # dl_valid\n",
    "      - Dataloader for the valid set\n",
    "      \n",
    "    # optimizer\n",
    "      - optimizer attached to model params from torch.optim\n",
    "      \n",
    "    # loss_function\n",
    "      - function to calculate the loss; loss_function(model(X),y)\n",
    "      \n",
    "    # epochs\n",
    "      - number of epochs to train for\n",
    "      \n",
    "    # device\n",
    "      - torch.device to load the model to \n",
    "      \n",
    "    # sched\n",
    "      - scheduler for the optimizer learning rate\n",
    "      \n",
    "    # should_pass\n",
    "      - if loss should be passed to the scheduler.\n",
    "      \n",
    "    # print_every\n",
    "      - to print the loss every n epochs\n",
    "    \n",
    "    # load_best \n",
    "      - whether to load the best model as determined by accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    total_time_start = time.time()\n",
    "    # ----------------------------\n",
    "    phases = [TR,VA]\n",
    "    dl = {TR:dl_train, VA:dl_valid}\n",
    "    sizes = {k:len(dl[k].dataset) for k in dl}\n",
    "    \n",
    "    # To contain metrics to plot later\n",
    "    losses_all = {TR:[],VA:[]}\n",
    "    losses_epo = {TR:[],VA:[]}\n",
    "    accuracy_epo = {TR:[],VA:[]}\n",
    "    \n",
    "    # Markers\n",
    "    least_loss = float('inf')\n",
    "    best_accuracy = 0\n",
    "    best_model = deepcopy(model.state_dict())\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Convenience functions (that won't be used elsewhere)\n",
    "    \n",
    "    # Function to get formatted epochs (from 1 not 0)\n",
    "    r_just_val = len(str(epochs))*2 + 3\n",
    "    estr = lambda e: f\"[{e + 1}/{epochs}]\".rjust(r_just_val)\n",
    "    \n",
    "    # Convenience function to print every `print_every` epochs.\n",
    "    def eprint(e,st):\n",
    "        if ((e + 1) % print_every == 0):\n",
    "            print(st,end=\"\")\n",
    "\n",
    "    def grad_times(t):\n",
    "        t = t*1000\n",
    "        return f\"{t:0.3f} ms\".rjust(10)\n",
    "    \n",
    "    # Convenience function for phase strings.\n",
    "    def statstr(phase,loss,accu,time,infr, rjust=True):\n",
    "        infr = grad_times(infr)\n",
    "        st =  f\"{phase} :: loss: {loss:0.4f} | accu: {accu:0.4f} | infr: {infr}  | time: {time} \\n\"\n",
    "        if rjust:\n",
    "            return st.rjust(r_just_val + len(st) + 3)\n",
    "        else:\n",
    "            return st\n",
    "\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Training \n",
    "    model.to(device)\n",
    "    for e in range(epochs):\n",
    "        epoch_time_start = time.time()\n",
    "        \n",
    "        # Training or validation phase\n",
    "        for phase in phases:\n",
    "            phase_time_start = time.time()\n",
    "\n",
    "            # Running metrics\n",
    "            running_loss = 0\n",
    "            running_corr = 0\n",
    "\n",
    "            is_tr = phase == TR\n",
    "            if is_tr:\n",
    "                  model.train()\n",
    "            else:\n",
    "                  model.eval()\n",
    "                  \n",
    "            # Keeping track of computation times.\n",
    "            inference_times = []\n",
    "            dl_p = dl[phase]\n",
    "            dl_l = len(dl_p)\n",
    "            if is_tr:\n",
    "                eprint(e,estr(e)+f\" - \")\n",
    "            for b,batch in enumerate(dl_p):\n",
    "                X,y = batch\n",
    "                bs = y.size(0)\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                \n",
    "                optim.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(is_tr):\n",
    "                    # Forward pass and log times \n",
    "                    inference_time_start = time.time()\n",
    "                    y_ = model(X)\n",
    "                    inference_time_end = time.time()\n",
    "                    inference_times.append((inference_time_end-inference_time_start)/bs)\n",
    "                    \n",
    "                    loss = loss_function(y_,y)\n",
    "                    if is_tr:\n",
    "                        # Backprop (grad calc and param update)\n",
    "                        loss.backward()\n",
    "                        optim.step()\n",
    "                        \n",
    "                    elif not is_tr and sched is not None:\n",
    "                        # Scheduler step\n",
    "                        if should_pass:\n",
    "                            sched.step(loss.item())\n",
    "                        else:\n",
    "                            sched.step()\n",
    "                    \n",
    "                running_loss += loss.item() * bs\n",
    "                running_corr += (y_.argmax(dim=1) == y).sum().item()\n",
    "                losses_all[phase].append(loss.item())\n",
    "             \n",
    "            # Calculate phase losses and scores\n",
    "            phase_loss = running_loss / sizes[phase]\n",
    "            phase_accu = running_corr / sizes[phase]\n",
    "                  \n",
    "            # Calculate timings\n",
    "            mean_inf = np.array(inference_times).mean()\n",
    "            # Log scores and losses\n",
    "            losses_epo[phase].append(phase_loss)\n",
    "            accuracy_epo[phase].append(phase_accu)\n",
    "                \n",
    "            # Update markers save best model\n",
    "            if not is_tr:\n",
    "                if phase_accu > best_accuracy:\n",
    "                    best_accuracy = phase_accu\n",
    "                    best_model = deepcopy(model.state_dict())\n",
    "                if phase_loss < least_loss:\n",
    "                    least_loss = phase_loss\n",
    "            \n",
    "            # Logging phase time\n",
    "            phase_time_end = time.time()\n",
    "            phase_time = ftime(phase_time_start,phase_time_end)\n",
    "            if is_tr:\n",
    "                eprint(e,statstr(phase, phase_loss,phase_accu, phase_time, mean_inf, False))\n",
    "            else:\n",
    "                eprint(e,statstr(phase, phase_loss,phase_accu, phase_time, mean_inf))\n",
    "        \n",
    "        # Logging epoch times\n",
    "        epoch_time_end = time.time()\n",
    "        epoch_time = f\"epoch time: {ftime(epoch_time_start, epoch_time_end)}\\n\"\n",
    "        eprint(e,epoch_time.rjust(len(epoch_time)+r_just_val+3)+\"\\n\")\n",
    "    \n",
    "    if load_best:\n",
    "        model.load_state_dict(best_model)\n",
    "    \n",
    "    # ----------------------------\n",
    "    total_time_end = time.time()\n",
    "    total_time = ftime(total_time_start,total_time_end)\n",
    "    print(f\"least loss: {least_loss:0.4f} | best accu: {best_accuracy:0.4f} | total time taken: {total_time}\")\n",
    "    \n",
    "    return {\"losses_epo\":losses_epo,\"accuracy_epo\":accuracy_epo,\"losses_all\":losses_all}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
